{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1cca287",
   "metadata": {
    "id": "b1cca287"
   },
   "source": [
    "# GANO trained on GRF data on 2D domain\n",
    "\n",
    "The inputs to the generative model are samples of a GRF\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "separated-allowance",
   "metadata": {
    "id": "separated-allowance",
    "outputId": "4840d864-a5ca-4ab9-9bad-1e805b642ada"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/depot/bera89/apps/IDEAS_hviswan/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "#from random_fields import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1025c5e9",
   "metadata": {
    "id": "1025c5e9"
   },
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "invisible-silly",
   "metadata": {
    "id": "invisible-silly"
   },
   "outputs": [],
   "source": [
    "res = 64-8 #resolution\n",
    "npad = 4  # padding of latio 8/128 in U-NO\n",
    "ntrain = 5156 # number of training samples\n",
    "ntest = 1290 # number of testing samples\n",
    "modes = 50 # number of Fourier modes in the initial FNO layer\n",
    "d_co_domain = 16  #the dimension of the co-domain of the initial U-NO layer.\n",
    "lr = 2e-4 #learning rate of the optimizer\n",
    "device = 'cuda:11'\n",
    "epochs = 2000\n",
    "Î»_grad = 10.0 # Lagrange coefficinet for gradient penalty\n",
    "n_critic = 5 # every n_critic iteration the generator is updated\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial-charge",
   "metadata": {
    "id": "initial-charge"
   },
   "outputs": [],
   "source": [
    "# normalization, pointwise gaussian\n",
    "class InputNormalizer(object):\n",
    "    def __init__(self, x, eps=0.00001):\n",
    "        super(InputNormalizer, self).__init__()\n",
    "\n",
    "        # x could be in shape of ntrain*n or ntrain*T*n or ntrain*n*T\n",
    "        self.mean = torch.mean(x).mean(dim=0)\n",
    "        self.std = torch.std(x).mean(dim=0)\n",
    "        self.eps = eps\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = (x - self.mean) / (self.std + self.eps)\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, sample_idx=None):\n",
    "        if sample_idx is None:\n",
    "            std = self.std + self.eps # n\n",
    "            mean = self.mean\n",
    "        else:\n",
    "            if len(self.mean.shape) == len(sample_idx[0].shape):\n",
    "                std = self.std[sample_idx] + self.eps  # batch*n\n",
    "                mean = self.mean[sample_idx]\n",
    "            if len(self.mean.shape) > len(sample_idx[0].shape):\n",
    "                std = self.std[:,sample_idx]+ self.eps # T*batch*n\n",
    "                mean = self.mean[:,sample_idx]\n",
    "\n",
    "        # x is in shape of batch*n or T*batch*n\n",
    "        x = (x * std) + mean\n",
    "        return x\n",
    "\n",
    "    def cuda(self):\n",
    "        self.mean = self.mean.to(device)\n",
    "        self.std = self.std.to(device)\n",
    "\n",
    "    def cpu(self):\n",
    "        self.mean = self.mean.cpu()\n",
    "        self.std = self.std.cpu()\n",
    "class MinMaxNormalizer:\n",
    "    def __init__(self, batch):\n",
    "        self.batch = batch\n",
    "        self.min = batch.min()\n",
    "        self.max = batch.max()\n",
    "    def encode(self, batch): # min max normalization -> [0,1]\n",
    "        encoded = (batch - self.min) / (self.max - self.min)\n",
    "        return encoded \n",
    "    def decode(self, batch): # returns to standard values -> [a,b]\n",
    "        decoded = batch * (self.max - self.min) + self.min\n",
    "        return decoded \n",
    "    def cuda(self):\n",
    "        self.min = self.min.cuda()\n",
    "        self.max = self.max.cuda()\n",
    "    def cpu(self):\n",
    "        self.min = self.min.cpu()\n",
    "        self.max = self.max.cpu()\n",
    "\n",
    "def compute_acovf(z):\n",
    "    from scipy.stats import binned_statistic\n",
    "    z_hat = torch.fft.rfft2(z)\n",
    "    acf = torch.fft.irfft2(torch.conj(z_hat) * z_hat)\n",
    "    acf = torch.fft.fftshift(acf).mean(dim=0) / z[0].numel()\n",
    "    acf_r = acf.view(-1).cpu().detach().numpy()\n",
    "    lags_x, lags_y = torch.meshgrid(torch.arange(res) - res//2, torch.arange(res) - res//2)\n",
    "    lags_r = torch.sqrt(lags_x**2 + lags_y**2).view(-1).cpu().detach().numpy()\n",
    "\n",
    "    idx = np.argsort(lags_r)\n",
    "    lags_r = lags_r[idx]\n",
    "    acf_r = acf_r[idx]\n",
    "\n",
    "    bin_means, bin_edges, binnumber = binned_statistic(lags_r, acf_r, 'mean', bins=np.linspace(0.0, res, 50))\n",
    "    return bin_edges[:-1], bin_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ebf3616",
   "metadata": {
    "id": "3ebf3616"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np\n",
    "import random\n",
    "def LoadDataBatches(batchNumber, batches=4, isNormalized=True, isTrain=True, percent=1):\n",
    "    if(isTrain):\n",
    "        x_train = torch.load('/scratch/gilbreth/hviswan/x_train_vimeo_256_diff_type.pt')\n",
    "        y_train = torch.load('/scratch/gilbreth/hviswan/y_train_vimeo_256_diff_type.pt')\n",
    "        ntrain = x_train.shape[0]\n",
    "        \n",
    "        ntrain_slice = int(ntrain*percent)\n",
    "        if(ntrain_slice < ntrain-30):\n",
    "            start_index = min(0, random.randint(0, ntrain-ntrain_slice-30))\n",
    "        else:\n",
    "            start_index = 0\n",
    "        x_train = x_train[0+start_index:start_index+ntrain_slice]\n",
    "        x_train = torch.permute(x_train, (1, 0,2,3,4))\n",
    "        print(x_train.shape)\n",
    "        #ntrain = x_train.shape[1]\n",
    "        y_train = y_train.reshape(ntrain, x_train.shape[2], x_train.shape[3], x_train.shape[4])\n",
    "        y_train = y_train[0+start_index:ntrain_slice+start_index]\n",
    "        print(x_train.shape)\n",
    "        print(y_train.shape)\n",
    "        if(isNormalized):\n",
    "            y_normalizer = MinMaxNormalizer(y_train)\n",
    "            x_normalizer = MinMaxNormalizer(x_train)\n",
    "            x_train = x_normalizer.encode(x_train)\n",
    "            y_train = y_normalizer.encode(y_train)\n",
    "        train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train[0], x_train[1], y_train), batch_size=batches, shuffle=True)\n",
    "        if(isNormalized):\n",
    "            return train_loader, y_normalizer\n",
    "        return train_loader\n",
    "    else:\n",
    "        x_test = torch.load('/scratch/gilbreth/hviswan/x_test_vimeo_256_diff_type.pt')\n",
    "        y_test = torch.load('/scratch/gilbreth/hviswan/y_test_vimeo_256_diff_type.pt')\n",
    "        ntest = x_test.shape[0]\n",
    "        #x_test = x_test[0: 500]\n",
    "        x_test = torch.permute(x_test, (1, 0,2,3,4))\n",
    "        \n",
    "        y_test = y_test.reshape(ntest, x_test.shape[2], x_test.shape[3], x_test.shape[4])\n",
    "        #y_test = y_test[0:500]\n",
    "        print(x_test.shape)\n",
    "        print(y_test.shape)\n",
    "        if(isNormalized):\n",
    "            y_normalizer = MinMaxNormalizer(y_test)\n",
    "            x_normalizer = MinMaxNormalizer(x_test)\n",
    "            x_test = x_normalizer.encode(x_test)\n",
    "            y_test = y_normalizer.encode(y_test)\n",
    "        test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test[0], x_test[1], y_test), batch_size=batches, shuffle=True)\n",
    "        if(isNormalized):\n",
    "            return test_loader, y_normalizer\n",
    "        return test_loader\n",
    "#LoadDataBatches(2, batches=10, isNormalized=False, percent=1)\n",
    "#LoadDataBatches(2, batches=10, isNormalized=False, percent=1, isTrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "modern-party",
   "metadata": {
    "id": "modern-party"
   },
   "outputs": [],
   "source": [
    "def kernel(in_chan=2, up_dim=32):\n",
    "    \"\"\"\n",
    "        Kernel network apply on grid\n",
    "    \"\"\"\n",
    "    layers = nn.Sequential(\n",
    "                nn.Linear(in_chan, up_dim, bias=True), torch.nn.GELU(),\n",
    "                nn.Linear(up_dim, up_dim, bias=True), torch.nn.GELU(),\n",
    "                nn.Linear(up_dim, 1, bias=False)\n",
    "            )\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ef624f",
   "metadata": {
    "id": "12ef624f"
   },
   "source": [
    "### Generator operator and Discriminator functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "binding-thesis",
   "metadata": {
    "id": "binding-thesis"
   },
   "outputs": [],
   "source": [
    "class SpectralConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dim1, dim2,modes1 = None, modes2 = None):\n",
    "        super(SpectralConv2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "        in_channels = int(in_channels)\n",
    "        out_channels = int(out_channels)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.dim1 = dim1 #output dimensions\n",
    "        self.dim2 = dim2\n",
    "        #self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "        #self.dropout = nn.Dropout(p=0.42)\n",
    "        if modes1 is not None:\n",
    "            self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "            self.modes2 = modes2\n",
    "        else:\n",
    "            self.modes1 = dim1//2 + 1 #if not given take the highest number of modes can be taken\n",
    "            self.modes2 = dim2//2 \n",
    "        self.scale = (1 / (2*in_channels))**(1.0/2.0)\n",
    "        self.weights1 = nn.Parameter(self.scale * (torch.randn(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat)))\n",
    "        self.weights2 = nn.Parameter(self.scale * (torch.randn(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat)))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul2d(self, input, weights):\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "\n",
    "    def forward(self, x, dim1 = None,dim2 = None):\n",
    "        if dim1 is not None:\n",
    "            self.dim1 = dim1\n",
    "            self.dim2 = dim2\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfft2(x)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels,  self.dim1, self.dim2//2 + 1 , dtype=torch.cfloat, device=x.device)\n",
    "        #print(\"Out FT Shape = \", out_ft.shape)\n",
    "        #print(\"x_ft Shape = \", x_ft.shape)\n",
    "        #print(\"Modes1 = \", self.modes1, \" Modes2 = \", self.modes2, \" self.dim1 = \", self.dim1, \" self.dim2//2 + 1 = \", self.dim2//2+1)\n",
    "        out_ft[:, :, :self.modes1, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfft2(out_ft, s=(self.dim1, self.dim2))\n",
    "        #x = self.dropout(x)\n",
    "        #x = self.batch_norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class pointwise_op(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel,dim1, dim2):\n",
    "        super(pointwise_op,self).__init__()\n",
    "        self.conv = nn.Conv2d(int(in_channel), int(out_channel), 1)\n",
    "        self.dim1 = int(dim1)\n",
    "        self.dim2 = int(dim2)\n",
    "        #self.alpha_drop = nn.AlphaDropout(p=0.42)\n",
    "        #self.layer_norm = nn.LayerNorm([int(out_channel), self.dim1, self.dim2])\n",
    "        #self.batch_norm = nn.BatchNorm2d(int(out_channel))\n",
    "        #self.upsample = nn.Upsample(size=[self.dim1, self.dim2], mode='bicubic', align_corners=True)\n",
    "\n",
    "    def forward(self,x, dim1 = None, dim2 = None):\n",
    "        if dim1 is None:\n",
    "            dim1 = self.dim1\n",
    "            dim2 = self.dim2\n",
    "        x_out = self.conv(x)\n",
    "        x_out = torch.nn.functional.interpolate(x_out, size = (dim1, dim2),mode = 'bicubic',align_corners=True)\n",
    "        #x_out = self.upsample(x_out)\n",
    "        #x_out = self.alpha_drop(x_out)\n",
    "        #x_out = self.layer_norm(x_out)\n",
    "        #x_out = self.batch_norm(x_out)\n",
    "        return x_out\n",
    "\n",
    "class UNO(nn.Module):\n",
    "    def __init__(self, in_d_co_domain, d_co_domain, pad = 0, factor = 16/4):\n",
    "        super(UNO, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the coefficient function and locations (a(x, y), x, y)\n",
    "        input shape: (batchsize, x=s, y=s, c=3)\n",
    "        output: the solution \n",
    "        output shape: (batchsize, x=s, y=s, c=1)\n",
    "        \"\"\"\n",
    "        self.in_d_co_domain = in_d_co_domain # input channel\n",
    "        self.d_co_domain = d_co_domain \n",
    "        self.factor = factor\n",
    "        self.factor2 = factor/4\n",
    "        self.padding = pad  # pad the domain if input is non-periodic\n",
    "        self.reduceColor = nn.Conv2d(3, 1, kernel_size=1)\n",
    "        self.interpolation = nn.Conv2d(3, 3, kernel_size=1)\n",
    "\n",
    "        self.fc0 = nn.Linear(self.in_d_co_domain, self.d_co_domain) # input channel is 3: (a(x, y), x, y)\n",
    "\n",
    "        self.conv0 = SpectralConv2d(self.d_co_domain, 4*factor*self.d_co_domain, 16, 16, 32, 32)\n",
    "\n",
    "        self.conv1 = SpectralConv2d(4*factor*self.d_co_domain, 8*factor*self.d_co_domain, 16, 16, 26,26)\n",
    "\n",
    "        self.conv2 = SpectralConv2d(8*factor*self.d_co_domain, 16*factor*self.d_co_domain, 8, 8,13,13)\n",
    "        \n",
    "        self.conv2_1 = SpectralConv2d(16*factor*self.d_co_domain, 32*factor*self.d_co_domain, 4, 4,7,7)\n",
    "        \n",
    "        self.conv2_9 = SpectralConv2d(32*factor*self.d_co_domain, 16*factor*self.d_co_domain, 8, 8,7,7)\n",
    "    \n",
    "\n",
    "        self.conv3 = SpectralConv2d(32*factor*self.d_co_domain, 8*factor*self.d_co_domain, 16, 16,13,13)\n",
    "\n",
    "        self.conv4 = SpectralConv2d(16*factor*self.d_co_domain, 4*factor*self.d_co_domain, 32, 32,26,26)\n",
    "\n",
    "        self.conv5 = SpectralConv2d(8*factor*self.d_co_domain, self.d_co_domain, 48, 48,32,32) # will be reshaped\n",
    "\n",
    "        self.w0 = pointwise_op(self.d_co_domain,4*factor*self.d_co_domain,75, 75) #\n",
    "        \n",
    "        self.w1 = pointwise_op(4*factor*self.d_co_domain, 8*factor*self.d_co_domain, 50, 50) #\n",
    "        \n",
    "        self.w2 = pointwise_op(8*factor*self.d_co_domain, 16*factor*self.d_co_domain, 25, 25) #\n",
    "        \n",
    "        self.w2_1 = pointwise_op(16*factor*self.d_co_domain, 32*factor*self.d_co_domain, 12, 12)\n",
    "        \n",
    "        #VAE Code begin\n",
    "        self.linearFlatten1 = nn.Linear(384*12*12, 128)\n",
    "        self.linearFlatten2 = nn.Linear(128, 4)\n",
    "        self.linearFlatten3 = nn.Linear(128, 4)\n",
    "        self.softmax1 = nn.Softmax(dim=1)\n",
    "        self.N = torch.distributions.Normal(0, 10)\n",
    "        self.N.loc = self.N.loc.cuda()\n",
    "        self.N.scale = self.N.scale.cuda()\n",
    "        self.kl = 0\n",
    "        self.linearUnflatten = nn.Linear(4, 128)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.linearUnflatten2 = nn.Linear(128, 384*12*12)\n",
    "        self.relu2 = nn.ReLU(True)\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(384, 12, 12))\n",
    "        #VAE Code End\n",
    "        \n",
    "        self.w2_9 = pointwise_op(32*factor*self.d_co_domain, 16*factor*self.d_co_domain, 25, 25)\n",
    "        \n",
    "        self.w3 = pointwise_op(32*factor*self.d_co_domain, 8*factor*self.d_co_domain, 50, 50) #\n",
    "        \n",
    "        self.w4 = pointwise_op(16*factor*self.d_co_domain, 4*factor*self.d_co_domain, 75, 75)\n",
    "        \n",
    "        self.w5 = pointwise_op(8*factor*self.d_co_domain, self.d_co_domain, 100, 100) # will be reshaped\n",
    "\n",
    "        self.fc1 = nn.Linear(2*self.d_co_domain, 4*self.d_co_domain)\n",
    "        self.fc2 = nn.Linear(4*self.d_co_domain, 3)\n",
    "        self.increaseColor = nn.Conv2d(1, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        grid = self.get_grid(x[0].shape, x.device)\n",
    "        #print(grid.shape)\n",
    "        \n",
    "        #print(x[0].shape)\n",
    "        #x_l = torch.cat((x[0], grid), dim=-1).cuda()\n",
    "        #x_r = torch.cat((x[1], grid), dim=-1).cuda()\n",
    "\n",
    "        x_l = x[0].permute(0,3,1,2)\n",
    "        x_r = x[1].permute(0,3,1,2)\n",
    "        #print(x_l.shape)\n",
    "        x1 = self.interpolation(x_l)\n",
    "        x2 = self.interpolation(x_r)\n",
    "        ##print(x1.shape)\n",
    "        x = x1 + x2\n",
    "        x3 = x\n",
    "        #x = self.reduceColor(x)\n",
    "        x = x.permute(0,2,3,1)\n",
    "        #x = torch.cat((x, grid), dim=-1).cuda()\n",
    "        #print(x.shape)\n",
    "        x_fc0 = self.fc0(x)\n",
    "\n",
    "        x_fc0 = F.gelu(x_fc0)\n",
    "        \n",
    "        x_fc0 = x_fc0.permute(0, 3, 1, 2)\n",
    "        \n",
    "        \n",
    "        x_fc0 = F.pad(x_fc0, [0,self.padding, 0,self.padding])\n",
    "        \n",
    "        D1,D2 = x_fc0.shape[-2],x_fc0.shape[-1]\n",
    "        \n",
    "        x1_c0 = self.conv0(x_fc0,int(D1*self.factor2),int(D2*self.factor2))\n",
    "        x2_c0 = self.w0(x_fc0,int(D1*self.factor2),int(D2*self.factor2))\n",
    "        x_c0 = x1_c0 + x2_c0\n",
    "        x_c0 = F.gelu(x_c0)\n",
    "\n",
    "        x1_c1 = self.conv1(x_c0 ,D1//2,D2//2)\n",
    "        x2_c1 = self.w1(x_c0 ,D1//2,D2//2)\n",
    "        x_c1 = x1_c1 + x2_c1\n",
    "        x_c1 = F.gelu(x_c1)\n",
    "\n",
    "        x1_c2 = self.conv2(x_c1 ,D1//4,D2//4)\n",
    "        x2_c2 = self.w2(x_c1 ,D1//4,D2//4)\n",
    "        x_c2 = x1_c2 + x2_c2\n",
    "        x_c2 = F.gelu(x_c2 )\n",
    "\n",
    "        x1_c2_1 = self.conv2_1(x_c2,D1//8,D2//8)\n",
    "        x2_c2_1 = self.w2_1(x_c2,D1//8,D2//8)\n",
    "        x_c2_1 = x1_c2_1 + x2_c2_1\n",
    "        x_c2_1 = F.gelu(x_c2_1)\n",
    "        \"\"\"\n",
    "        #Variational Autoencoder part - Extract mu and sigma\n",
    "        x_c2_1 = torch.flatten(x_c2_1, start_dim=1)\n",
    "        x_c2_1 = self.linearFlatten1(x_c2_1)\n",
    "        mu = self.linearFlatten2(x_c2_1)\n",
    "        sigma = self.linearFlatten3(x_c2_1)\n",
    "        std = torch.exp(0.5*sigma)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = sigma*self.N.sample(mu.shape) + mu\n",
    "        #z = mu + std*eps\n",
    "\n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n",
    "        z = self.linearUnflatten(z)\n",
    "        z = self.linearUnflatten2(z)\n",
    "        x_c2_1 = self.unflatten(z)\n",
    "        #print(\"x_c2_1 shape = \", x_c2_1.shape)\n",
    "        #Variational Autoencoder -end\n",
    "        \"\"\"\n",
    "        \n",
    "        x1_c2_9 = self.conv2_9(x_c2_1,D1//4,D2//4)\n",
    "        x2_c2_9 = self.w2_9(x_c2_1,D1//4,D2//4)\n",
    "        x_c2_9 = x1_c2_9 + x2_c2_9\n",
    "        x_c2_9 = F.gelu(x_c2_9)\n",
    "        x_c2_9 = torch.cat([x_c2_9, x_c2], dim=1)\n",
    "\n",
    "        x1_c3 = self.conv3(x_c2_9,D1//2,D2//2)\n",
    "        x2_c3 = self.w3(x_c2_9,D1//2,D2//2)\n",
    "        x_c3 = x1_c3 + x2_c3\n",
    "        x_c3 = F.gelu(x_c3)\n",
    "        x_c3 = torch.cat([x_c3, x_c1], dim=1)\n",
    "\n",
    "        x1_c4 = self.conv4(x_c3,int(D1*self.factor2),int(D2*self.factor2))\n",
    "        x2_c4 = self.w4(x_c3,int(D1*self.factor2),int(D2*self.factor2))\n",
    "        x_c4 = x1_c4 + x2_c4\n",
    "        x_c4 = F.gelu(x_c4)\n",
    "        x_c4 = torch.cat([x_c4, x_c0], dim=1)\n",
    "\n",
    "        x1_c5 = self.conv5(x_c4,D1,D2)\n",
    "        x2_c5 = self.w5(x_c4,D1,D2)\n",
    "        x_c5 = x1_c5 + x2_c5\n",
    "        x_c5 = F.gelu(x_c5)\n",
    "\n",
    "\n",
    "        x_c5 = torch.cat([x_c5, x_fc0], dim=1)\n",
    "        if self.padding!=0:\n",
    "            x_c5 = x_c5[..., :-self.padding, :-self.padding]\n",
    "\n",
    "        x_c5 = x_c5.permute(0, 2, 3, 1)\n",
    "        \n",
    "        x_fc1 = self.fc1(x_c5)\n",
    "        x_fc1 = F.gelu(x_fc1)\n",
    "        \n",
    "        x_out = self.fc2(x_fc1)\n",
    "        \n",
    "        #x3 = torch.permute(x3, (0, 2,3,1))\n",
    "        x3 = (x_l+x_r)/2\n",
    "        #print(x3.shape)\n",
    "        x3 = torch.permute(x3, (0,2,3,1))\n",
    "        #x_fin = x_out + x3\n",
    "        #x_fin = torch.permute(x_fin, (0, 3, 1, 2))\n",
    "        #x_out = self.interpolation(x_fin)\n",
    "        #x_out = torch.permute(x_out, (0, 3, 1, 2))\n",
    "        #print(x_out.shape)\n",
    "        #x_out = self.increaseColor(x_out)\n",
    "        return x_out\n",
    "    \n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3827bf34",
   "metadata": {
    "id": "3827bf34"
   },
   "outputs": [],
   "source": [
    "class UNO_vanilla(nn.Module):\n",
    "    def __init__(self, in_d_co_domain, d_co_domain, pad = 0, factor = 16/4):\n",
    "        super(UNO_vanilla, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the coefficient function and locations (a(x, y), x, y)\n",
    "        input shape: (batchsize, x=s, y=s, c=3)\n",
    "        output: the solution \n",
    "        output shape: (batchsize, x=s, y=s, c=1)\n",
    "        \"\"\"\n",
    "        self.in_d_co_domain = in_d_co_domain # input channel\n",
    "        self.d_co_domain = d_co_domain \n",
    "        self.factor = factor\n",
    "        self.factor2 = factor/4\n",
    "        self.padding = pad  # pad the domain if input is non-periodic\n",
    "\n",
    "        self.fc0 = nn.Linear(self.in_d_co_domain, self.d_co_domain) # input channel is 3: (a(x, y), x, y)\n",
    "\n",
    "        self.conv0 = SpectralConv2d(self.d_co_domain, 4*factor*self.d_co_domain, 16, 16, 42, 42)\n",
    "\n",
    "        self.conv1 = SpectralConv2d(4*factor*self.d_co_domain, 8*factor*self.d_co_domain, 16, 16, 21,21)\n",
    "\n",
    "        self.conv2 = SpectralConv2d(8*factor*self.d_co_domain, 16*factor*self.d_co_domain, 8, 8,10,10)\n",
    "        \n",
    "        self.conv2_1 = SpectralConv2d(16*factor*self.d_co_domain, 32*factor*self.d_co_domain, 4, 4,5,5)\n",
    "        \n",
    "        self.conv2_9 = SpectralConv2d(32*factor*self.d_co_domain, 16*factor*self.d_co_domain, 8, 8,5,5)\n",
    "    \n",
    "\n",
    "        self.conv3 = SpectralConv2d(32*factor*self.d_co_domain, 8*factor*self.d_co_domain, 16, 16,10,10)\n",
    "\n",
    "        self.conv4 = SpectralConv2d(16*factor*self.d_co_domain, 4*factor*self.d_co_domain, 32, 32,21,21)\n",
    "\n",
    "        self.conv5 = SpectralConv2d(8*factor*self.d_co_domain, self.d_co_domain, 48, 48,42,42) # will be reshaped\n",
    "\n",
    "        self.w0 = pointwise_op(self.d_co_domain,4*factor*self.d_co_domain,75, 75) #\n",
    "        \n",
    "        self.w1 = pointwise_op(4*factor*self.d_co_domain, 8*factor*self.d_co_domain, 50, 50) #\n",
    "        \n",
    "        self.w2 = pointwise_op(8*factor*self.d_co_domain, 16*factor*self.d_co_domain, 25, 25) #\n",
    "        \n",
    "        self.w2_1 = pointwise_op(16*factor*self.d_co_domain, 32*factor*self.d_co_domain, 12, 12)\n",
    "\n",
    "        self.w2_9 = pointwise_op(32*factor*self.d_co_domain, 16*factor*self.d_co_domain, 25, 25)\n",
    "        \n",
    "        self.w3 = pointwise_op(32*factor*self.d_co_domain, 8*factor*self.d_co_domain, 50, 50) #\n",
    "        \n",
    "        self.w4 = pointwise_op(16*factor*self.d_co_domain, 4*factor*self.d_co_domain, 75, 75)\n",
    "        \n",
    "        self.w5 = pointwise_op(8*factor*self.d_co_domain, self.d_co_domain, 100, 100) # will be reshaped\n",
    "\n",
    "        self.fc1 = nn.Linear(2*self.d_co_domain, 4*self.d_co_domain)\n",
    "        self.fc2 = nn.Linear(4*self.d_co_domain, self.d_co_domain)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        grid = self.get_grid(x[0].shape, x.device)\n",
    "        x = x.permute(0,2,3,1)\n",
    "\n",
    "        x_fc0 = self.fc0(x)\n",
    "\n",
    "        x_fc0 = F.gelu(x_fc0)\n",
    "        \n",
    "        x_fc0 = x_fc0.permute(0, 3, 1, 2)\n",
    "        \n",
    "        \n",
    "        x_fc0 = F.pad(x_fc0, [0,self.padding, 0,self.padding])\n",
    "        \n",
    "        D1,D2 = x_fc0.shape[-2],x_fc0.shape[-1]\n",
    "        \n",
    "        x1_c0 = self.conv0(x_fc0,int(D1*self.factor2),int(D2*self.factor2))\n",
    "        x2_c0 = self.w0(x_fc0,int(D1*self.factor2),int(D2*self.factor2))\n",
    "        x_c0 = x1_c0 + x2_c0\n",
    "        x_c0 = F.gelu(x_c0)\n",
    "\n",
    "        x1_c1 = self.conv1(x_c0 ,D1//2,D2//2)\n",
    "        x2_c1 = self.w1(x_c0 ,D1//2,D2//2)\n",
    "        x_c1 = x1_c1 + x2_c1\n",
    "        x_c1 = F.gelu(x_c1)\n",
    "\n",
    "        x1_c2 = self.conv2(x_c1 ,D1//4,D2//4)\n",
    "        x2_c2 = self.w2(x_c1 ,D1//4,D2//4)\n",
    "        x_c2 = x1_c2 + x2_c2\n",
    "        x_c2 = F.gelu(x_c2 )\n",
    "\n",
    "        x1_c2_1 = self.conv2_1(x_c2,D1//8,D2//8)\n",
    "        x2_c2_1 = self.w2_1(x_c2,D1//8,D2//8)\n",
    "        x_c2_1 = x1_c2_1 + x2_c2_1\n",
    "        x_c2_1 = F.gelu(x_c2_1)\n",
    "\n",
    "        \n",
    "        x1_c2_9 = self.conv2_9(x_c2_1,D1//4,D2//4)\n",
    "        x2_c2_9 = self.w2_9(x_c2_1,D1//4,D2//4)\n",
    "        x_c2_9 = x1_c2_9 + x2_c2_9\n",
    "        x_c2_9 = F.gelu(x_c2_9)\n",
    "        x_c2_9 = torch.cat([x_c2_9, x_c2], dim=1)\n",
    "\n",
    "        x1_c3 = self.conv3(x_c2_9,D1//2,D2//2)\n",
    "        x2_c3 = self.w3(x_c2_9,D1//2,D2//2)\n",
    "        x_c3 = x1_c3 + x2_c3\n",
    "        x_c3 = F.gelu(x_c3)\n",
    "        x_c3 = torch.cat([x_c3, x_c1], dim=1)\n",
    "\n",
    "        x1_c4 = self.conv4(x_c3,int(D1*self.factor2),int(D2*self.factor2))\n",
    "        x2_c4 = self.w4(x_c3,int(D1*self.factor2),int(D2*self.factor2))\n",
    "        x_c4 = x1_c4 + x2_c4\n",
    "        x_c4 = F.gelu(x_c4)\n",
    "        x_c4 = torch.cat([x_c4, x_c0], dim=1)\n",
    "\n",
    "        x1_c5 = self.conv5(x_c4,D1,D2)\n",
    "        x2_c5 = self.w5(x_c4,D1,D2)\n",
    "        x_c5 = x1_c5 + x2_c5\n",
    "        x_c5 = F.gelu(x_c5)\n",
    "\n",
    "\n",
    "        x_c5 = torch.cat([x_c5, x_fc0], dim=1)\n",
    "        if self.padding!=0:\n",
    "            x_c5 = x_c5[..., :-self.padding, :-self.padding]\n",
    "\n",
    "        x_c5 = x_c5.permute(0, 2, 3, 1)\n",
    "        \n",
    "        x_fc1 = self.fc1(x_c5)\n",
    "        x_fc1 = F.gelu(x_fc1)\n",
    "        \n",
    "        x_out = self.fc2(x_fc1)\n",
    "        x_out = x_out.permute(0,3,1,2)\n",
    "        x_out = F.gelu(x_out)\n",
    "        return x_out\n",
    "    \n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43f62f8b",
   "metadata": {
    "id": "43f62f8b"
   },
   "outputs": [],
   "source": [
    "class SpectralConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dim1, dim2,modes1 = None, modes2 = None):\n",
    "        super(SpectralConv2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "        in_channels = int(in_channels)\n",
    "        out_channels = int(out_channels)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.dim1 = dim1 #output dimensions\n",
    "        self.dim2 = dim2\n",
    "        #self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "        #self.dropout = nn.Dropout(p=0.42)\n",
    "        if modes1 is not None:\n",
    "            self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "            self.modes2 = modes2\n",
    "        else:\n",
    "            self.modes1 = dim1//2 + 1 #if not given take the highest number of modes can be taken\n",
    "            self.modes2 = dim2//2 \n",
    "        self.scale = (1 / (2*in_channels))**(1.0/2.0)\n",
    "        self.weights1 = nn.Parameter(self.scale * (torch.randn(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat)))\n",
    "        self.weights2 = nn.Parameter(self.scale * (torch.randn(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat)))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul2d(self, input, weights):\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "\n",
    "    def forward(self, x, dim1 = None,dim2 = None):\n",
    "        if dim1 is not None:\n",
    "            self.dim1 = dim1\n",
    "            self.dim2 = dim2\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfft2(x)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels,  self.dim1, self.dim2//2 + 1 , dtype=torch.cfloat, device=x.device)\n",
    "        #print(\"Out FT Shape = \", out_ft.shape)\n",
    "        #print(\"x_ft Shape = \", x_ft.shape)\n",
    "        #print(\"Modes1 = \", self.modes1, \" Modes2 = \", self.modes2, \" self.dim1 = \", self.dim1, \" self.dim2//2 + 1 = \", self.dim2//2+1)\n",
    "        out_ft[:, :, :self.modes1, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfft2(out_ft, s=(self.dim1, self.dim2))\n",
    "        #x = self.dropout(x)\n",
    "        #x = self.batch_norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class pointwise_op(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel,dim1, dim2):\n",
    "        super(pointwise_op,self).__init__()\n",
    "        self.conv = nn.Conv2d(int(in_channel), int(out_channel), 1)\n",
    "        self.dim1 = int(dim1)\n",
    "        self.dim2 = int(dim2)\n",
    "        #self.alpha_drop = nn.AlphaDropout(p=0.42)\n",
    "        #self.layer_norm = nn.LayerNorm([int(out_channel), self.dim1, self.dim2])\n",
    "        #self.batch_norm = nn.BatchNorm2d(int(out_channel))\n",
    "        #self.upsample = nn.Upsample(size=[self.dim1, self.dim2], mode='bicubic', align_corners=True)\n",
    "\n",
    "    def forward(self,x, dim1 = None, dim2 = None):\n",
    "        if dim1 is None:\n",
    "            dim1 = self.dim1\n",
    "            dim2 = self.dim2\n",
    "        x_out = self.conv(x)\n",
    "        x_out = torch.nn.functional.interpolate(x_out, size = (dim1, dim2),mode = 'bicubic',align_corners=True)\n",
    "        #x_out = self.upsample(x_out)\n",
    "        #x_out = self.alpha_drop(x_out)\n",
    "        #x_out = self.layer_norm(x_out)\n",
    "        #x_out = self.batch_norm(x_out)\n",
    "        return x_out\n",
    "\n",
    "class UNO(nn.Module):\n",
    "    def __init__(self, in_d_co_domain, d_co_domain, pad = 0, factor = 16/4):\n",
    "        super(UNO, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the coefficient function and locations (a(x, y), x, y)\n",
    "        input shape: (batchsize, x=s, y=s, c=3)\n",
    "        output: the solution \n",
    "        output shape: (batchsize, x=s, y=s, c=1)\n",
    "        \"\"\"\n",
    "        self.in_d_co_domain = in_d_co_domain # input channel\n",
    "        self.d_co_domain = d_co_domain \n",
    "        self.factor = factor\n",
    "        self.factor2 = factor/4\n",
    "        self.padding = pad  # pad the domain if input is non-periodic\n",
    "        self.reduceColor = nn.Conv2d(3, 1, kernel_size=1)\n",
    "        self.interpolation = nn.Conv2d(3, 3, kernel_size=1)\n",
    "\n",
    "        self.fc0 = nn.Linear(self.in_d_co_domain, self.d_co_domain) # input channel is 3: (a(x, y), x, y)\n",
    "\n",
    "        self.conv0 = SpectralConv2d(self.d_co_domain, 4*factor*self.d_co_domain, 16, 16, 42, 42)\n",
    "\n",
    "        self.conv1 = SpectralConv2d(4*factor*self.d_co_domain, 8*factor*self.d_co_domain, 16, 16, 21,21)\n",
    "\n",
    "        self.conv2 = SpectralConv2d(8*factor*self.d_co_domain, 16*factor*self.d_co_domain, 8, 8,10,10)\n",
    "        \n",
    "        self.conv2_1 = SpectralConv2d(16*factor*self.d_co_domain, 32*factor*self.d_co_domain, 4, 4,5,5)\n",
    "        \n",
    "        self.conv2_9 = SpectralConv2d(32*factor*self.d_co_domain, 16*factor*self.d_co_domain, 8, 8,5,5)\n",
    "    \n",
    "\n",
    "        self.conv3 = SpectralConv2d(32*factor*self.d_co_domain, 8*factor*self.d_co_domain, 16, 16,10,10)\n",
    "\n",
    "        self.conv4 = SpectralConv2d(16*factor*self.d_co_domain, 4*factor*self.d_co_domain, 32, 32,21,21)\n",
    "\n",
    "        self.conv5 = SpectralConv2d(8*factor*self.d_co_domain, self.d_co_domain, 48, 48,42,42) # will be reshaped\n",
    "\n",
    "        self.w0 = pointwise_op(self.d_co_domain,4*factor*self.d_co_domain,75, 75) #\n",
    "        \n",
    "        self.w1 = pointwise_op(4*factor*self.d_co_domain, 8*factor*self.d_co_domain, 50, 50) #\n",
    "        \n",
    "        self.w2 = pointwise_op(8*factor*self.d_co_domain, 16*factor*self.d_co_domain, 25, 25) #\n",
    "        \n",
    "        self.w2_1 = pointwise_op(16*factor*self.d_co_domain, 32*factor*self.d_co_domain, 12, 12)\n",
    "        \"\"\"\n",
    "        #VAE Code begin\n",
    "        self.linearFlatten1 = nn.Linear(384*12*12, 128)\n",
    "        self.linearFlatten2 = nn.Linear(128, 4)\n",
    "        self.linearFlatten3 = nn.Linear(128, 4)\n",
    "        self.softmax1 = nn.Softmax(dim=1)\n",
    "        self.N = torch.distributions.Normal(0, 10)\n",
    "        self.N.loc = self.N.loc.cuda()\n",
    "        self.N.scale = self.N.scale.cuda()\n",
    "        self.kl = 0\n",
    "        self.linearUnflatten = nn.Linear(4, 128)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.linearUnflatten2 = nn.Linear(128, 384*12*12)\n",
    "        self.relu2 = nn.ReLU(True)\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(384, 12, 12))\n",
    "        #VAE Code End\n",
    "        \"\"\"\n",
    "        self.w2_9 = pointwise_op(32*factor*self.d_co_domain, 16*factor*self.d_co_domain, 25, 25)\n",
    "        \n",
    "        self.w3 = pointwise_op(32*factor*self.d_co_domain, 8*factor*self.d_co_domain, 50, 50) #\n",
    "        \n",
    "        self.w4 = pointwise_op(16*factor*self.d_co_domain, 4*factor*self.d_co_domain, 75, 75)\n",
    "        \n",
    "        self.w5 = pointwise_op(8*factor*self.d_co_domain, self.d_co_domain, 100, 100) # will be reshaped\n",
    "\n",
    "        self.fc1 = nn.Linear(2*self.d_co_domain, 4*self.d_co_domain)\n",
    "        self.fc2 = nn.Linear(4*self.d_co_domain, 3)\n",
    "        self.increaseColor = nn.Conv2d(1, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        grid = self.get_grid(x[0].shape, x.device)\n",
    "        #print(grid.shape)\n",
    "        \n",
    "        #print(x[0].shape)\n",
    "        #x_l = torch.cat((x[0], grid), dim=-1).cuda()\n",
    "        #x_r = torch.cat((x[1], grid), dim=-1).cuda()\n",
    "\n",
    "        x_l = x[0]\n",
    "        x_r = x[1]\n",
    "        #print(x_l.shape)\n",
    "        x1 = self.interpolation(x_l)\n",
    "        x2 = self.interpolation(x_r)\n",
    "        ##print(x1.shape)\n",
    "        x = x1 + x2\n",
    "        x3 = x\n",
    "        #x = self.reduceColor(x)\n",
    "        x = x.permute(0,2,3,1)\n",
    "        #x = torch.cat((x, grid), dim=-1).cuda()\n",
    "        #print(x.shape)\n",
    "        x_fc0 = self.fc0(x)\n",
    "\n",
    "        x_fc0 = F.gelu(x_fc0)\n",
    "        \n",
    "        x_fc0 = x_fc0.permute(0, 3, 1, 2)\n",
    "        \n",
    "        \n",
    "        x_fc0 = F.pad(x_fc0, [0,self.padding, 0,self.padding])\n",
    "        \n",
    "        D1,D2 = x_fc0.shape[-2],x_fc0.shape[-1]\n",
    "        \n",
    "        x1_c0 = self.conv0(x_fc0,int(D1*self.factor2),int(D2*self.factor2))\n",
    "        x2_c0 = self.w0(x_fc0,int(D1*self.factor2),int(D2*self.factor2))\n",
    "        x_c0 = x1_c0 + x2_c0\n",
    "        x_c0 = F.gelu(x_c0)\n",
    "\n",
    "        x1_c1 = self.conv1(x_c0 ,D1//2,D2//2)\n",
    "        x2_c1 = self.w1(x_c0 ,D1//2,D2//2)\n",
    "        x_c1 = x1_c1 + x2_c1\n",
    "        x_c1 = F.gelu(x_c1)\n",
    "\n",
    "        x1_c2 = self.conv2(x_c1 ,D1//4,D2//4)\n",
    "        x2_c2 = self.w2(x_c1 ,D1//4,D2//4)\n",
    "        x_c2 = x1_c2 + x2_c2\n",
    "        x_c2 = F.gelu(x_c2 )\n",
    "\n",
    "        x1_c2_1 = self.conv2_1(x_c2,D1//8,D2//8)\n",
    "        x2_c2_1 = self.w2_1(x_c2,D1//8,D2//8)\n",
    "        x_c2_1 = x1_c2_1 + x2_c2_1\n",
    "        x_c2_1 = F.gelu(x_c2_1)\n",
    "        \"\"\"\n",
    "        #Variational Autoencoder part - Extract mu and sigma\n",
    "        x_c2_1 = torch.flatten(x_c2_1, start_dim=1)\n",
    "        x_c2_1 = self.linearFlatten1(x_c2_1)\n",
    "        mu = self.linearFlatten2(x_c2_1)\n",
    "        sigma = self.linearFlatten3(x_c2_1)\n",
    "        std = torch.exp(0.5*sigma)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = sigma*self.N.sample(mu.shape) + mu\n",
    "        #z = mu + std*eps\n",
    "\n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n",
    "        z = self.linearUnflatten(z)\n",
    "        z = self.linearUnflatten2(z)\n",
    "        x_c2_1 = self.unflatten(z)\n",
    "        #print(\"x_c2_1 shape = \", x_c2_1.shape)\n",
    "        #Variational Autoencoder -end\n",
    "        \"\"\"\n",
    "        \n",
    "        x1_c2_9 = self.conv2_9(x_c2_1,D1//4,D2//4)\n",
    "        x2_c2_9 = self.w2_9(x_c2_1,D1//4,D2//4)\n",
    "        x_c2_9 = x1_c2_9 + x2_c2_9\n",
    "        x_c2_9 = F.gelu(x_c2_9)\n",
    "        x_c2_9 = torch.cat([x_c2_9, x_c2], dim=1)\n",
    "\n",
    "        x1_c3 = self.conv3(x_c2_9,D1//2,D2//2)\n",
    "        x2_c3 = self.w3(x_c2_9,D1//2,D2//2)\n",
    "        x_c3 = x1_c3 + x2_c3\n",
    "        x_c3 = F.gelu(x_c3)\n",
    "        x_c3 = torch.cat([x_c3, x_c1], dim=1)\n",
    "\n",
    "        x1_c4 = self.conv4(x_c3,int(D1*self.factor2),int(D2*self.factor2))\n",
    "        x2_c4 = self.w4(x_c3,int(D1*self.factor2),int(D2*self.factor2))\n",
    "        x_c4 = x1_c4 + x2_c4\n",
    "        x_c4 = F.gelu(x_c4)\n",
    "        x_c4 = torch.cat([x_c4, x_c0], dim=1)\n",
    "\n",
    "        x1_c5 = self.conv5(x_c4,D1,D2)\n",
    "        x2_c5 = self.w5(x_c4,D1,D2)\n",
    "        x_c5 = x1_c5 + x2_c5\n",
    "        x_c5 = F.gelu(x_c5)\n",
    "\n",
    "\n",
    "        x_c5 = torch.cat([x_c5, x_fc0], dim=1)\n",
    "        if self.padding!=0:\n",
    "            x_c5 = x_c5[..., :-self.padding, :-self.padding]\n",
    "\n",
    "        x_c5 = x_c5.permute(0, 2, 3, 1)\n",
    "        \n",
    "        x_fc1 = self.fc1(x_c5)\n",
    "        x_fc1 = F.gelu(x_fc1)\n",
    "        \n",
    "        x_out = self.fc2(x_fc1)\n",
    "        \n",
    "        #x3 = torch.permute(x3, (0, 2,3,1))\n",
    "        x3 = (x_l+x_r)/2\n",
    "        #print(x3.shape)\n",
    "        x3 = torch.permute(x_out, (0,3,1,2))\n",
    "        #x_fin = x_out + x3\n",
    "        #x_fin = torch.permute(x_fin, (0, 3, 1, 2))\n",
    "        #x_out = self.interpolation(x_fin)\n",
    "        #x_out = torch.permute(x_out, (0, 3, 1, 2))\n",
    "        #print(x_out.shape)\n",
    "        #x_out = self.increaseColor(x_out)\n",
    "        return x3\n",
    "    \n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69fe9150",
   "metadata": {
    "id": "69fe9150"
   },
   "outputs": [],
   "source": [
    "class NIO(nn.Module):\n",
    "    def __init__(self, in_d_co_domain, d_co_domain, pad = 0, factor = 10/4):\n",
    "        super(NIO, self).__init__()\n",
    "        self.in_d_co_domain = in_d_co_domain # input channel\n",
    "        self.d_co_domain = d_co_domain \n",
    "        self.factor = factor\n",
    "        self.factor2 = factor/4\n",
    "        self.interpolation = nn.Conv2d(3, 3, kernel_size=1)\n",
    "        self.UNO_block_1 = UNO(self.in_d_co_domain, d_co_domain, pad=0, factor=16/4)\n",
    "        self.UNO_block_2 = UNO(self.in_d_co_domain, d_co_domain, pad=0, factor=16/4)\n",
    "        self.UNO_block_3 = UNO(self.in_d_co_domain, d_co_domain, pad=0, factor=16/4)\n",
    "        self.UNO_block_4 = UNO(self.in_d_co_domain, d_co_domain, pad=0, factor=16/4)\n",
    "        #self.UNO_block_5 = UNO(self.in_d_co_domain, d_co_domain, pad=0, factor=20/4)\n",
    "    def forward(self, x):\n",
    "        uno_1_out = self.UNO_block_1(x)\n",
    "        #print(uno_1_out.shape)\n",
    "        #print(x[0].shape)\n",
    "        #print(x.shape)\n",
    "        x_int_l = self.interpolation(x[0])\n",
    "        x_int_r = self.interpolation(x[1])\n",
    "        x_int = x_int_l + x_int_r\n",
    "        #x_int = torch.permute(x_int, (0,2,3,1))\n",
    "        y = torch.stack((x_int, uno_1_out))\n",
    "        \n",
    "        #y = x[0] + uno_1_out\n",
    "        #print(\"Y Shape = \", y.shape)\n",
    "        uno_2_out = self.UNO_block_2(y)\n",
    "        y = torch.stack((x_int, uno_2_out))\n",
    "        #y = x[1] + uno_2_out\n",
    "        uno_3_out = self.UNO_block_3(y)\n",
    "        y = torch.stack((x_int, uno_3_out))\n",
    "        uno_4_out = self.UNO_block_4(x)\n",
    "        #y = torch.stack((x_int, uno_4_out))\n",
    "        #x = self.UNO_block_5(y)\n",
    "        return uno_4_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "074b9438",
   "metadata": {
    "id": "074b9438"
   },
   "outputs": [],
   "source": [
    "class NIO_split_channel(nn.Module):\n",
    "    def __init__(self, in_d_co_domain, d_co_domain, pad=0, factor=3/4):\n",
    "        super(NIO_split_channel, self).__init__()\n",
    "        #self.UNO_block_red = UNO_vanilla(1, 1, pad=0, factor=16/4)\n",
    "        #self.UNO_block_green = UNO_vanilla(1, 1, pad=0, factor=16/4)\n",
    "        #self.UNO_block_blue = UNO_vanilla(1, 1, pad=0, factor=16/4)\n",
    "        self.UNO_block_1 = UNO_vanilla(6, 3, pad=0, factor=16/4)\n",
    "        self.UNO_block_2 = UNO_vanilla(9, 3, pad=0, factor=16/4)\n",
    "        #self.UNO_block_3 = UNO_vanilla(12, 3, pad=0, factor=16/4)\n",
    "        #self.UNO_block_4 = UNO_vanilla(15, 3, pad=0, factor=16/4)\n",
    "        #self.UNO_block_merge = UNO_vanilla(3, 3, pad=0, factor=16/4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_l = x[0].permute(0,3,1,2)\n",
    "        x_r = x[1].permute(0,3,1,2)\n",
    "        x_l_colors = x_l.permute(1,0,2,3)\n",
    "        x_r_colors = x_r.permute(1,0,2,3)\n",
    "        \n",
    "        input_stack_1 = torch.cat((x_l, x_r), dim=1)\n",
    "        \n",
    "        stack_out_1 = self.UNO_block_1(input_stack_1)\n",
    "\n",
    "        #print(input_stack.shape)\n",
    "        #input_sum = (x_l+x_r)/2.0\n",
    "        \n",
    "        #input_stack_2 = torch.cat((stack_out_1, input_stack_1), dim=1)\n",
    "        #print(input_stack_2.shape)\n",
    "        #stack_out_2 = self.UNO_block_2(input_stack_2)\n",
    "        \n",
    "        #input_stack_3 = torch.cat((stack_out_2, input_stack_2), dim=1)\n",
    "        #stack_out_3 = self.UNO_block_3(input_stack_3)\n",
    "        \n",
    "        #input_stack_4 = torch.cat((stack_out_3, input_stack_3), dim=1)\n",
    "        #stack_out_4 = self.UNO_block_4(input_stack_4)\n",
    "        \n",
    "        #add_out = self.UNO_block_add(input_sum)\n",
    "        #x_out = self.UNO_block_merge(stack_out_4)\n",
    "        \"\"\"\n",
    "        #print(x_l_colors.shape)\n",
    "        x_l_red = x_l_colors[0]\n",
    "        x_l_green = x_l_colors[1]\n",
    "        x_l_blue = x_l_colors[2]\n",
    "        \n",
    "        x_r_red = x_r_colors[0]\n",
    "        x_r_green = x_r_colors[1]\n",
    "        x_r_blue = x_r_colors[2]\n",
    "        \n",
    "        #x_red = torch.stack((x_l_red, x_r_red)).permute(1,0,2,3)\n",
    "        #x_green = torch.stack((x_l_green, x_r_green)).permute(1,0,2,3)\n",
    "        #x_blue = torch.stack((x_l_blue, x_r_blue)).permute(1,0,2,3)\n",
    "        x_red = (x_r_red + x_l_red).reshape(x.shape[1], 1, x.shape[2], x.shape[3])\n",
    "        x_green = (x_r_green + x_l_green).reshape(x.shape[1], 1, x.shape[2], x.shape[3])\n",
    "        x_blue = (x_l_blue + x_r_blue).reshape(x.shape[1], 1, x.shape[2], x.shape[3])\n",
    "        \n",
    "        r = self.UNO_block_red(x_red)\n",
    "        g = self.UNO_block_green(x_green)\n",
    "        b = self.UNO_block_blue(x_blue)\n",
    "        uno_x = torch.cat((r,g,b), dim=1) + 0.001*self.sub_mean(x_l)[0] + 0.001*self.sub_mean(x_r)[0]\n",
    "        x_out = self.UNO_block_merge(uno_x)\n",
    "        \"\"\"\n",
    "        return stack_out_1\n",
    "    \n",
    "    def sub_mean(self, x):\n",
    "        mean = x.mean(2, keepdim=True).mean(3, keepdim=True)\n",
    "        x -= mean\n",
    "        return x, mean\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f79d1c87",
   "metadata": {
    "id": "f79d1c87"
   },
   "outputs": [],
   "source": [
    "class NIO_fine(nn.Module):\n",
    "    def __init__(self, in_d_co_domain, d_co_domain, pad=0, factor=3/4):\n",
    "        super(NIO_fine, self).__init__()\n",
    "        self.in_d_co_domain = in_d_co_domain\n",
    "        self.d_co_domain = d_co_domain\n",
    "        self.factor = factor\n",
    "        self.factor2 = factor/4\n",
    "        self.UNO_block = UNO(self.in_d_co_domain, d_co_domain, pad=0, factor=16/4)\n",
    "        #self.UNO_block2 = UNO(self.in_d_co_domain, d_co_domain, pad=0, factor=12/4)\n",
    "        \"\"\"self.layer1 = nn.Conv2d(3, 3, 3, padding=1)\n",
    "        self.norm1 = nn.BatchNorm2d(3)\n",
    "        self.layer2 = nn.Conv2d(3, 3, 3, padding=1)\n",
    "        self.norm2 = nn.BatchNorm2d(3)\n",
    "        self.layer3 = nn.Conv2d(3, 3, 3, padding=1)\n",
    "        self.norm3 = nn.BatchNorm2d(3)\n",
    "        self.layer4 = nn.Conv2d(3, 3, 3, padding=1)\n",
    "        self.norm4 = nn.BatchNorm2d(3)\n",
    "        self.layer5 = nn.Conv2d(3, 3, 3, padding=1)\n",
    "        self.norm5 = nn.BatchNorm2d(3)\n",
    "        #self.layer2 = nn.Conv2d(3, 3, 3, padding=1)\"\"\"\n",
    "    def forward(self, x):\n",
    "        UNO_out = self.UNO_block(x)\n",
    "        #UNO_out = torch.stack((x[0], UNO_out))\n",
    "        #UNO_out = self.UNO_block2(UNO_out)\n",
    "        #UNO_out = torch.permute(UNO_out, (0,3, 1, 2))\n",
    "        #layer1out = self.norm1(self.layer1(UNO_out))\n",
    "        #layer2out = self.norm2(self.layer2(layer1out+UNO_out))\n",
    "        #layer3out = self.norm3(self.layer3(layer2out+layer1out))\n",
    "        #layer4out = self.norm4(self.layer4(layer3out+layer2out))\n",
    "        #layer5out = self.norm5(self.layer5(layer4out+layer3out))\n",
    "        #layer5out = torch.permute(layer5out, (0,2,3,1))\n",
    "        #output = layer5out\n",
    "        return UNO_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b2059e9",
   "metadata": {
    "id": "1b2059e9"
   },
   "outputs": [],
   "source": [
    "class NIO_huge(nn.Module):\n",
    "    def __init__(self, in_d_co_domain, d_co_domain, pad = 0, factor = 10/4):\n",
    "        super(NIO_huge, self).__init__()\n",
    "        self.in_d_co_domain = in_d_co_domain # input channel\n",
    "        self.d_co_domain = d_co_domain \n",
    "        self.factor = factor\n",
    "        self.factor2 = factor/4\n",
    "        self.interpolation = nn.Conv2d(3, 3, kernel_size=1)\n",
    "        self.UNO_block_1 = UNO(self.in_d_co_domain, d_co_domain, pad=0, factor=16/4)\n",
    "        self.UNO_block_2 = UNO(self.in_d_co_domain, d_co_domain, pad=0, factor=16/4)\n",
    "        self.UNO_block_3 = UNO(self.in_d_co_domain, d_co_domain, pad=0, factor=16/4)\n",
    "        self.UNO_block_4 = UNO(self.in_d_co_domain, d_co_domain, pad=0, factor=16/4)\n",
    "        self.UNO_block_5 = UNO(self.in_d_co_domain, d_co_domain, pad=0, factor=16/4)\n",
    "    def forward(self, x):\n",
    "        uno_1_out = self.UNO_block_1(x)\n",
    "        #print(uno_1_out.shape)\n",
    "        #print(x[0].shape)\n",
    "        #print(x.shape)\n",
    "        x_int_l = self.interpolation(x[0].permute(0,3,1,2))\n",
    "        x_int_r = self.interpolation(x[1].permute(0,3,1,2))\n",
    "        x_int = x_int_l + x_int_r\n",
    "        x_int = torch.permute(x_int, (0,2,3,1))\n",
    "        y = torch.stack((x_int, uno_1_out))\n",
    "        \n",
    "        #y = x[0] + uno_1_out\n",
    "        #print(\"Y Shape = \", y.shape)\n",
    "        uno_2_out = self.UNO_block_2(y)\n",
    "        y = torch.stack((x_int, uno_2_out))\n",
    "        #y = x[1] + uno_2_out\n",
    "        uno_3_out = self.UNO_block_3(y)\n",
    "        y = torch.stack((x_int, uno_3_out))\n",
    "        uno_4_out = self.UNO_block_4(x)\n",
    "        y = torch.stack((x_int, uno_4_out))\n",
    "        x = self.UNO_block_5(y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "worth-exhibit",
   "metadata": {
    "id": "worth-exhibit",
    "outputId": "28cd8bb9-7259-416b-a7bc-62fc6eb6f30b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "NIO                                      [10, 3, 85, 85]           --\n",
       "ââUNO: 1-1                               [10, 3, 85, 85]           10\n",
       "â    ââConv2d: 2-1                       [10, 3, 85, 85]           12\n",
       "â    ââConv2d: 2-2                       [10, 3, 85, 85]           (recursive)\n",
       "â    ââLinear: 2-3                       [10, 85, 85, 3]           12\n",
       "â    ââSpectralConv2d: 2-4               [10, 48, 85, 85]          508,032\n",
       "â    ââpointwise_op: 2-5                 [10, 48, 85, 85]          --\n",
       "â    â    ââConv2d: 3-1                  [10, 48, 85, 85]          192\n",
       "â    ââSpectralConv2d: 2-6               [10, 96, 42, 42]          4,064,256\n",
       "â    ââpointwise_op: 2-7                 [10, 96, 42, 42]          --\n",
       "â    â    ââConv2d: 3-2                  [10, 96, 85, 85]          4,704\n",
       "â    ââSpectralConv2d: 2-8               [10, 192, 21, 21]         3,686,400\n",
       "â    ââpointwise_op: 2-9                 [10, 192, 21, 21]         --\n",
       "â    â    ââConv2d: 3-3                  [10, 192, 42, 42]         18,624\n",
       "â    ââSpectralConv2d: 2-10              [10, 384, 10, 10]         3,686,400\n",
       "â    ââpointwise_op: 2-11                [10, 384, 10, 10]         --\n",
       "â    â    ââConv2d: 3-4                  [10, 384, 21, 21]         74,112\n",
       "â    ââSpectralConv2d: 2-12              [10, 192, 21, 21]         3,686,400\n",
       "â    ââpointwise_op: 2-13                [10, 192, 21, 21]         --\n",
       "â    â    ââConv2d: 3-5                  [10, 192, 10, 10]         73,920\n",
       "â    ââSpectralConv2d: 2-14              [10, 96, 42, 42]          7,372,800\n",
       "â    ââpointwise_op: 2-15                [10, 96, 42, 42]          --\n",
       "â    â    ââConv2d: 3-6                  [10, 96, 21, 21]          36,960\n",
       "â    ââSpectralConv2d: 2-16              [10, 48, 85, 85]          8,128,512\n",
       "â    ââpointwise_op: 2-17                [10, 48, 85, 85]          --\n",
       "â    â    ââConv2d: 3-7                  [10, 48, 42, 42]          9,264\n",
       "â    ââSpectralConv2d: 2-18              [10, 3, 85, 85]           1,016,064\n",
       "â    ââpointwise_op: 2-19                [10, 3, 85, 85]           --\n",
       "â    â    ââConv2d: 3-8                  [10, 3, 85, 85]           291\n",
       "â    ââLinear: 2-20                      [10, 85, 85, 12]          84\n",
       "â    ââLinear: 2-21                      [10, 85, 85, 3]           39\n",
       "ââConv2d: 1-2                            [10, 3, 85, 85]           12\n",
       "ââConv2d: 1-3                            [10, 3, 85, 85]           (recursive)\n",
       "ââUNO: 1-4                               [10, 3, 85, 85]           10\n",
       "â    ââConv2d: 2-22                      [10, 3, 85, 85]           12\n",
       "â    ââConv2d: 2-23                      [10, 3, 85, 85]           (recursive)\n",
       "â    ââLinear: 2-24                      [10, 85, 85, 3]           12\n",
       "â    ââSpectralConv2d: 2-25              [10, 48, 85, 85]          508,032\n",
       "â    ââpointwise_op: 2-26                [10, 48, 85, 85]          --\n",
       "â    â    ââConv2d: 3-9                  [10, 48, 85, 85]          192\n",
       "â    ââSpectralConv2d: 2-27              [10, 96, 42, 42]          4,064,256\n",
       "â    ââpointwise_op: 2-28                [10, 96, 42, 42]          --\n",
       "â    â    ââConv2d: 3-10                 [10, 96, 85, 85]          4,704\n",
       "â    ââSpectralConv2d: 2-29              [10, 192, 21, 21]         3,686,400\n",
       "â    ââpointwise_op: 2-30                [10, 192, 21, 21]         --\n",
       "â    â    ââConv2d: 3-11                 [10, 192, 42, 42]         18,624\n",
       "â    ââSpectralConv2d: 2-31              [10, 384, 10, 10]         3,686,400\n",
       "â    ââpointwise_op: 2-32                [10, 384, 10, 10]         --\n",
       "â    â    ââConv2d: 3-12                 [10, 384, 21, 21]         74,112\n",
       "â    ââSpectralConv2d: 2-33              [10, 192, 21, 21]         3,686,400\n",
       "â    ââpointwise_op: 2-34                [10, 192, 21, 21]         --\n",
       "â    â    ââConv2d: 3-13                 [10, 192, 10, 10]         73,920\n",
       "â    ââSpectralConv2d: 2-35              [10, 96, 42, 42]          7,372,800\n",
       "â    ââpointwise_op: 2-36                [10, 96, 42, 42]          --\n",
       "â    â    ââConv2d: 3-14                 [10, 96, 21, 21]          36,960\n",
       "â    ââSpectralConv2d: 2-37              [10, 48, 85, 85]          8,128,512\n",
       "â    ââpointwise_op: 2-38                [10, 48, 85, 85]          --\n",
       "â    â    ââConv2d: 3-15                 [10, 48, 42, 42]          9,264\n",
       "â    ââSpectralConv2d: 2-39              [10, 3, 85, 85]           1,016,064\n",
       "â    ââpointwise_op: 2-40                [10, 3, 85, 85]           --\n",
       "â    â    ââConv2d: 3-16                 [10, 3, 85, 85]           291\n",
       "â    ââLinear: 2-41                      [10, 85, 85, 12]          84\n",
       "â    ââLinear: 2-42                      [10, 85, 85, 3]           39\n",
       "ââUNO: 1-5                               [10, 3, 85, 85]           10\n",
       "â    ââConv2d: 2-43                      [10, 3, 85, 85]           12\n",
       "â    ââConv2d: 2-44                      [10, 3, 85, 85]           (recursive)\n",
       "â    ââLinear: 2-45                      [10, 85, 85, 3]           12\n",
       "â    ââSpectralConv2d: 2-46              [10, 48, 85, 85]          508,032\n",
       "â    ââpointwise_op: 2-47                [10, 48, 85, 85]          --\n",
       "â    â    ââConv2d: 3-17                 [10, 48, 85, 85]          192\n",
       "â    ââSpectralConv2d: 2-48              [10, 96, 42, 42]          4,064,256\n",
       "â    ââpointwise_op: 2-49                [10, 96, 42, 42]          --\n",
       "â    â    ââConv2d: 3-18                 [10, 96, 85, 85]          4,704\n",
       "â    ââSpectralConv2d: 2-50              [10, 192, 21, 21]         3,686,400\n",
       "â    ââpointwise_op: 2-51                [10, 192, 21, 21]         --\n",
       "â    â    ââConv2d: 3-19                 [10, 192, 42, 42]         18,624\n",
       "â    ââSpectralConv2d: 2-52              [10, 384, 10, 10]         3,686,400\n",
       "â    ââpointwise_op: 2-53                [10, 384, 10, 10]         --\n",
       "â    â    ââConv2d: 3-20                 [10, 384, 21, 21]         74,112\n",
       "â    ââSpectralConv2d: 2-54              [10, 192, 21, 21]         3,686,400\n",
       "â    ââpointwise_op: 2-55                [10, 192, 21, 21]         --\n",
       "â    â    ââConv2d: 3-21                 [10, 192, 10, 10]         73,920\n",
       "â    ââSpectralConv2d: 2-56              [10, 96, 42, 42]          7,372,800\n",
       "â    ââpointwise_op: 2-57                [10, 96, 42, 42]          --\n",
       "â    â    ââConv2d: 3-22                 [10, 96, 21, 21]          36,960\n",
       "â    ââSpectralConv2d: 2-58              [10, 48, 85, 85]          8,128,512\n",
       "â    ââpointwise_op: 2-59                [10, 48, 85, 85]          --\n",
       "â    â    ââConv2d: 3-23                 [10, 48, 42, 42]          9,264\n",
       "â    ââSpectralConv2d: 2-60              [10, 3, 85, 85]           1,016,064\n",
       "â    ââpointwise_op: 2-61                [10, 3, 85, 85]           --\n",
       "â    â    ââConv2d: 3-24                 [10, 3, 85, 85]           291\n",
       "â    ââLinear: 2-62                      [10, 85, 85, 12]          84\n",
       "â    ââLinear: 2-63                      [10, 85, 85, 3]           39\n",
       "ââUNO: 1-6                               [10, 3, 85, 85]           10\n",
       "â    ââConv2d: 2-64                      [10, 3, 85, 85]           12\n",
       "â    ââConv2d: 2-65                      [10, 3, 85, 85]           (recursive)\n",
       "â    ââLinear: 2-66                      [10, 85, 85, 3]           12\n",
       "â    ââSpectralConv2d: 2-67              [10, 48, 85, 85]          508,032\n",
       "â    ââpointwise_op: 2-68                [10, 48, 85, 85]          --\n",
       "â    â    ââConv2d: 3-25                 [10, 48, 85, 85]          192\n",
       "â    ââSpectralConv2d: 2-69              [10, 96, 42, 42]          4,064,256\n",
       "â    ââpointwise_op: 2-70                [10, 96, 42, 42]          --\n",
       "â    â    ââConv2d: 3-26                 [10, 96, 85, 85]          4,704\n",
       "â    ââSpectralConv2d: 2-71              [10, 192, 21, 21]         3,686,400\n",
       "â    ââpointwise_op: 2-72                [10, 192, 21, 21]         --\n",
       "â    â    ââConv2d: 3-27                 [10, 192, 42, 42]         18,624\n",
       "â    ââSpectralConv2d: 2-73              [10, 384, 10, 10]         3,686,400\n",
       "â    ââpointwise_op: 2-74                [10, 384, 10, 10]         --\n",
       "â    â    ââConv2d: 3-28                 [10, 384, 21, 21]         74,112\n",
       "â    ââSpectralConv2d: 2-75              [10, 192, 21, 21]         3,686,400\n",
       "â    ââpointwise_op: 2-76                [10, 192, 21, 21]         --\n",
       "â    â    ââConv2d: 3-29                 [10, 192, 10, 10]         73,920\n",
       "â    ââSpectralConv2d: 2-77              [10, 96, 42, 42]          7,372,800\n",
       "â    ââpointwise_op: 2-78                [10, 96, 42, 42]          --\n",
       "â    â    ââConv2d: 3-30                 [10, 96, 21, 21]          36,960\n",
       "â    ââSpectralConv2d: 2-79              [10, 48, 85, 85]          8,128,512\n",
       "â    ââpointwise_op: 2-80                [10, 48, 85, 85]          --\n",
       "â    â    ââConv2d: 3-31                 [10, 48, 42, 42]          9,264\n",
       "â    ââSpectralConv2d: 2-81              [10, 3, 85, 85]           1,016,064\n",
       "â    ââpointwise_op: 2-82                [10, 3, 85, 85]           --\n",
       "â    â    ââConv2d: 3-32                 [10, 3, 85, 85]           291\n",
       "â    ââLinear: 2-83                      [10, 85, 85, 12]          84\n",
       "â    ââLinear: 2-84                      [10, 85, 85, 3]           39\n",
       "==========================================================================================\n",
       "Total params: 129,468,364\n",
       "Trainable params: 129,468,364\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 179.60\n",
       "==========================================================================================\n",
       "Input size (MB): 1.73\n",
       "Forward/backward pass size (MB): 1003.25\n",
       "Params size (MB): 1032.26\n",
       "Estimated Total Size (MB): 2037.24\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = NIO(3, 3, pad=1).cuda()\n",
    "#NF = NIO_fine(3,4, pad=1).cuda()\n",
    "#nn_params = sum(p.numel() for p in G.parameters() if p.requires_grad)\n",
    "#print(\"Number generator parameters: \", nn_params)\n",
    "#randTensor = torch.rand((200,100,100,3), dtype=torch.float32).cuda()\n",
    "summary(G, input_size=(2,10, 3,85,85))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adbbeb2f",
   "metadata": {
    "id": "adbbeb2f"
   },
   "outputs": [],
   "source": [
    "def _fspecial_gauss_1d(size, sigma):\n",
    "    r\"\"\"Create 1-D gauss kernel\n",
    "    Args:\n",
    "        size (int): the size of gauss kernel\n",
    "        sigma (float): sigma of normal distribution\n",
    "    Returns:\n",
    "        torch.Tensor: 1D kernel (1 x 1 x size)\n",
    "    \"\"\"\n",
    "    coords = torch.arange(size, dtype=torch.float)\n",
    "    coords -= size // 2\n",
    "\n",
    "    g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))\n",
    "    g /= g.sum()\n",
    "\n",
    "    return g.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def gaussian_filter(input, win):\n",
    "    r\"\"\" Blur input with 1-D kernel\n",
    "    Args:\n",
    "        input (torch.Tensor): a batch of tensors to be blurred\n",
    "        window (torch.Tensor): 1-D gauss kernel\n",
    "    Returns:\n",
    "        torch.Tensor: blurred tensors\n",
    "    \"\"\"\n",
    "    assert all([ws == 1 for ws in win.shape[1:-1]]), win.shape\n",
    "    if len(input.shape) == 4:\n",
    "        conv = F.conv2d\n",
    "    elif len(input.shape) == 5:\n",
    "        conv = F.conv3d\n",
    "    else:\n",
    "        raise NotImplementedError(input.shape)\n",
    "\n",
    "    C = input.shape[1]\n",
    "    out = input\n",
    "    for i, s in enumerate(input.shape[2:]):\n",
    "        if s >= win.shape[-1]:\n",
    "            out = conv(out, weight=win.transpose(2 + i, -1), stride=1, padding=0, groups=C)\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"Skipping Gaussian Smoothing at dimension 2+{i} for input: {input.shape} and win size: {win.shape[-1]}\"\n",
    "            )\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _ssim(X, Y, data_range, win, size_average=True, K=(0.01, 0.03)):\n",
    "\n",
    "    r\"\"\" Calculate ssim index for X and Y\n",
    "    Args:\n",
    "        X (torch.Tensor): images\n",
    "        Y (torch.Tensor): images\n",
    "        win (torch.Tensor): 1-D gauss kernel\n",
    "        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n",
    "        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
    "    Returns:\n",
    "        torch.Tensor: ssim results.\n",
    "    \"\"\"\n",
    "    K1, K2 = K\n",
    "    # batch, channel, [depth,] height, width = X.shape\n",
    "    compensation = 1.0\n",
    "\n",
    "    C1 = (K1 * data_range) ** 2\n",
    "    C2 = (K2 * data_range) ** 2\n",
    "\n",
    "    win = win.to(X.device, dtype=X.dtype)\n",
    "\n",
    "    mu1 = gaussian_filter(X, win)\n",
    "    mu2 = gaussian_filter(Y, win)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = compensation * (gaussian_filter(X * X, win) - mu1_sq)\n",
    "    sigma2_sq = compensation * (gaussian_filter(Y * Y, win) - mu2_sq)\n",
    "    sigma12 = compensation * (gaussian_filter(X * Y, win) - mu1_mu2)\n",
    "\n",
    "    cs_map = (2 * sigma12 + C2) / (sigma1_sq + sigma2_sq + C2)  # set alpha=beta=gamma=1\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) / (mu1_sq + mu2_sq + C1)) * cs_map\n",
    "\n",
    "    ssim_per_channel = torch.flatten(ssim_map, 2).mean(-1)\n",
    "    cs = torch.flatten(cs_map, 2).mean(-1)\n",
    "    return ssim_per_channel, cs\n",
    "\n",
    "\n",
    "def ssim(\n",
    "    X,\n",
    "    Y,\n",
    "    data_range=255,\n",
    "    size_average=True,\n",
    "    win_size=11,\n",
    "    win_sigma=1.5,\n",
    "    win=None,\n",
    "    K=(0.01, 0.03),\n",
    "    nonnegative_ssim=False,\n",
    "):\n",
    "    r\"\"\" interface of ssim\n",
    "    Args:\n",
    "        X (torch.Tensor): a batch of images, (N,C,H,W)\n",
    "        Y (torch.Tensor): a batch of images, (N,C,H,W)\n",
    "        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n",
    "        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
    "        win_size: (int, optional): the size of gauss kernel\n",
    "        win_sigma: (float, optional): sigma of normal distribution\n",
    "        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n",
    "        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n",
    "        nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu\n",
    "    Returns:\n",
    "        torch.Tensor: ssim results\n",
    "    \"\"\"\n",
    "    if not X.shape == Y.shape:\n",
    "        raise ValueError(f\"Input images should have the same dimensions, but got {X.shape} and {Y.shape}.\")\n",
    "\n",
    "    for d in range(len(X.shape) - 1, 1, -1):\n",
    "        X = X.squeeze(dim=d)\n",
    "        Y = Y.squeeze(dim=d)\n",
    "\n",
    "    if len(X.shape) not in (4, 5):\n",
    "        raise ValueError(f\"Input images should be 4-d or 5-d tensors, but got {X.shape}\")\n",
    "\n",
    "    if not X.type() == Y.type():\n",
    "        raise ValueError(f\"Input images should have the same dtype, but got {X.type()} and {Y.type()}.\")\n",
    "\n",
    "    if win is not None:  # set win_size\n",
    "        win_size = win.shape[-1]\n",
    "\n",
    "    if not (win_size % 2 == 1):\n",
    "        raise ValueError(\"Window size should be odd.\")\n",
    "\n",
    "    if win is None:\n",
    "        win = _fspecial_gauss_1d(win_size, win_sigma)\n",
    "        win = win.repeat([X.shape[1]] + [1] * (len(X.shape) - 1))\n",
    "\n",
    "    ssim_per_channel, cs = _ssim(X, Y, data_range=data_range, win=win, size_average=False, K=K)\n",
    "    if nonnegative_ssim:\n",
    "        ssim_per_channel = torch.relu(ssim_per_channel)\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_per_channel.mean()\n",
    "    else:\n",
    "        return ssim_per_channel.mean(1)\n",
    "\n",
    "\n",
    "def ms_ssim(\n",
    "    X, Y, data_range=255, size_average=True, win_size=11, win_sigma=1.5, win=None, weights=None, K=(0.01, 0.03)\n",
    "):\n",
    "\n",
    "    r\"\"\" interface of ms-ssim\n",
    "    Args:\n",
    "        X (torch.Tensor): a batch of images, (N,C,[T,]H,W)\n",
    "        Y (torch.Tensor): a batch of images, (N,C,[T,]H,W)\n",
    "        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n",
    "        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
    "        win_size: (int, optional): the size of gauss kernel\n",
    "        win_sigma: (float, optional): sigma of normal distribution\n",
    "        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n",
    "        weights (list, optional): weights for different levels\n",
    "        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n",
    "    Returns:\n",
    "        torch.Tensor: ms-ssim results\n",
    "    \"\"\"\n",
    "    if not X.shape == Y.shape:\n",
    "        raise ValueError(f\"Input images should have the same dimensions, but got {X.shape} and {Y.shape}.\")\n",
    "\n",
    "    for d in range(len(X.shape) - 1, 1, -1):\n",
    "        X = X.squeeze(dim=d)\n",
    "        Y = Y.squeeze(dim=d)\n",
    "\n",
    "    if not X.type() == Y.type():\n",
    "        raise ValueError(f\"Input images should have the same dtype, but got {X.type()} and {Y.type()}.\")\n",
    "\n",
    "    if len(X.shape) == 4:\n",
    "        avg_pool = F.avg_pool2d\n",
    "    elif len(X.shape) == 5:\n",
    "        avg_pool = F.avg_pool3d\n",
    "    else:\n",
    "        raise ValueError(f\"Input images should be 4-d or 5-d tensors, but got {X.shape}\")\n",
    "\n",
    "    if win is not None:  # set win_size\n",
    "        win_size = win.shape[-1]\n",
    "\n",
    "    if not (win_size % 2 == 1):\n",
    "        raise ValueError(\"Window size should be odd.\")\n",
    "\n",
    "    smaller_side = min(X.shape[-2:])\n",
    "    assert smaller_side > (win_size - 1) * (\n",
    "        2 ** 4\n",
    "    ), \"Image size should be larger than %d due to the 4 downsamplings in ms-ssim\" % ((win_size - 1) * (2 ** 4))\n",
    "\n",
    "    if weights is None:\n",
    "        weights = [0.0448, 0.2856, 0.3001, 0.2363, 0.1333]\n",
    "    weights = X.new_tensor(weights)\n",
    "\n",
    "    if win is None:\n",
    "        win = _fspecial_gauss_1d(win_size, win_sigma)\n",
    "        win = win.repeat([X.shape[1]] + [1] * (len(X.shape) - 1))\n",
    "\n",
    "    levels = weights.shape[0]\n",
    "    mcs = []\n",
    "    for i in range(levels):\n",
    "        ssim_per_channel, cs = _ssim(X, Y, win=win, data_range=data_range, size_average=False, K=K)\n",
    "\n",
    "        if i < levels - 1:\n",
    "            mcs.append(torch.relu(cs))\n",
    "            padding = [s % 2 for s in X.shape[2:]]\n",
    "            X = avg_pool(X, kernel_size=2, padding=padding)\n",
    "            Y = avg_pool(Y, kernel_size=2, padding=padding)\n",
    "\n",
    "    ssim_per_channel = torch.relu(ssim_per_channel)  # (batch, channel)\n",
    "    mcs_and_ssim = torch.stack(mcs + [ssim_per_channel], dim=0)  # (level, batch, channel)\n",
    "    ms_ssim_val = torch.prod(mcs_and_ssim ** weights.view(-1, 1, 1), dim=0)\n",
    "\n",
    "    if size_average:\n",
    "        return ms_ssim_val.mean()\n",
    "    else:\n",
    "        return ms_ssim_val.mean(1)\n",
    "\n",
    "\n",
    "class SSIM(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_range=255,\n",
    "        size_average=True,\n",
    "        win_size=11,\n",
    "        win_sigma=1.5,\n",
    "        channel=3,\n",
    "        spatial_dims=2,\n",
    "        K=(0.01, 0.03),\n",
    "        nonnegative_ssim=False,\n",
    "    ):\n",
    "        r\"\"\" class for ssim\n",
    "        Args:\n",
    "            data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n",
    "            size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
    "            win_size: (int, optional): the size of gauss kernel\n",
    "            win_sigma: (float, optional): sigma of normal distribution\n",
    "            channel (int, optional): input channels (default: 3)\n",
    "            K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n",
    "            nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu.\n",
    "        \"\"\"\n",
    "\n",
    "        super(SSIM, self).__init__()\n",
    "        self.win_size = win_size\n",
    "        self.win = _fspecial_gauss_1d(win_size, win_sigma).repeat([channel, 1] + [1] * spatial_dims)\n",
    "        self.size_average = size_average\n",
    "        self.data_range = data_range\n",
    "        self.K = K\n",
    "        self.nonnegative_ssim = nonnegative_ssim\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return ssim(\n",
    "            X,\n",
    "            Y,\n",
    "            data_range=self.data_range,\n",
    "            size_average=self.size_average,\n",
    "            win=self.win,\n",
    "            K=self.K,\n",
    "            nonnegative_ssim=self.nonnegative_ssim,\n",
    "        )\n",
    "\n",
    "\n",
    "class MS_SSIM(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_range=255,\n",
    "        size_average=True,\n",
    "        win_size=11,\n",
    "        win_sigma=1.5,\n",
    "        channel=3,\n",
    "        spatial_dims=2,\n",
    "        weights=None,\n",
    "        K=(0.01, 0.03),\n",
    "    ):\n",
    "        r\"\"\" class for ms-ssim\n",
    "        Args:\n",
    "            data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n",
    "            size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
    "            win_size: (int, optional): the size of gauss kernel\n",
    "            win_sigma: (float, optional): sigma of normal distribution\n",
    "            channel (int, optional): input channels (default: 3)\n",
    "            weights (list, optional): weights for different levels\n",
    "            K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n",
    "        \"\"\"\n",
    "\n",
    "        super(MS_SSIM, self).__init__()\n",
    "        self.win_size = win_size\n",
    "        self.win = _fspecial_gauss_1d(win_size, win_sigma).repeat([channel, 1] + [1] * spatial_dims)\n",
    "        self.size_average = size_average\n",
    "        self.data_range = data_range\n",
    "        self.weights = weights\n",
    "        self.K = K\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return ms_ssim(\n",
    "            X,\n",
    "            Y,\n",
    "            data_range=self.data_range,\n",
    "            size_average=self.size_average,\n",
    "            win=self.win,\n",
    "            weights=self.weights,\n",
    "            K=self.K,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56317d41",
   "metadata": {
    "id": "56317d41"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "class VGGPerceptualLoss(torch.nn.Module):\n",
    "    def __init__(self, resize=True):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        blocks = []\n",
    "        blocks.append(torchvision.models.vgg16(pretrained=True).features[:4].eval().cuda())\n",
    "        blocks.append(torchvision.models.vgg16(pretrained=True).features[4:9].eval().cuda())\n",
    "        blocks.append(torchvision.models.vgg16(pretrained=True).features[9:16].eval().cuda())\n",
    "        blocks.append(torchvision.models.vgg16(pretrained=True).features[16:23].eval().cuda())\n",
    "        for bl in blocks:\n",
    "            for p in bl.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.blocks = torch.nn.ModuleList(blocks).cuda()\n",
    "        self.transform = torch.nn.functional.interpolate\n",
    "        self.resize = resize\n",
    "        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "\n",
    "    def forward(self, input, target, feature_layers=[0, 1, 2, 3], style_layers=[]):\n",
    "        if input.shape[1] != 3:\n",
    "            input = input.repeat(1, 3, 1, 1)\n",
    "            target = target.repeat(1, 3, 1, 1)\n",
    "        self.mean = self.mean.cuda()\n",
    "        self.std = self.std.cuda()\n",
    "        input = (input-self.mean) / self.std\n",
    "        target = (target-self.mean) / self.std\n",
    "        if self.resize:\n",
    "            input = self.transform(input, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "            target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "        loss = 0.0\n",
    "        x = input\n",
    "        y = target\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            y = block(y)\n",
    "            if i in feature_layers:\n",
    "                loss += torch.nn.functional.l1_loss(x, y)\n",
    "            if i in style_layers:\n",
    "                act_x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "                act_y = y.reshape(y.shape[0], y.shape[1], -1)\n",
    "                gram_x = act_x @ act_x.permute(0, 2, 1)\n",
    "                gram_y = act_y @ act_y.permute(0, 2, 1)\n",
    "                loss += torch.nn.functional.l1_loss(gram_x, gram_y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abd212c0",
   "metadata": {
    "id": "abd212c0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class MeanShift(nn.Conv2d):\n",
    "    def __init__(self, rgb_mean, rgb_std, sign=-1):\n",
    "        super(MeanShift, self).__init__(3, 3, kernel_size=1)\n",
    "        std = torch.Tensor(rgb_std)\n",
    "        self.weight.data = torch.eye(3).view(3, 3, 1, 1)\n",
    "        self.weight.data.div_(std.view(3, 1, 1, 1))\n",
    "        self.bias.data = sign * torch.Tensor(rgb_mean)\n",
    "        self.bias.data.div_(std)\n",
    "        self.requires_grad = False\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, loss_type):\n",
    "        super(VGG, self).__init__()\n",
    "        vgg_features = models.vgg19(pretrained=True).features\n",
    "        modules = [m.cuda() for m in vgg_features]\n",
    "        conv_index = loss_type\n",
    "        if conv_index == '22':\n",
    "            self.vgg = nn.Sequential(*modules[:8])\n",
    "        elif conv_index == '33':\n",
    "            self.vgg = nn.Sequential(*modules[:16])\n",
    "        elif conv_index == '44':\n",
    "            self.vgg = nn.Sequential(*modules[:26])\n",
    "        elif conv_index == '54':\n",
    "            self.vgg = nn.Sequential(*modules[:35])\n",
    "        elif conv_index == 'P':\n",
    "            self.vgg = nn.ModuleList([\n",
    "                nn.Sequential(*modules[:8]),\n",
    "                nn.Sequential(*modules[8:16]),\n",
    "                nn.Sequential(*modules[16:26]),\n",
    "                nn.Sequential(*modules[26:35])\n",
    "            ])\n",
    "        self.vgg = nn.DataParallel(self.vgg).cuda()\n",
    "\n",
    "        vgg_mean = (0.485, 0.456, 0.406)\n",
    "        vgg_std = (0.229, 0.224, 0.225)\n",
    "        self.sub_mean = MeanShift(vgg_mean, vgg_std)\n",
    "        self.vgg.requires_grad = False\n",
    "        # self.criterion = nn.L1Loss()\n",
    "        self.conv_index = conv_index\n",
    "\n",
    "    def forward(self, sr, hr):\n",
    "        def _forward(x):\n",
    "            x = x.cpu()\n",
    "            x = self.sub_mean(x)\n",
    "            x = self.vgg(x)\n",
    "            return x.cuda()\n",
    "        def _forward_all(x):\n",
    "            feats = []\n",
    "            x = x.cpu()\n",
    "            x = self.sub_mean(x)\n",
    "            for module in self.vgg.module:\n",
    "                x = module(x.cuda())\n",
    "                feats.append(x.cuda())\n",
    "            return feats\n",
    "\n",
    "        if self.conv_index == 'P':\n",
    "            vgg_sr_feats = _forward_all(sr)\n",
    "            with torch.no_grad():\n",
    "                vgg_hr_feats = _forward_all(hr.detach())\n",
    "            loss = 0\n",
    "            for i in range(len(vgg_sr_feats)):\n",
    "                loss_f = F.mse_loss(vgg_sr_feats[i], vgg_hr_feats[i])\n",
    "                #print(loss_f)\n",
    "                loss += loss_f\n",
    "            #print()\n",
    "        else:\n",
    "            vgg_sr = _forward(sr)\n",
    "            with torch.no_grad():\n",
    "                vgg_hr = _forward(hr.detach())\n",
    "            loss = F.mse_loss(vgg_sr, vgg_hr)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "numeric-instrumentation",
   "metadata": {
    "id": "numeric-instrumentation"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as time\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import torchvision.transforms.functional as FF\n",
    "import cv2\n",
    "import PIL\n",
    "myloss = torch.nn.MSELoss()\n",
    "huberloss = torch.nn.HuberLoss()\n",
    "kldivloss = torch.nn.KLDivLoss()\n",
    "vggloss = VGG('22')\n",
    "RHO = 0.05\n",
    "BETA = 0.01\n",
    "def kl_divergence(rho, rho_hat):\n",
    "    rho_hat = torch.mean(F.sigmoid(rho_hat), 1) # sigmoid because we need the probability distributions\n",
    "    rho = torch.tensor([rho] * len(rho_hat)).cuda()\n",
    "    return torch.sum(rho * torch.log(rho/rho_hat) + (1 - rho) * torch.log((1 - rho)/(1 - rho_hat)))\n",
    "\n",
    "# define the sparse loss function\n",
    "def sparse_loss(rho, images):\n",
    "    values = images\n",
    "    loss = 0\n",
    "    return kl_divergence(rho, images)\n",
    "#ssim_loss = SSIM()\n",
    "epochs = 2500\n",
    "class MS_SSIM_Loss(MS_SSIM):\n",
    "    def forward(self, img1, img2):\n",
    "        return 100*( 1 - super(MS_SSIM_Loss, self).forward(img1, img2) )\n",
    "\n",
    "class SSIM_Loss(SSIM):\n",
    "    def forward(self, img1, img2):\n",
    "        return 100*( 1 - super(SSIM_Loss, self).forward(img1, img2) )\n",
    "ssim_loss = MS_SSIM_Loss(win_size=3, win_sigma=1.5, data_range=1.0, size_average=True, channel=3)\n",
    "def train_JSGANO(D, G, train_data, epochs, D_optim, G_optim, scheduler=None):\n",
    "    losses_D = np.zeros(epochs)\n",
    "    losses_G = np.zeros(epochs)\n",
    "    ssims_G = np.zeros(epochs)\n",
    "    #train_data,y_normalizer  = LoadDataBatches(0, isNormalized=True, batches=10, isTrain=True, percent=0.1)\n",
    "    train_data = LoadDataBatches(0, isNormalized=False, batches=2, isTrain=True)\n",
    "    #base_G = torch.load('/depot/bera89/data/hviswan/UNO_2f_vimeo_vgg_rgb150.pt')\n",
    "    #base_G = torch.load('/depot/bera89/data/hviswan/UNO_2f_vimeo_lowres_rgb300.pt')\n",
    "    #complex_G = torch.load('/depot/bera89/data/hviswan/UNO_2f_vimeo_fine_norm_rgb200.pt')\n",
    "    for i in range(1, epochs+1):\n",
    "        t1 = time.now()\n",
    "        loss_D = 0.0\n",
    "        loss_G = 0.0\n",
    "        ssim_G = 0.0\n",
    "        train_counter = 0\n",
    "        ssim_batch=0\n",
    "        for j in range(1):\n",
    "            #del train_data\n",
    "            #train_data = LoadDataBatches(j, batches=10)\n",
    "            for xl,xr,y in train_data:\n",
    "                train_counter += 1\n",
    "                temp = 0.0001*xl + 0.0001*xr\n",
    "                temp = temp.cuda()\n",
    "                xl = xl.cpu().detach().numpy()\n",
    "                xr = xr.cpu().detach().numpy()\n",
    "                #y = y.cpu().detach().numpy()\n",
    "                x = torch.from_numpy(np.asarray([xl,xr]).astype(np.float32))\n",
    "                y = y.cuda()\n",
    "                x = x.cuda()\n",
    "                #y = y.cuda()\n",
    "                G_optim.zero_grad()\n",
    "                #base_x_out = base_G(x).cuda()\n",
    "                #x_avg = torch.from_numpy(np.asarray((xl+xr)/2.0).astype(np.float32)).cuda()\n",
    "                #x = torch.stack((base_x_out, base_x_out))\n",
    "                #complex_x_out = complex_G(x).reshape(x.shape[1], x.shape[2],x.shape[3],x.shape[4])\n",
    "                #x = torch.stack((complex_x_out, complex_x_out))\n",
    "                #x = torch.permute(complex_x_out, (0, 3, 1, 2))\n",
    "                x_syn = G(x)\n",
    "                #x_complex = complex_G(x)\n",
    "                #x_complex = y_normalizer.decode(x_complex)\n",
    "                #print(x_syn.shape)\n",
    "                #base_x_out = base_G(x).permute(0,3,1,2)\n",
    "                #x_syn = x_syn #+ 0.001*base_x_out\n",
    "                #x_syn = torch.permute(x_syn, (0, 2, 3, 1))\n",
    "                #x_syn = x_syn + temp.cuda()\n",
    "                \n",
    "                #x_syn = 0.6*x_syn #+ 0.4*base_x_out\n",
    "                #x_syn = y_normalizer.decode(x_syn)\n",
    "                #y = y_normalizer.decode(y)\n",
    "                x_syn2 = torch.permute(x_syn, (0, 2, 3, 1)).type(torch.float32)\n",
    "                #x_syn2 = x_syn2.detach().cpu().numpy()\n",
    "                #for k in range(x_syn2.shape[0]):\n",
    "                    #x_syn2[k] = FF.adjust_saturation(x_syn2[k], 1.5)\n",
    "                #x_syn2 = torch.from_numpy(x_syn2.astype(np.float32))\n",
    "                y2 = torch.permute(y, (0, 2, 3,1)).type(torch.float32)\n",
    "                y = y.reshape(y.shape[0], y.shape[1],y.shape[2],y.shape[3])\n",
    "                #print(\"Y2 = \", y2.shape, \" Y = \", y.shape, \" x_syn = \", x_syn.shape, \" x_syn2 = \", x_syn2.shape)\n",
    "                \n",
    "                W_loss = huberloss(x_syn.type(torch.float32), y.type(torch.float32)) #+ 0.001*vggloss(x_syn2.type(torch.float32), y2.type(torch.float32))#+ 0.01*torch.abs(kldivloss(x_syn2.type(torch.float32), y2.type(torch.float32)))\n",
    "                ssim_value = W_loss.item()\n",
    "                ssim_individual = 0.0\n",
    "                for k in range(y.shape[0]):\n",
    "                    ssim_individual += ssim(x_syn2[k].detach().cpu().numpy(), y2[k].detach().cpu().numpy(), multichannel=True)\n",
    "                \n",
    "                ssim_batch += ssim_individual/y.shape[0]\n",
    "                loss = W_loss\n",
    "                loss.backward()\n",
    "                loss_G += loss.item()            \n",
    "                G_optim.step()\n",
    "                if(train_counter %25 == 0 or train_counter == 1):\n",
    "                    print(\"Batch = \",train_counter,\" Train Loss = \", loss_G/train_counter, \" SSIM = \", ssim_batch/(train_counter))\n",
    "            losses_D[i] = loss_D / train_counter\n",
    "            losses_G[i] = loss_G / train_counter\n",
    "            ssims_G[i] = ssim_batch / train_counter\n",
    "            t2 = time.now()\n",
    "            test_counter = 1\n",
    "            \"\"\"\n",
    "            test_counter = 0\n",
    "            ssim_batch = 0\n",
    "            for xl, xr, y in validation_data:\n",
    "                temp = 0.05*xl + 0.1*xr\n",
    "                xl = xl.cpu().detach().numpy()\n",
    "                xr = xr.cpu().detach().numpy()\n",
    "                x = torch.from_numpy(np.asarray([xl,xr]).astype(np.float32))\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "                x_syn = G(x).cuda()\n",
    "                x_syn = x_syn + temp.cuda()\n",
    "                x_syn = y_normalizer_test.decode(x_syn)\n",
    "                y = y_normalizer_test.decode(y)\n",
    "                x_syn2 = torch.permute(x_syn, (0, 3, 1, 2)).type(torch.float32)\n",
    "                y2 = torch.permute(y, (0, 3, 1,2)).type(torch.float32)\n",
    "                y = y.reshape(y.shape[0], y.shape[1],y.shape[2],y.shape[3])\n",
    "                W_loss = huberloss(x_syn2.type(torch.float32), y2.type(torch.float32))*10000\n",
    "                ssim_individual = 0.0\n",
    "                for k in range(y.shape[0]):\n",
    "                    ssim_individual += ssim(x_syn[k].detach().cpu().numpy(), y[k].detach().cpu().numpy(), multichannel=True)\n",
    "                \n",
    "                ssim_batch += ssim_individual/y.shape[0]\n",
    "                loss = W_loss\n",
    "                loss.backward()\n",
    "                G_optim.step()\n",
    "                loss_D += loss.item()\n",
    "                test_counter += 1\n",
    "                if(test_counter %50 == 0 or test_counter == 1):\n",
    "                    print(\"Batch = \",test_counter,\" Validation Loss = \", loss_D/test_counter, \" SSIM = \", ssim_batch/(test_counter))\n",
    "            losses_D[i] = loss_D/test_counter\n",
    "            \"\"\"\n",
    "            print(\"Loader #: \", j, \" Epoch: \", i, \"/ \",epochs,\" -  Time: \", t2-t1, \"s - Validation Loss: \", losses_D[i], \" - Train Loss: \", losses_G[i], \" - train SSIM: \", ssims_G[i], \" validation SSIM: \", ssim_batch/test_counter)\n",
    "        if(i%20 == 0):\n",
    "            torch.save(G, '/depot/bera89/data/hviswan/NIO_diff_data'+str(i)+'.pt')\n",
    "            \n",
    "    return losses_D, losses_G, D, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "soviet-distinction",
   "metadata": {
    "id": "soviet-distinction"
   },
   "outputs": [],
   "source": [
    "def calculate_gradient_penalty(model, real_images, fake_images, device):\n",
    "    \"\"\"Calculates the gradient penalty loss for GANO\"\"\"\n",
    "    # Random weight term for interpolation between real and fake data\n",
    "    alpha = torch.randn((real_images.size(0), 1, 1, 1)).cuda()\n",
    "    # Get random interpolation between real and fake data\n",
    "    interpolates = (alpha * real_images + ((1 - alpha) * fake_images)).requires_grad_(True)\n",
    "\n",
    "    model_interpolates = model(interpolates)\n",
    "    grad_outputs = torch.ones(model_interpolates.size(), requires_grad=False).cuda()\n",
    "\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=model_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=grad_outputs,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = torch.mean((gradients.norm(2, dim=1) - 1/np.sqrt(res * res)) ** 2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-merchandise",
   "metadata": {
    "id": "aging-merchandise",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#D = torch.load('/depot/bera89/data/hviswan/GANO_DISC300.pt')\n",
    "#D.eval()\n",
    "NF = torch.load('/depot/bera89/data/hviswan/UNO_2f_vimeo_lowres_vgg_rgb_huge800.pt')\n",
    "summary(NF, input_size=(2,10, 85,85,3))\n",
    "NF.train()\n",
    "device = 'cuda:11'\n",
    "#G = torch.load('/depot/bera89/data/hviswan/UNO_2f_vimeo_full50.pt')\n",
    "#D.train()\n",
    "NF.train()\n",
    "G_optimizer = torch.optim.Adam(NF.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "#D_optimizer = torch.optim.Adam(D.parameters(), lr=lr, weight_decay=1e-4)\n",
    "losses_D, losses_G, D, G = train_JSGANO(None, NF, None, epochs, None, G_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adefe98d",
   "metadata": {
    "id": "adefe98d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4400, 3, 256, 256])\n",
      "torch.Size([2, 4400, 3, 256, 256])\n",
      "torch.Size([4400, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_251569/2875651774.py:94: FutureWarning: `multichannel` is a deprecated argument name for `structural_similarity`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  ssim_individual += ssim(x_syn2[k].detach().cpu().numpy(), y2[k].detach().cpu().numpy(), multichannel=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch =  1  Train Loss =  0.01808980107307434  SSIM =  0.4963040351867676\n",
      "Batch =  25  Train Loss =  0.050299235358834266  SSIM =  0.474745671749115\n",
      "Batch =  50  Train Loss =  0.04651483990252018  SSIM =  0.47658284023404124\n",
      "Batch =  75  Train Loss =  0.04389621866246064  SSIM =  0.47789837390184403\n",
      "Batch =  100  Train Loss =  0.04261285550892353  SSIM =  0.4753354243189096\n",
      "Batch =  125  Train Loss =  0.04097736694663763  SSIM =  0.48061338770389556\n",
      "Batch =  150  Train Loss =  0.03960523299872875  SSIM =  0.4877914507190386\n",
      "Batch =  175  Train Loss =  0.03804480896464416  SSIM =  0.4940035080909729\n",
      "Batch =  200  Train Loss =  0.03626095469109714  SSIM =  0.500408344939351\n",
      "Batch =  225  Train Loss =  0.033812807930840384  SSIM =  0.5135735202497906\n",
      "Batch =  250  Train Loss =  0.03144796752370894  SSIM =  0.5231404889822007\n",
      "Batch =  275  Train Loss =  0.029301699197597123  SSIM =  0.5370410579442978\n",
      "Batch =  300  Train Loss =  0.027526127914121994  SSIM =  0.549080718656381\n",
      "Batch =  325  Train Loss =  0.025998647159252028  SSIM =  0.5604553606418463\n",
      "Batch =  350  Train Loss =  0.024565978441387415  SSIM =  0.5722074927176748\n",
      "Batch =  375  Train Loss =  0.023297702436645827  SSIM =  0.5814960063695908\n",
      "Batch =  400  Train Loss =  0.022186129743931814  SSIM =  0.591895764619112\n",
      "Batch =  425  Train Loss =  0.02123651011792176  SSIM =  0.5987530037234812\n",
      "Batch =  450  Train Loss =  0.020320164621807635  SSIM =  0.607239700489574\n",
      "Batch =  475  Train Loss =  0.019506214564960253  SSIM =  0.6143565151565953\n",
      "Batch =  500  Train Loss =  0.018781708119669928  SSIM =  0.6192165273427963\n",
      "Batch =  525  Train Loss =  0.018160179003053122  SSIM =  0.6252403108846574\n",
      "Batch =  550  Train Loss =  0.017564716107339007  SSIM =  0.6297796247222207\n",
      "Batch =  575  Train Loss =  0.017017304254898235  SSIM =  0.6344072238258693\n",
      "Batch =  600  Train Loss =  0.016552114536558898  SSIM =  0.6389644382148981\n",
      "Batch =  625  Train Loss =  0.016088376829214393  SSIM =  0.6431627785205841\n",
      "Batch =  650  Train Loss =  0.015644780128048017  SSIM =  0.6465230287726109\n",
      "Batch =  675  Train Loss =  0.015287539416372224  SSIM =  0.6495459115725977\n",
      "Batch =  700  Train Loss =  0.014918313896882215  SSIM =  0.6530006773131234\n",
      "Batch =  725  Train Loss =  0.014573276913243121  SSIM =  0.6561313582494341\n",
      "Batch =  750  Train Loss =  0.01427435943344608  SSIM =  0.6599044397473335\n",
      "Batch =  775  Train Loss =  0.013970910083322276  SSIM =  0.66362993136529\n",
      "Batch =  800  Train Loss =  0.013696985754795605  SSIM =  0.6660269148368388\n",
      "Batch =  825  Train Loss =  0.013409841708189835  SSIM =  0.6687553645534949\n",
      "Batch =  850  Train Loss =  0.013184254909454681  SSIM =  0.6704398005937828\n",
      "Batch =  875  Train Loss =  0.012981290696854038  SSIM =  0.6718367245452744\n",
      "Batch =  900  Train Loss =  0.012767067638665645  SSIM =  0.673726335523857\n",
      "Batch =  925  Train Loss =  0.012558356544405624  SSIM =  0.6754739524464349\n",
      "Batch =  950  Train Loss =  0.012357289057568107  SSIM =  0.6779142735898495\n",
      "Batch =  975  Train Loss =  0.012179864070211084  SSIM =  0.6791382404550528\n",
      "Batch =  1000  Train Loss =  0.011993380503961816  SSIM =  0.6817670472487807\n",
      "Batch =  1025  Train Loss =  0.01184252402972339  SSIM =  0.6825061156473509\n",
      "Batch =  1050  Train Loss =  0.011657800032158515  SSIM =  0.6842487693897316\n",
      "Batch =  1075  Train Loss =  0.011480379115985057  SSIM =  0.686082823269589\n",
      "Batch =  1100  Train Loss =  0.011331485857636752  SSIM =  0.6876246720484712\n",
      "Batch =  1125  Train Loss =  0.011186896584100194  SSIM =  0.6885196530487803\n",
      "Batch =  1150  Train Loss =  0.011033880166587946  SSIM =  0.690055019861978\n",
      "Batch =  1175  Train Loss =  0.01089498295458628  SSIM =  0.6916423987898421\n",
      "Batch =  1200  Train Loss =  0.010781201439191743  SSIM =  0.6927010852408906\n",
      "Batch =  1225  Train Loss =  0.01063866022166473  SSIM =  0.6944613408859895\n",
      "Batch =  1250  Train Loss =  0.010520823179371655  SSIM =  0.6956124725162983\n",
      "Batch =  1275  Train Loss =  0.010422717765631044  SSIM =  0.6960510363006124\n",
      "Batch =  1300  Train Loss =  0.010357151370710478  SSIM =  0.6960345463053538\n",
      "Batch =  1325  Train Loss =  0.01024177985397642  SSIM =  0.697167057366866\n",
      "Batch =  1350  Train Loss =  0.010147547734021727  SSIM =  0.6977197607855002\n",
      "Batch =  1375  Train Loss =  0.010068481945602054  SSIM =  0.6979073411822319\n",
      "Batch =  1400  Train Loss =  0.009975947237440518  SSIM =  0.6988177790120244\n",
      "Batch =  1425  Train Loss =  0.009891729564324284  SSIM =  0.6992725540828286\n",
      "Batch =  1450  Train Loss =  0.009801026557106525  SSIM =  0.6998425008536413\n",
      "Batch =  1475  Train Loss =  0.009717217294294072  SSIM =  0.7006652457860567\n",
      "Batch =  1500  Train Loss =  0.009645906931720674  SSIM =  0.7018057272260387\n",
      "Batch =  1525  Train Loss =  0.009561045143188389  SSIM =  0.702441418065888\n",
      "Batch =  1550  Train Loss =  0.009492685543898973  SSIM =  0.7030217285334103\n",
      "Batch =  1575  Train Loss =  0.009414366689983696  SSIM =  0.7038377434133537\n",
      "Batch =  1600  Train Loss =  0.009352557500606054  SSIM =  0.7043689343356527\n",
      "Batch =  1625  Train Loss =  0.009283471359942968  SSIM =  0.705241549118207\n",
      "Batch =  1650  Train Loss =  0.00922691957326606  SSIM =  0.7057324295987686\n",
      "Batch =  1675  Train Loss =  0.009163223160708795  SSIM =  0.7063823671149674\n",
      "Batch =  1700  Train Loss =  0.009099033308722188  SSIM =  0.7069496034458279\n",
      "Batch =  1725  Train Loss =  0.009036235386384246  SSIM =  0.7077504908369071\n",
      "Batch =  1750  Train Loss =  0.008987018018960952  SSIM =  0.7078604618417366\n",
      "Batch =  1775  Train Loss =  0.008921137072250876  SSIM =  0.7087708472890754\n",
      "Batch =  1800  Train Loss =  0.008870454544925856  SSIM =  0.709572754090445\n",
      "Batch =  1825  Train Loss =  0.008810236362663851  SSIM =  0.710220122110762\n",
      "Batch =  1850  Train Loss =  0.008763670242089476  SSIM =  0.7106247879947359\n",
      "Batch =  1875  Train Loss =  0.008709715457012256  SSIM =  0.7108815010011196\n",
      "Batch =  1900  Train Loss =  0.008647505669597242  SSIM =  0.7116221652160349\n",
      "Batch =  1925  Train Loss =  0.00860411106421334  SSIM =  0.7121761369569735\n",
      "Batch =  1950  Train Loss =  0.008547020258250623  SSIM =  0.7131342315692932\n",
      "Batch =  1975  Train Loss =  0.008506825462019047  SSIM =  0.7131116785599461\n",
      "Batch =  2000  Train Loss =  0.008469020081858616  SSIM =  0.7135784569550305\n",
      "Batch =  2025  Train Loss =  0.00841326770292204  SSIM =  0.7141510413476715\n",
      "Batch =  2050  Train Loss =  0.00836530819075273  SSIM =  0.7142013412732177\n",
      "Batch =  2075  Train Loss =  0.008317236618492303  SSIM =  0.7149206527977823\n",
      "Batch =  2100  Train Loss =  0.0082632867880221  SSIM =  0.7158667051313179\n",
      "Batch =  2125  Train Loss =  0.008217767998603557  SSIM =  0.7163750871647807\n",
      "Batch =  2150  Train Loss =  0.008174793513486335  SSIM =  0.7167048799315857\n",
      "Batch =  2175  Train Loss =  0.008124720095817385  SSIM =  0.7176611451119527\n",
      "Batch =  2200  Train Loss =  0.008081546177431433  SSIM =  0.7179995356432416\n",
      "Loader #:  0  Epoch:  1 /  2500  -  Time:  0:04:12.671751 s - Validation Loss:  0.0  - Train Loss:  0.008081546177431433  - train SSIM:  0.7179995356432416  validation SSIM:  1579.5989784151316\n",
      "Batch =  1  Train Loss =  0.010388309136033058  SSIM =  0.6083800494670868\n",
      "Batch =  25  Train Loss =  0.005572874071076512  SSIM =  0.7344113349914551\n",
      "Batch =  50  Train Loss =  0.0055622205324471  SSIM =  0.7313859406113624\n",
      "Batch =  75  Train Loss =  0.005563919483684003  SSIM =  0.7347473998864492\n",
      "Batch =  100  Train Loss =  0.005197685615858063  SSIM =  0.7465877279639244\n",
      "Batch =  125  Train Loss =  0.005027729256078601  SSIM =  0.7507629320621491\n",
      "Batch =  150  Train Loss =  0.004949288217661282  SSIM =  0.7514089107513428\n",
      "Batch =  175  Train Loss =  0.004857779096013733  SSIM =  0.7501584907940456\n",
      "Batch =  200  Train Loss =  0.004864861039677635  SSIM =  0.7503294117003679\n",
      "Batch =  225  Train Loss =  0.004808279009432428  SSIM =  0.7502465714348687\n",
      "Batch =  250  Train Loss =  0.004765204962808639  SSIM =  0.7517623053193092\n",
      "Batch =  275  Train Loss =  0.004661737067878924  SSIM =  0.7556028354167938\n",
      "Batch =  300  Train Loss =  0.004613538060414916  SSIM =  0.7586805689831575\n",
      "Batch =  325  Train Loss =  0.004635735613007385  SSIM =  0.7577095746077024\n",
      "Batch =  350  Train Loss =  0.004643130893819034  SSIM =  0.7554794286404337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch =  375  Train Loss =  0.004663847658783198  SSIM =  0.7559711496432622\n",
      "Batch =  400  Train Loss =  0.0047232862983946685  SSIM =  0.7537263138592243\n",
      "Batch =  425  Train Loss =  0.0046815972854657205  SSIM =  0.7547705804600435\n",
      "Batch =  450  Train Loss =  0.004642137735952727  SSIM =  0.756720436182287\n",
      "Batch =  475  Train Loss =  0.004656219087222493  SSIM =  0.7560167724207828\n",
      "Batch =  500  Train Loss =  0.004671575756161474  SSIM =  0.7544475065469742\n",
      "Batch =  525  Train Loss =  0.004644907453163926  SSIM =  0.7538551415148236\n",
      "Batch =  550  Train Loss =  0.004665819248713722  SSIM =  0.751953940933401\n",
      "Batch =  575  Train Loss =  0.004700095163678508  SSIM =  0.7513903702860293\n",
      "Batch =  600  Train Loss =  0.004682861519686412  SSIM =  0.7513046279797951\n",
      "Batch =  625  Train Loss =  0.004698662759084255  SSIM =  0.7505844020843506\n",
      "Batch =  650  Train Loss =  0.00467893469801101  SSIM =  0.750648327103028\n",
      "Batch =  675  Train Loss =  0.004683684570525117  SSIM =  0.7514386234460053\n",
      "Batch =  700  Train Loss =  0.004696288270642981  SSIM =  0.7507052611453192\n",
      "Batch =  725  Train Loss =  0.004694602433270936  SSIM =  0.7511489993333816\n",
      "Batch =  750  Train Loss =  0.004682366118921588  SSIM =  0.7516934931476911\n",
      "Batch =  775  Train Loss =  0.004677178695001789  SSIM =  0.7522936119379536\n",
      "Batch =  800  Train Loss =  0.004648239720263518  SSIM =  0.753428890388459\n",
      "Batch =  825  Train Loss =  0.004633549840302404  SSIM =  0.7538995584213373\n",
      "Batch =  850  Train Loss =  0.004655619774917689  SSIM =  0.7532278085982098\n",
      "Batch =  875  Train Loss =  0.004634083602577448  SSIM =  0.7536528591939381\n",
      "Batch =  900  Train Loss =  0.004632452388630352  SSIM =  0.7539806119104226\n",
      "Batch =  925  Train Loss =  0.004634129701912202  SSIM =  0.7539548674709088\n",
      "Batch =  950  Train Loss =  0.00462588714006798  SSIM =  0.7549265697363176\n",
      "Batch =  975  Train Loss =  0.004622075619438711  SSIM =  0.7545970045832487\n",
      "Batch =  1000  Train Loss =  0.00461650555417873  SSIM =  0.7544546790793538\n",
      "Batch =  1025  Train Loss =  0.004603132966156231  SSIM =  0.7544297694651092\n",
      "Batch =  1050  Train Loss =  0.004611396193781513  SSIM =  0.7546526374916236\n",
      "Batch =  1075  Train Loss =  0.004601432115846682  SSIM =  0.7551532445605411\n",
      "Batch =  1100  Train Loss =  0.0045973535759416834  SSIM =  0.754846759526567\n",
      "Batch =  1125  Train Loss =  0.004579931643998458  SSIM =  0.7554378778603342\n",
      "Batch =  1150  Train Loss =  0.004575219673975168  SSIM =  0.7556160902912202\n",
      "Batch =  1175  Train Loss =  0.004572419122832411  SSIM =  0.7561790927737317\n",
      "Batch =  1200  Train Loss =  0.004569064326060471  SSIM =  0.7562259502274294\n",
      "Batch =  1225  Train Loss =  0.0045859503313632945  SSIM =  0.7556483765341797\n",
      "Batch =  1250  Train Loss =  0.004584600800089538  SSIM =  0.7560837026298046\n",
      "Batch =  1275  Train Loss =  0.004569234399435421  SSIM =  0.7565039806798393\n",
      "Batch =  1300  Train Loss =  0.004567704480589152  SSIM =  0.75639384098351\n",
      "Batch =  1325  Train Loss =  0.0045689221395109345  SSIM =  0.7561443333187193\n",
      "Batch =  1350  Train Loss =  0.004565222099923563  SSIM =  0.7564488777242325\n",
      "Batch =  1375  Train Loss =  0.004578477239193903  SSIM =  0.7558669814142314\n",
      "Batch =  1400  Train Loss =  0.0045760879287679145  SSIM =  0.7561191173057471\n",
      "Batch =  1425  Train Loss =  0.004593222549373055  SSIM =  0.7556061679386256\n",
      "Batch =  1450  Train Loss =  0.004577541805297972  SSIM =  0.7559304290239153\n",
      "Batch =  1475  Train Loss =  0.0045683408797308174  SSIM =  0.7561809716133748\n",
      "Batch =  1500  Train Loss =  0.004569052620674484  SSIM =  0.756208031202356\n",
      "Batch =  1525  Train Loss =  0.004579731161576375  SSIM =  0.755462379186857\n",
      "Batch =  1550  Train Loss =  0.004584178843257588  SSIM =  0.7558878995862699\n",
      "Batch =  1575  Train Loss =  0.004579625206090333  SSIM =  0.7561526348808455\n",
      "Batch =  1600  Train Loss =  0.004580540199349344  SSIM =  0.7559654236631468\n",
      "Batch =  1625  Train Loss =  0.004580699500197975  SSIM =  0.7559988788778965\n",
      "Batch =  1650  Train Loss =  0.004572525759559181  SSIM =  0.7562474188614975\n",
      "Batch =  1675  Train Loss =  0.0045797557077620794  SSIM =  0.7560310778377661\n",
      "Batch =  1700  Train Loss =  0.004577272670610589  SSIM =  0.7557964963816545\n",
      "Batch =  1725  Train Loss =  0.004592203513644906  SSIM =  0.7552148908938187\n",
      "Batch =  1750  Train Loss =  0.004590318188803004  SSIM =  0.7553167593862329\n",
      "Batch =  1775  Train Loss =  0.004586212664094924  SSIM =  0.7552795868734239\n",
      "Batch =  1800  Train Loss =  0.0045767183738320855  SSIM =  0.7557082471044527\n",
      "Batch =  1825  Train Loss =  0.004582239999499632  SSIM =  0.7554693921828923\n",
      "Batch =  1850  Train Loss =  0.004574305921214054  SSIM =  0.7555176211974105\n",
      "Batch =  1875  Train Loss =  0.004564359645048777  SSIM =  0.7556835731466611\n",
      "Batch =  1900  Train Loss =  0.0045630662332558514  SSIM =  0.7555383393599798\n",
      "Batch =  1925  Train Loss =  0.004556315503560926  SSIM =  0.7559047944708304\n",
      "Batch =  1950  Train Loss =  0.004558537824777886  SSIM =  0.755753486955013\n",
      "Batch =  1975  Train Loss =  0.004548586427346108  SSIM =  0.7559926124577281\n",
      "Batch =  2000  Train Loss =  0.004545714532519923  SSIM =  0.7560182683207095\n",
      "Batch =  2025  Train Loss =  0.004549306401620722  SSIM =  0.755994194087423\n",
      "Batch =  2050  Train Loss =  0.004547375625470744  SSIM =  0.7558381631570619\n",
      "Batch =  2075  Train Loss =  0.004553345138605415  SSIM =  0.7558648811155055\n",
      "Batch =  2100  Train Loss =  0.004556356004544623  SSIM =  0.7554862151720694\n",
      "Batch =  2125  Train Loss =  0.004557808344663285  SSIM =  0.7554852516686215\n",
      "Batch =  2150  Train Loss =  0.004561989097016712  SSIM =  0.7552846057297186\n",
      "Batch =  2175  Train Loss =  0.004559678026587413  SSIM =  0.7553165004410963\n",
      "Batch =  2200  Train Loss =  0.004548527274561242  SSIM =  0.7556658524173228\n",
      "Loader #:  0  Epoch:  2 /  2500  -  Time:  0:04:15.204740 s - Validation Loss:  0.0  - Train Loss:  0.004548527274561242  - train SSIM:  0.7556658524173228  validation SSIM:  1662.46487531811\n",
      "Batch =  1  Train Loss =  0.007832180708646774  SSIM =  0.7307665348052979\n",
      "Batch =  25  Train Loss =  0.005056034657172859  SSIM =  0.7549120450019836\n",
      "Batch =  50  Train Loss =  0.004621981449890882  SSIM =  0.7657967591285706\n",
      "Batch =  75  Train Loss =  0.004773464608006179  SSIM =  0.7568869201342264\n",
      "Batch =  100  Train Loss =  0.004718584541697055  SSIM =  0.7589479300379753\n",
      "Batch =  125  Train Loss =  0.004608889073133469  SSIM =  0.7556740652322769\n",
      "Batch =  150  Train Loss =  0.004584742009950181  SSIM =  0.7569687727093697\n",
      "Batch =  175  Train Loss =  0.0046283746005168986  SSIM =  0.759945045539311\n",
      "Batch =  200  Train Loss =  0.004688516066526063  SSIM =  0.7614097336679697\n",
      "Batch =  225  Train Loss =  0.004703225068644517  SSIM =  0.7624816164043214\n",
      "Batch =  250  Train Loss =  0.004648879330139607  SSIM =  0.7643200508356094\n",
      "Batch =  275  Train Loss =  0.004659710053608499  SSIM =  0.7620424288511276\n",
      "Batch =  300  Train Loss =  0.004563599212560803  SSIM =  0.7634502745668094\n",
      "Batch =  325  Train Loss =  0.004490019354181221  SSIM =  0.7653568997749916\n",
      "Batch =  350  Train Loss =  0.004463119961853538  SSIM =  0.7651477582539831\n",
      "Batch =  375  Train Loss =  0.004413884694688022  SSIM =  0.7668868260383606\n",
      "Batch =  400  Train Loss =  0.004400746096798684  SSIM =  0.7641436145082117\n",
      "Batch =  425  Train Loss =  0.004382683334543424  SSIM =  0.7665028502310024\n",
      "Batch =  450  Train Loss =  0.004387093220526973  SSIM =  0.7652946416536967\n",
      "Batch =  475  Train Loss =  0.004357773021941906  SSIM =  0.7661455438952697\n",
      "Batch =  500  Train Loss =  0.004331246864050627  SSIM =  0.7660741455256939\n",
      "Batch =  525  Train Loss =  0.00435115411035007  SSIM =  0.7661938323009582\n",
      "Batch =  550  Train Loss =  0.004348561245609414  SSIM =  0.7653907027840614\n",
      "Batch =  575  Train Loss =  0.00434768464618727  SSIM =  0.7646882254144419\n",
      "Batch =  600  Train Loss =  0.004369798170325036  SSIM =  0.7640723538398743\n",
      "Batch =  625  Train Loss =  0.004383650478348136  SSIM =  0.7624181652784348\n",
      "Batch =  650  Train Loss =  0.004387326746140248  SSIM =  0.7616570383310318\n",
      "Batch =  675  Train Loss =  0.004388544993233626  SSIM =  0.7610263253141333\n",
      "Batch =  700  Train Loss =  0.00442183147583689  SSIM =  0.7590788152388164\n",
      "Batch =  725  Train Loss =  0.004405761923849711  SSIM =  0.758984117281848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch =  750  Train Loss =  0.004426995695801452  SSIM =  0.7588753321568171\n",
      "Batch =  775  Train Loss =  0.004416786466589978  SSIM =  0.7586694799123271\n",
      "Batch =  800  Train Loss =  0.004408074664024752  SSIM =  0.7586009951680899\n",
      "Batch =  825  Train Loss =  0.00440317212004271  SSIM =  0.7588097379063115\n",
      "Batch =  850  Train Loss =  0.004377384093685952  SSIM =  0.7590428896160687\n",
      "Batch =  875  Train Loss =  0.004373155800426113  SSIM =  0.7589303884335926\n",
      "Batch =  900  Train Loss =  0.004363353888491272  SSIM =  0.759194438672728\n",
      "Batch =  925  Train Loss =  0.004339089296204415  SSIM =  0.759556931788857\n",
      "Batch =  950  Train Loss =  0.004357575191748573  SSIM =  0.7588500407181288\n",
      "Batch =  975  Train Loss =  0.0043430239412312706  SSIM =  0.7590584739049275\n",
      "Batch =  1000  Train Loss =  0.0043590031834319235  SSIM =  0.7587548944503069\n",
      "Batch =  1025  Train Loss =  0.004344939658668165  SSIM =  0.7593044965587011\n",
      "Batch =  1050  Train Loss =  0.004348146293777972  SSIM =  0.7598934005520174\n",
      "Batch =  1075  Train Loss =  0.004338547453977341  SSIM =  0.7598385888164819\n",
      "Batch =  1100  Train Loss =  0.004314205851617523  SSIM =  0.7602900071950122\n",
      "Batch =  1125  Train Loss =  0.00429931227542046  SSIM =  0.7607937898735205\n",
      "Batch =  1150  Train Loss =  0.004290791789636664  SSIM =  0.7604867616911297\n",
      "Batch =  1175  Train Loss =  0.004290000554124647  SSIM =  0.760046419586907\n",
      "Batch =  1200  Train Loss =  0.00427549673714869  SSIM =  0.7602494521532208\n",
      "Batch =  1225  Train Loss =  0.004267773424413017  SSIM =  0.7604395778173086\n",
      "Batch =  1250  Train Loss =  0.004263854192220606  SSIM =  0.7603528473228216\n",
      "Batch =  1275  Train Loss =  0.004253749147436454  SSIM =  0.7607522134073809\n",
      "Batch =  1300  Train Loss =  0.004243319452664134  SSIM =  0.7605968371578135\n",
      "Batch =  1325  Train Loss =  0.00422821175884719  SSIM =  0.7611186703392919\n",
      "Batch =  1350  Train Loss =  0.004212008854438965  SSIM =  0.7614542236234303\n",
      "Batch =  1375  Train Loss =  0.004192212606505067  SSIM =  0.76176685399088\n",
      "Batch =  1400  Train Loss =  0.0041837371523018065  SSIM =  0.7619118080048689\n",
      "Batch =  1425  Train Loss =  0.004177574558743161  SSIM =  0.7621276897479567\n",
      "Batch =  1450  Train Loss =  0.004162556480130598  SSIM =  0.7622913395407899\n",
      "Batch =  1475  Train Loss =  0.004147374376761175  SSIM =  0.7622434173069768\n",
      "Batch =  1500  Train Loss =  0.004141986652335618  SSIM =  0.7620523251866301\n",
      "Batch =  1525  Train Loss =  0.004133288191810831  SSIM =  0.7619044806502882\n",
      "Batch =  1550  Train Loss =  0.004131185766891576  SSIM =  0.7617735598813141\n",
      "Batch =  1575  Train Loss =  0.004128335057184958  SSIM =  0.7616362299308891\n",
      "Batch =  1600  Train Loss =  0.004113104216539796  SSIM =  0.7619597311667167\n",
      "Batch =  1625  Train Loss =  0.0041010106585812394  SSIM =  0.7618698250307486\n",
      "Batch =  1650  Train Loss =  0.004084724766205799  SSIM =  0.7616167846118862\n",
      "Batch =  1675  Train Loss =  0.004073219775698329  SSIM =  0.7612711599135576\n",
      "Batch =  1700  Train Loss =  0.004057174170995816  SSIM =  0.761711592154906\n",
      "Batch =  1725  Train Loss =  0.004039509639386898  SSIM =  0.7621260017566923\n",
      "Batch =  1750  Train Loss =  0.004021610140002199  SSIM =  0.7627478670882327\n",
      "Batch =  1775  Train Loss =  0.004004915367457753  SSIM =  0.7628274583207889\n",
      "Batch =  1800  Train Loss =  0.003997482313748656  SSIM =  0.762992017250508\n",
      "Batch =  1825  Train Loss =  0.003990260829147564  SSIM =  0.7630783953711595\n",
      "Batch =  1850  Train Loss =  0.003975045963259716  SSIM =  0.7631701267470379\n",
      "Batch =  1875  Train Loss =  0.003966385292587801  SSIM =  0.7630796748856703\n",
      "Batch =  1900  Train Loss =  0.003954365208276278  SSIM =  0.7630832709744573\n",
      "Batch =  1925  Train Loss =  0.003942379153064488  SSIM =  0.7630092889593019\n",
      "Batch =  1950  Train Loss =  0.0039245018287329  SSIM =  0.7637193384327186\n",
      "Batch =  1975  Train Loss =  0.003916604200015552  SSIM =  0.7633838541843468\n",
      "Batch =  2000  Train Loss =  0.003908541710130521  SSIM =  0.7635736314300448\n",
      "Batch =  2025  Train Loss =  0.003901366672355015  SSIM =  0.7635635174516543\n",
      "Batch =  2050  Train Loss =  0.003888321618944202  SSIM =  0.7636797285752326\n",
      "Batch =  2075  Train Loss =  0.0038849749539400934  SSIM =  0.7637728717977024\n",
      "Batch =  2100  Train Loss =  0.0038804200677328656  SSIM =  0.7635104429171908\n",
      "Batch =  2125  Train Loss =  0.003872262284036397  SSIM =  0.7635398726340602\n",
      "Batch =  2150  Train Loss =  0.0038570733187817644  SSIM =  0.7637181857938683\n",
      "Batch =  2175  Train Loss =  0.0038499509352366386  SSIM =  0.7640253052714913\n",
      "Batch =  2200  Train Loss =  0.0038421303799524055  SSIM =  0.7641460591232913\n",
      "Loader #:  0  Epoch:  3 /  2500  -  Time:  0:04:14.529971 s - Validation Loss:  0.0  - Train Loss:  0.0038421303799524055  - train SSIM:  0.7641460591232913  validation SSIM:  1681.1213300712407\n",
      "Batch =  1  Train Loss =  0.004407905042171478  SSIM =  0.7359026670455933\n",
      "Batch =  25  Train Loss =  0.0033330025523900988  SSIM =  0.7499694055318833\n",
      "Batch =  50  Train Loss =  0.0032744977483525873  SSIM =  0.7574239367246628\n",
      "Batch =  75  Train Loss =  0.0032577535579912365  SSIM =  0.763893547852834\n",
      "Batch =  100  Train Loss =  0.0033946987130912022  SSIM =  0.7579010125994682\n",
      "Batch =  125  Train Loss =  0.003389844979625195  SSIM =  0.757779301404953\n",
      "Batch =  150  Train Loss =  0.0033672819059574976  SSIM =  0.7594020182887713\n",
      "Batch =  175  Train Loss =  0.0033676026969416332  SSIM =  0.7608020907640457\n",
      "Batch =  200  Train Loss =  0.003442245490296045  SSIM =  0.7597275232523679\n",
      "Batch =  225  Train Loss =  0.0033556030270281353  SSIM =  0.7617865499522951\n",
      "Batch =  250  Train Loss =  0.003290621491498314  SSIM =  0.7643130345344543\n",
      "Batch =  275  Train Loss =  0.0032104212201242758  SSIM =  0.7674980503862554\n",
      "Batch =  300  Train Loss =  0.0032125726779728817  SSIM =  0.766512639572223\n",
      "Batch =  325  Train Loss =  0.003203367604605424  SSIM =  0.7683613457129552\n",
      "Batch =  350  Train Loss =  0.0032475785993405486  SSIM =  0.7661030167341232\n",
      "Batch =  375  Train Loss =  0.0032972489243838937  SSIM =  0.7646918797492981\n",
      "Batch =  400  Train Loss =  0.0032513037138414804  SSIM =  0.766084882132709\n",
      "Batch =  425  Train Loss =  0.0032630156308604296  SSIM =  0.7660069795566447\n",
      "Batch =  450  Train Loss =  0.0032796002318435866  SSIM =  0.7658661712540521\n",
      "Batch =  475  Train Loss =  0.0032242629387244388  SSIM =  0.768389444664905\n",
      "Batch =  500  Train Loss =  0.003215905097022187  SSIM =  0.7692547520697117\n",
      "Batch =  525  Train Loss =  0.0032753868950420014  SSIM =  0.7684034242800304\n",
      "Batch =  550  Train Loss =  0.0032513958783502773  SSIM =  0.7698183226856319\n",
      "Batch =  575  Train Loss =  0.0032512890353657144  SSIM =  0.7699655559529429\n",
      "Batch =  600  Train Loss =  0.0032596470199738785  SSIM =  0.7695194439093271\n",
      "Batch =  625  Train Loss =  0.0032517082060221584  SSIM =  0.7705566878318787\n",
      "Batch =  650  Train Loss =  0.0032382927452160332  SSIM =  0.77066352725029\n",
      "Batch =  675  Train Loss =  0.003243338293439053  SSIM =  0.7705083601342307\n",
      "Batch =  700  Train Loss =  0.003234051624687189  SSIM =  0.771445413189275\n",
      "Batch =  725  Train Loss =  0.0032410296501094024  SSIM =  0.7706967420208043\n",
      "Batch =  750  Train Loss =  0.0032277537817523502  SSIM =  0.7710746114651362\n",
      "Batch =  775  Train Loss =  0.003239016468043349  SSIM =  0.7702846579061401\n",
      "Batch =  800  Train Loss =  0.003216883382519882  SSIM =  0.7716273623006419\n",
      "Batch =  825  Train Loss =  0.003205962502715093  SSIM =  0.7720266675271771\n",
      "Batch =  850  Train Loss =  0.0032035193665925525  SSIM =  0.7721810388082967\n",
      "Batch =  875  Train Loss =  0.003217688802995586  SSIM =  0.771856025240251\n",
      "Batch =  900  Train Loss =  0.0032245110499368617  SSIM =  0.7714978521275851\n",
      "Batch =  925  Train Loss =  0.003226854691826235  SSIM =  0.7710387990482755\n",
      "Batch =  950  Train Loss =  0.0032283828757537873  SSIM =  0.7705858657430661\n",
      "Batch =  975  Train Loss =  0.0032164372187495852  SSIM =  0.7710059570540221\n",
      "Batch =  1000  Train Loss =  0.0031875639103964206  SSIM =  0.7714560570158064\n",
      "Batch =  1025  Train Loss =  0.003183143500809376  SSIM =  0.7721240557439444\n",
      "Batch =  1050  Train Loss =  0.0031873237887824267  SSIM =  0.7717662172693582\n",
      "Batch =  1075  Train Loss =  0.0031855000254053752  SSIM =  0.7720909361818502\n",
      "Batch =  1100  Train Loss =  0.003184293624243847  SSIM =  0.7723298271440647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch =  1125  Train Loss =  0.003192560206102725  SSIM =  0.7719193821913666\n",
      "Batch =  1150  Train Loss =  0.003195160064813377  SSIM =  0.7718948610872031\n",
      "Batch =  1175  Train Loss =  0.0031991457140464535  SSIM =  0.7715363407927625\n",
      "Batch =  1200  Train Loss =  0.003207601931038274  SSIM =  0.7712726442795247\n",
      "Batch =  1225  Train Loss =  0.003207417959513675  SSIM =  0.7714798916633032\n",
      "Batch =  1250  Train Loss =  0.0032102135952329263  SSIM =  0.7716656857162714\n",
      "Batch =  1275  Train Loss =  0.0032180467825474253  SSIM =  0.7713639074476326\n",
      "Batch =  1300  Train Loss =  0.0032084929260264078  SSIM =  0.7715025214077188\n",
      "Batch =  1325  Train Loss =  0.003211693874481222  SSIM =  0.7715302766184762\n",
      "Batch =  1350  Train Loss =  0.0032204305844950594  SSIM =  0.7710920913048365\n",
      "Batch =  1375  Train Loss =  0.0032114532570862633  SSIM =  0.771312474686991\n",
      "Batch =  1400  Train Loss =  0.0032119661251116278  SSIM =  0.7713807400768357\n",
      "Batch =  1425  Train Loss =  0.0032006008882614735  SSIM =  0.771926213787836\n",
      "Batch =  1450  Train Loss =  0.003188488877815965  SSIM =  0.7724034029990434\n",
      "Batch =  1475  Train Loss =  0.0031927372089200416  SSIM =  0.7721611064192602\n",
      "Batch =  1500  Train Loss =  0.0031821301308033677  SSIM =  0.7725163190588356\n",
      "Batch =  1525  Train Loss =  0.003185088279767756  SSIM =  0.7723601538455878\n",
      "Batch =  1550  Train Loss =  0.0031954604512148144  SSIM =  0.772188156066883\n",
      "Batch =  1575  Train Loss =  0.0031835974698575836  SSIM =  0.7727601422511396\n",
      "Batch =  1600  Train Loss =  0.003171828676931909  SSIM =  0.7734950621449389\n",
      "Batch =  1625  Train Loss =  0.003161649422146953  SSIM =  0.7739332038553861\n",
      "Batch =  1650  Train Loss =  0.003155575030338668  SSIM =  0.774147764491764\n",
      "Batch =  1675  Train Loss =  0.003161568158122002  SSIM =  0.774045035681173\n",
      "Batch =  1700  Train Loss =  0.003155571483366657  SSIM =  0.7740274294531521\n",
      "Batch =  1725  Train Loss =  0.0031519247617984216  SSIM =  0.7740845031172469\n",
      "Batch =  1750  Train Loss =  0.003148825972623724  SSIM =  0.7740309265192066\n",
      "Batch =  1775  Train Loss =  0.003156559394597089  SSIM =  0.7738571153665093\n",
      "Batch =  1800  Train Loss =  0.0031508291461149283  SSIM =  0.7740980715076956\n",
      "Batch =  1825  Train Loss =  0.003145880993688479  SSIM =  0.7741435188483702\n",
      "Batch =  1850  Train Loss =  0.0031436966046237864  SSIM =  0.7743342985998134\n",
      "Batch =  1875  Train Loss =  0.0031560579826279233  SSIM =  0.7739817583898704\n",
      "Batch =  1900  Train Loss =  0.003153710543395535  SSIM =  0.7740291101387456\n",
      "Batch =  1925  Train Loss =  0.003149543157944988  SSIM =  0.7740222981475391\n",
      "Batch =  1950  Train Loss =  0.0031571277260380344  SSIM =  0.7736575756947963\n",
      "Batch =  1975  Train Loss =  0.0031525977677960373  SSIM =  0.7738872809972189\n",
      "Batch =  2000  Train Loss =  0.0031529890218225773  SSIM =  0.773509929517284\n",
      "Batch =  2025  Train Loss =  0.0031474011401461874  SSIM =  0.7739073644688835\n",
      "Batch =  2050  Train Loss =  0.0031469452303902407  SSIM =  0.7739358497774456\n",
      "Batch =  2075  Train Loss =  0.003143227198133133  SSIM =  0.7741333417091744\n",
      "Batch =  2100  Train Loss =  0.0031406201335180198  SSIM =  0.7742772417426819\n",
      "Batch =  2125  Train Loss =  0.0031485513428445248  SSIM =  0.7739194673878305\n",
      "Batch =  2150  Train Loss =  0.0031540850441101505  SSIM =  0.7737997077665356\n",
      "Batch =  2175  Train Loss =  0.0031551774366940745  SSIM =  0.7739030404423161\n",
      "Batch =  2200  Train Loss =  0.003150311382117004  SSIM =  0.7741760709085925\n",
      "Loader #:  0  Epoch:  4 /  2500  -  Time:  0:04:13.138069 s - Validation Loss:  0.0  - Train Loss:  0.003150311382117004  - train SSIM:  0.7741760709085925  validation SSIM:  1703.1873559989035\n",
      "Batch =  1  Train Loss =  0.002051013056188822  SSIM =  0.8424273729324341\n",
      "Batch =  25  Train Loss =  0.002098317870986648  SSIM =  0.8329901099205017\n",
      "Batch =  50  Train Loss =  0.0025093650809139946  SSIM =  0.8095747473835945\n",
      "Batch =  75  Train Loss =  0.0025296748912660406  SSIM =  0.8041695562998453\n",
      "Batch =  100  Train Loss =  0.0025397754820005503  SSIM =  0.8026567152142525\n",
      "Batch =  125  Train Loss =  0.0026880542811704797  SSIM =  0.7955640420913697\n",
      "Batch =  150  Train Loss =  0.002686582684497504  SSIM =  0.7918540691335996\n",
      "Batch =  175  Train Loss =  0.0027170865320035125  SSIM =  0.7901341481719698\n",
      "Batch =  200  Train Loss =  0.002707805605168687  SSIM =  0.7888249468058348\n",
      "Batch =  225  Train Loss =  0.002706240482705956  SSIM =  0.7889770393901401\n",
      "Batch =  250  Train Loss =  0.0028177975312573835  SSIM =  0.7830884057283402\n",
      "Batch =  275  Train Loss =  0.0028448068410877815  SSIM =  0.7848379964178259\n",
      "Batch =  300  Train Loss =  0.0028595396026503293  SSIM =  0.7840143011510372\n",
      "Batch =  325  Train Loss =  0.0028611638008330304  SSIM =  0.7834083635073442\n",
      "Batch =  350  Train Loss =  0.002909476040430101  SSIM =  0.7818440877539771\n",
      "Batch =  375  Train Loss =  0.0029633798121164244  SSIM =  0.7789501313765844\n",
      "Batch =  400  Train Loss =  0.002987268199794926  SSIM =  0.7777354875206948\n",
      "Batch =  425  Train Loss =  0.003014745844719822  SSIM =  0.7764933075974969\n",
      "Batch =  450  Train Loss =  0.0030159288378328914  SSIM =  0.776837780740526\n",
      "Batch =  475  Train Loss =  0.0029908673999536977  SSIM =  0.7778319124485317\n",
      "Batch =  500  Train Loss =  0.00296743851643987  SSIM =  0.7791730095744133\n",
      "Batch =  525  Train Loss =  0.0029801645697582336  SSIM =  0.7784663473708289\n",
      "Batch =  550  Train Loss =  0.00299315961297411  SSIM =  0.7779473674568262\n",
      "Batch =  575  Train Loss =  0.00301540241938631  SSIM =  0.7772240351075711\n",
      "Batch =  600  Train Loss =  0.003034364130459532  SSIM =  0.7757142764081557\n",
      "Batch =  625  Train Loss =  0.0030386755496030673  SSIM =  0.7754251694440841\n",
      "Batch =  650  Train Loss =  0.003051840894824216  SSIM =  0.7753545259970885\n",
      "Batch =  675  Train Loss =  0.003060285813731348  SSIM =  0.775035037000974\n",
      "Batch =  700  Train Loss =  0.0030606713956928744  SSIM =  0.7754716434436185\n",
      "Batch =  725  Train Loss =  0.0030539354271905753  SSIM =  0.776516440839603\n",
      "Batch =  750  Train Loss =  0.0030566319093923086  SSIM =  0.7765829689105351\n",
      "Batch =  775  Train Loss =  0.003039420530474144  SSIM =  0.777134319639975\n",
      "Batch =  800  Train Loss =  0.003037514572879445  SSIM =  0.7774788070097566\n",
      "Batch =  825  Train Loss =  0.003032697881377923  SSIM =  0.7778067309025563\n",
      "Batch =  850  Train Loss =  0.003036093613259522  SSIM =  0.7778657573812148\n",
      "Batch =  875  Train Loss =  0.003045595304125787  SSIM =  0.777795623745237\n",
      "Batch =  900  Train Loss =  0.0030500835749585854  SSIM =  0.77725743520591\n",
      "Batch =  925  Train Loss =  0.003049335730894519  SSIM =  0.7776397784014006\n",
      "Batch =  950  Train Loss =  0.0030446716828277875  SSIM =  0.7779146593652273\n",
      "Batch =  975  Train Loss =  0.00304605302211339  SSIM =  0.7776275100310643\n",
      "Batch =  1000  Train Loss =  0.003036720941818203  SSIM =  0.778462123721838\n",
      "Batch =  1025  Train Loss =  0.0030448338186521676  SSIM =  0.7783860427141189\n",
      "Batch =  1050  Train Loss =  0.0030633808043092443  SSIM =  0.7774797117000535\n",
      "Batch =  1075  Train Loss =  0.00306658964776055  SSIM =  0.7770750314551731\n",
      "Batch =  1100  Train Loss =  0.003078731167280983  SSIM =  0.7767636445977472\n",
      "Batch =  1125  Train Loss =  0.0030748495402771773  SSIM =  0.7768146790530946\n",
      "Batch =  1150  Train Loss =  0.0030720280806417577  SSIM =  0.7770535664713901\n",
      "Batch =  1175  Train Loss =  0.0030696894595662054  SSIM =  0.7772493850043479\n",
      "Batch =  1200  Train Loss =  0.0030731830291066825  SSIM =  0.7769884407892823\n",
      "Batch =  1225  Train Loss =  0.0030776540150186426  SSIM =  0.7767774112005623\n",
      "Batch =  1250  Train Loss =  0.003073624757502694  SSIM =  0.7770120448470116\n",
      "Batch =  1275  Train Loss =  0.003076253188831056  SSIM =  0.7768679582488303\n",
      "Batch =  1300  Train Loss =  0.0030798235891695814  SSIM =  0.7769938843296125\n",
      "Batch =  1325  Train Loss =  0.0030804690894921188  SSIM =  0.7766631607936239\n",
      "Batch =  1350  Train Loss =  0.00308812766625865  SSIM =  0.7764517859038379\n",
      "Batch =  1375  Train Loss =  0.0030773472865217957  SSIM =  0.7769302347004413\n",
      "Batch =  1400  Train Loss =  0.0030895952033980784  SSIM =  0.7764756011829844\n",
      "Batch =  1425  Train Loss =  0.003089176102984582  SSIM =  0.7762484365780102\n",
      "Batch =  1450  Train Loss =  0.0030909013911652188  SSIM =  0.7762655908254714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch =  1475  Train Loss =  0.0030913917214108505  SSIM =  0.7762815603639109\n",
      "Batch =  1500  Train Loss =  0.0030982607500918673  SSIM =  0.7759709618762135\n",
      "Batch =  1525  Train Loss =  0.003093203701084617  SSIM =  0.7762438558921462\n",
      "Batch =  1550  Train Loss =  0.003090053397078713  SSIM =  0.7765819236900537\n",
      "Batch =  1575  Train Loss =  0.0030942651380633287  SSIM =  0.7763731043844943\n",
      "Batch =  1600  Train Loss =  0.0030874161937299504  SSIM =  0.7765915574110113\n",
      "Batch =  1625  Train Loss =  0.0030953907664202584  SSIM =  0.7766198519720481\n",
      "Batch =  1650  Train Loss =  0.0030918775815216852  SSIM =  0.7768666698422396\n",
      "Batch =  1675  Train Loss =  0.0030922511251514724  SSIM =  0.7768577472554214\n",
      "Batch =  1700  Train Loss =  0.00309661461145665  SSIM =  0.7766431876381531\n",
      "Batch =  1725  Train Loss =  0.003094222648971903  SSIM =  0.7768023140512514\n",
      "Batch =  1750  Train Loss =  0.003091253862974034  SSIM =  0.776823261531336\n",
      "Batch =  1775  Train Loss =  0.00308206279584187  SSIM =  0.7774354097436011\n",
      "Batch =  1800  Train Loss =  0.0030837708575078672  SSIM =  0.7773329317880173\n",
      "Batch =  1825  Train Loss =  0.003080060733155322  SSIM =  0.7776330395491973\n",
      "Batch =  1850  Train Loss =  0.003087212491294407  SSIM =  0.7772289009170757\n",
      "Batch =  1875  Train Loss =  0.003098283276606041  SSIM =  0.7769513750135899\n",
      "Batch =  1900  Train Loss =  0.0030988812109066393  SSIM =  0.7769586020688477\n",
      "Batch =  1925  Train Loss =  0.0030897675575643126  SSIM =  0.7775058242233542\n",
      "Batch =  1950  Train Loss =  0.003093151373091715  SSIM =  0.7772973966961487\n",
      "Batch =  1975  Train Loss =  0.003097836849326857  SSIM =  0.7771513947486123\n",
      "Batch =  2000  Train Loss =  0.003103533652392798  SSIM =  0.7771391495224088\n",
      "Batch =  2025  Train Loss =  0.003101164720892147  SSIM =  0.7771708919218293\n",
      "Batch =  2050  Train Loss =  0.0031118528419259435  SSIM =  0.7767567039244786\n",
      "Batch =  2075  Train Loss =  0.0031156457307136964  SSIM =  0.7764900847353849\n",
      "Batch =  2100  Train Loss =  0.003115674828420327  SSIM =  0.7764775465907795\n",
      "Batch =  2125  Train Loss =  0.0031159022634931126  SSIM =  0.7765247907480772\n",
      "Batch =  2150  Train Loss =  0.003114412604888013  SSIM =  0.7767867758436952\n",
      "Batch =  2175  Train Loss =  0.003111814561463229  SSIM =  0.7768550501387009\n",
      "Batch =  2200  Train Loss =  0.003112789627193706  SSIM =  0.7769042920304293\n",
      "Loader #:  0  Epoch:  5 /  2500  -  Time:  0:04:14.991051 s - Validation Loss:  0.0  - Train Loss:  0.003112789627193706  - train SSIM:  0.7769042920304293  validation SSIM:  1709.1894424669445\n",
      "Batch =  1  Train Loss =  0.004757515154778957  SSIM =  0.738088071346283\n",
      "Batch =  25  Train Loss =  0.0036819946346804498  SSIM =  0.7662518787384033\n",
      "Batch =  50  Train Loss =  0.003680830716621131  SSIM =  0.7619540846347809\n",
      "Batch =  75  Train Loss =  0.003558442737751951  SSIM =  0.7623990607261658\n",
      "Batch =  100  Train Loss =  0.0035362528142286465  SSIM =  0.7639042785763741\n",
      "Batch =  125  Train Loss =  0.0033995961626060306  SSIM =  0.7696245234012604\n",
      "Batch =  150  Train Loss =  0.003253778760166218  SSIM =  0.7742840580145518\n",
      "Batch =  175  Train Loss =  0.003264126026271177  SSIM =  0.7746977952548436\n",
      "Batch =  200  Train Loss =  0.0033054419729160144  SSIM =  0.774154472798109\n",
      "Batch =  225  Train Loss =  0.0032799818206371532  SSIM =  0.775890425907241\n",
      "Batch =  250  Train Loss =  0.0032400807207450272  SSIM =  0.7774632021784782\n",
      "Batch =  275  Train Loss =  0.0031948228633369912  SSIM =  0.7785106003826314\n",
      "Batch =  300  Train Loss =  0.0031879455803815898  SSIM =  0.7776996745665868\n",
      "Batch =  325  Train Loss =  0.003133606497162523  SSIM =  0.7794825157752404\n",
      "Batch =  350  Train Loss =  0.003130202598404139  SSIM =  0.7793694981506892\n",
      "Batch =  375  Train Loss =  0.003142626080662012  SSIM =  0.779322517434756\n",
      "Batch =  400  Train Loss =  0.0031258724372310097  SSIM =  0.7802669358998537\n",
      "Batch =  425  Train Loss =  0.0031126224556389977  SSIM =  0.7797714048273423\n",
      "Batch =  450  Train Loss =  0.0031220694721236617  SSIM =  0.7801883019010226\n",
      "Batch =  475  Train Loss =  0.0031414710164780874  SSIM =  0.7796647612044686\n",
      "Batch =  500  Train Loss =  0.0031243392135947945  SSIM =  0.7802634607553482\n",
      "Batch =  525  Train Loss =  0.003088884676052701  SSIM =  0.7807862929219291\n",
      "Batch =  550  Train Loss =  0.003106855506087992  SSIM =  0.7794749808853323\n",
      "Batch =  575  Train Loss =  0.003082030808765684  SSIM =  0.7806601821339648\n",
      "Batch =  600  Train Loss =  0.003090531363801953  SSIM =  0.7802258506913979\n",
      "Batch =  625  Train Loss =  0.0030645504937972874  SSIM =  0.7813115933656692\n",
      "Batch =  650  Train Loss =  0.0030776582493070655  SSIM =  0.7805267154253446\n",
      "Batch =  675  Train Loss =  0.003073665319782406  SSIM =  0.7807526785797543\n",
      "Batch =  700  Train Loss =  0.003059209547500359  SSIM =  0.7814355803600379\n",
      "Batch =  725  Train Loss =  0.003087405577876027  SSIM =  0.7804803117184803\n",
      "Batch =  750  Train Loss =  0.003083542679766348  SSIM =  0.7813047470500072\n",
      "Batch =  775  Train Loss =  0.0030685306900939454  SSIM =  0.7822108926936503\n",
      "Batch =  800  Train Loss =  0.0030617412425817747  SSIM =  0.7828767092758789\n",
      "Batch =  825  Train Loss =  0.003072013905374192  SSIM =  0.7820453262374256\n",
      "Batch =  850  Train Loss =  0.0030765525968094796  SSIM =  0.7816667977895807\n",
      "Batch =  875  Train Loss =  0.0030738085445315977  SSIM =  0.7810463818183967\n",
      "Batch =  900  Train Loss =  0.0030762507029349863  SSIM =  0.7804068436928921\n",
      "Batch =  925  Train Loss =  0.0030718022621570255  SSIM =  0.7803690105072549\n",
      "Batch =  950  Train Loss =  0.0030541229836403786  SSIM =  0.7816898341123995\n",
      "Batch =  975  Train Loss =  0.0030341175651529993  SSIM =  0.7825854318149579\n",
      "Batch =  1000  Train Loss =  0.0030217303135286785  SSIM =  0.7831672667600215\n",
      "Batch =  1025  Train Loss =  0.003041144820858679  SSIM =  0.7824417234666464\n",
      "Batch =  1050  Train Loss =  0.0030522533267642194  SSIM =  0.7822789791403781\n",
      "Batch =  1075  Train Loss =  0.003042806748579646  SSIM =  0.7822041393469933\n",
      "Batch =  1100  Train Loss =  0.0030343439388062425  SSIM =  0.7819664460048079\n",
      "Batch =  1125  Train Loss =  0.003018668979496902  SSIM =  0.7825936809149053\n",
      "Batch =  1150  Train Loss =  0.003013143078866921  SSIM =  0.7831714384873276\n",
      "Batch =  1175  Train Loss =  0.003022637948569683  SSIM =  0.782791661433083\n",
      "Batch =  1200  Train Loss =  0.0030220292228113977  SSIM =  0.7830014570088436\n",
      "Batch =  1225  Train Loss =  0.003020363662389525  SSIM =  0.7833413200506142\n",
      "Batch =  1250  Train Loss =  0.0030250391814275646  SSIM =  0.7829622831016779\n",
      "Batch =  1275  Train Loss =  0.003018217928143784  SSIM =  0.7830917437549899\n",
      "Batch =  1300  Train Loss =  0.0030298687425070405  SSIM =  0.7828361490168251\n",
      "Batch =  1325  Train Loss =  0.0030278283475682947  SSIM =  0.7827941487673319\n",
      "Batch =  1350  Train Loss =  0.003031225580823418  SSIM =  0.7825637129307897\n",
      "Batch =  1375  Train Loss =  0.0030218243153913964  SSIM =  0.7830954541970383\n",
      "Batch =  1400  Train Loss =  0.0030245988319906507  SSIM =  0.7830829509906471\n",
      "Batch =  1425  Train Loss =  0.0030201113168783324  SSIM =  0.7835275990738158\n",
      "Batch =  1450  Train Loss =  0.003037412673393505  SSIM =  0.7826782300374632\n",
      "Batch =  1475  Train Loss =  0.0030465579120406753  SSIM =  0.7821605135348894\n",
      "Batch =  1500  Train Loss =  0.0030465283101948444  SSIM =  0.7820880753422776\n",
      "Batch =  1525  Train Loss =  0.003054434956179275  SSIM =  0.781678707611854\n",
      "Batch =  1550  Train Loss =  0.0030542883777431393  SSIM =  0.7817858539689934\n",
      "Batch =  1575  Train Loss =  0.0030550343950837113  SSIM =  0.7821942739661724\n",
      "Batch =  1600  Train Loss =  0.0030600377822247535  SSIM =  0.7815950772562065\n",
      "Batch =  1625  Train Loss =  0.003065579412245335  SSIM =  0.7815930571303917\n",
      "Batch =  1650  Train Loss =  0.0030670782471354345  SSIM =  0.7813092801178043\n",
      "Batch =  1675  Train Loss =  0.003071406913405187  SSIM =  0.7810538389722803\n",
      "Batch =  1700  Train Loss =  0.0030715912329084376  SSIM =  0.7813344829595265\n",
      "Batch =  1725  Train Loss =  0.0030782994509090844  SSIM =  0.7809832224487394\n",
      "Batch =  1750  Train Loss =  0.0030691373756604402  SSIM =  0.7813807167517287\n",
      "Batch =  1775  Train Loss =  0.0030689925193527557  SSIM =  0.7813454723211242\n",
      "Batch =  1800  Train Loss =  0.0030750314085566466  SSIM =  0.7808660881987048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch =  1825  Train Loss =  0.0030742654240489236  SSIM =  0.7811169219690643\n",
      "Batch =  1850  Train Loss =  0.003087057042654566  SSIM =  0.7806518769928732\n",
      "Batch =  1875  Train Loss =  0.0030915141214694205  SSIM =  0.7801189768572648\n",
      "Batch =  1900  Train Loss =  0.0030810025621950013  SSIM =  0.7806921087460298\n",
      "Batch =  1925  Train Loss =  0.003080845190138048  SSIM =  0.7805148883473564\n",
      "Batch =  1950  Train Loss =  0.003085680568004206  SSIM =  0.7801362512375299\n",
      "Batch =  1975  Train Loss =  0.0030821947687043206  SSIM =  0.780556160311533\n",
      "Batch =  2000  Train Loss =  0.003087416369577113  SSIM =  0.7803976668287068\n",
      "Batch =  2025  Train Loss =  0.003096990434784891  SSIM =  0.7799243863534044\n",
      "Batch =  2050  Train Loss =  0.003102531190869319  SSIM =  0.779629661379064\n",
      "Batch =  2075  Train Loss =  0.0031004698348795455  SSIM =  0.7796815175787513\n",
      "Batch =  2100  Train Loss =  0.0031019428419780783  SSIM =  0.7796739663077252\n",
      "Batch =  2125  Train Loss =  0.003103516168700641  SSIM =  0.779494128118543\n",
      "Batch =  2150  Train Loss =  0.003098238830774113  SSIM =  0.7796748939537724\n",
      "Batch =  2175  Train Loss =  0.00309809544420769  SSIM =  0.7794557097348673\n",
      "Batch =  2200  Train Loss =  0.0030949763334567914  SSIM =  0.7797593446380713\n",
      "Loader #:  0  Epoch:  6 /  2500  -  Time:  0:04:18.983933 s - Validation Loss:  0.0  - Train Loss:  0.0030949763334567914  - train SSIM:  0.7797593446380713  validation SSIM:  1715.4705582037568\n",
      "Batch =  1  Train Loss =  0.0013851517578586936  SSIM =  0.8162531554698944\n",
      "Batch =  25  Train Loss =  0.0032927355403080585  SSIM =  0.7793641459941864\n",
      "Batch =  50  Train Loss =  0.0032444576028501614  SSIM =  0.7725604063272477\n",
      "Batch =  75  Train Loss =  0.0030353353347163646  SSIM =  0.7839081106583278\n",
      "Batch =  100  Train Loss =  0.003104078019678127  SSIM =  0.7734353062510491\n",
      "Batch =  125  Train Loss =  0.003213996767764911  SSIM =  0.770715213060379\n",
      "Batch =  150  Train Loss =  0.003116055787734998  SSIM =  0.7752822695175806\n",
      "Batch =  175  Train Loss =  0.003116134264273569  SSIM =  0.7774485891205923\n",
      "Batch =  200  Train Loss =  0.0030334894350380636  SSIM =  0.7825605945289135\n",
      "Batch =  225  Train Loss =  0.0029829958285618988  SSIM =  0.7861662908395132\n",
      "Batch =  250  Train Loss =  0.0029568436270346866  SSIM =  0.7873614041805267\n",
      "Batch =  275  Train Loss =  0.0028956086301795123  SSIM =  0.7902857871489092\n",
      "Batch =  300  Train Loss =  0.0029058797412047474  SSIM =  0.7903565016388893\n",
      "Batch =  325  Train Loss =  0.0029212081592637474  SSIM =  0.7893807881612044\n",
      "Batch =  350  Train Loss =  0.0028948349970492667  SSIM =  0.7870383616430419\n",
      "Batch =  375  Train Loss =  0.002891892841629063  SSIM =  0.7871515385309855\n",
      "Batch =  400  Train Loss =  0.002918055568370619  SSIM =  0.786310194246471\n",
      "Batch =  425  Train Loss =  0.0028916263990515076  SSIM =  0.7866839723026051\n",
      "Batch =  450  Train Loss =  0.0028922930257006857  SSIM =  0.786710595091184\n",
      "Batch =  475  Train Loss =  0.0028904008191985714  SSIM =  0.7864785243962941\n",
      "Batch =  500  Train Loss =  0.002888366641534958  SSIM =  0.7865446292757988\n",
      "Batch =  525  Train Loss =  0.0029449557656577477  SSIM =  0.7838631403730029\n",
      "Batch =  550  Train Loss =  0.002946987898374738  SSIM =  0.7835742354122075\n",
      "Batch =  575  Train Loss =  0.0029418931769348844  SSIM =  0.7846606215186741\n",
      "Batch =  600  Train Loss =  0.0029528984560844643  SSIM =  0.7849831796437502\n",
      "Batch =  625  Train Loss =  0.002971620409702882  SSIM =  0.7839181887626648\n",
      "Batch =  650  Train Loss =  0.0029716621349619417  SSIM =  0.7836412979547794\n",
      "Batch =  675  Train Loss =  0.0029714255568278193  SSIM =  0.7833446257864988\n",
      "Batch =  700  Train Loss =  0.002995701014671275  SSIM =  0.7823661446358476\n",
      "Batch =  725  Train Loss =  0.003015896753961991  SSIM =  0.7819302520875273\n",
      "Batch =  750  Train Loss =  0.0030184864981177575  SSIM =  0.7813739975889524\n",
      "Batch =  775  Train Loss =  0.003015509338696457  SSIM =  0.7814828319703379\n",
      "Batch =  800  Train Loss =  0.0030338452451542253  SSIM =  0.7809590449184179\n",
      "Batch =  825  Train Loss =  0.003043069668946731  SSIM =  0.780840995293675\n",
      "Batch =  850  Train Loss =  0.003020433907821665  SSIM =  0.7816072896122932\n",
      "Batch =  875  Train Loss =  0.003031206016328984  SSIM =  0.7813261017629078\n",
      "Batch =  900  Train Loss =  0.003013685103990914  SSIM =  0.7824097205864059\n",
      "Batch =  925  Train Loss =  0.0030159090130552147  SSIM =  0.7820968262891511\n",
      "Batch =  950  Train Loss =  0.003037003802693155  SSIM =  0.7813753039272208\n",
      "Batch =  975  Train Loss =  0.003043668104353576  SSIM =  0.7807784541295125\n",
      "Batch =  1000  Train Loss =  0.003052560994954547  SSIM =  0.7802136038392782\n",
      "Batch =  1025  Train Loss =  0.0030835847733485535  SSIM =  0.7789321211053104\n",
      "Batch =  1050  Train Loss =  0.003095227002548719  SSIM =  0.7784552947254408\n",
      "Batch =  1075  Train Loss =  0.00308643403173381  SSIM =  0.7788045939040739\n",
      "Batch =  1100  Train Loss =  0.0030701648500028322  SSIM =  0.7795772203261202\n",
      "Batch =  1125  Train Loss =  0.0030729865378493234  SSIM =  0.778993694530593\n",
      "Batch =  1150  Train Loss =  0.003067419766954592  SSIM =  0.7790908398576405\n",
      "Batch =  1175  Train Loss =  0.0030764624833604917  SSIM =  0.7785157107545975\n",
      "Batch =  1200  Train Loss =  0.003077465314369571  SSIM =  0.7785521848623951\n",
      "Batch =  1225  Train Loss =  0.0030756664491847765  SSIM =  0.7783937145495902\n",
      "Batch =  1250  Train Loss =  0.003073403568007052  SSIM =  0.7788293478608131\n",
      "Batch =  1275  Train Loss =  0.0030600743443139045  SSIM =  0.7792611857021556\n",
      "Batch =  1300  Train Loss =  0.0030552374556338273  SSIM =  0.7796978801718125\n",
      "Batch =  1325  Train Loss =  0.003052273518713367  SSIM =  0.7800162811436743\n",
      "Batch =  1350  Train Loss =  0.0030562652372410175  SSIM =  0.7801574389360569\n",
      "Batch =  1375  Train Loss =  0.003057580993934111  SSIM =  0.7802039347453551\n",
      "Batch =  1400  Train Loss =  0.0030564929923587313  SSIM =  0.7802429777703115\n",
      "Batch =  1425  Train Loss =  0.0030610960774969213  SSIM =  0.7802443123177478\n",
      "Batch =  1450  Train Loss =  0.0030618057815370086  SSIM =  0.7805199511400585\n",
      "Batch =  1475  Train Loss =  0.0030628533036965023  SSIM =  0.7803800180200803\n",
      "Batch =  1500  Train Loss =  0.0030497100349069417  SSIM =  0.7810446961224079\n",
      "Batch =  1525  Train Loss =  0.0030659900124081732  SSIM =  0.7806385483995812\n",
      "Batch =  1550  Train Loss =  0.0030892579976728395  SSIM =  0.7793967155583443\n",
      "Batch =  1575  Train Loss =  0.003080383229950842  SSIM =  0.7796921749909719\n",
      "Batch =  1600  Train Loss =  0.0030829797004935243  SSIM =  0.7796354773640632\n",
      "Batch =  1625  Train Loss =  0.0030756869132075315  SSIM =  0.7797541432930873\n",
      "Batch =  1650  Train Loss =  0.003078369185457246  SSIM =  0.7797637838034919\n",
      "Batch =  1675  Train Loss =  0.0030775860827433558  SSIM =  0.7799579985373056\n",
      "Batch =  1700  Train Loss =  0.0030700692211650996  SSIM =  0.7801855051254525\n",
      "Batch =  1725  Train Loss =  0.003072467338963601  SSIM =  0.7798651050484698\n",
      "Batch =  1750  Train Loss =  0.003068299605269983  SSIM =  0.7801886865496636\n",
      "Batch =  1775  Train Loss =  0.0030665210486163423  SSIM =  0.7801312412174655\n",
      "Batch =  1800  Train Loss =  0.003070074034694699  SSIM =  0.7795053755574756\n",
      "Batch =  1825  Train Loss =  0.003064244630601423  SSIM =  0.7798259707101404\n",
      "Batch =  1850  Train Loss =  0.003072495820965127  SSIM =  0.7797303034485997\n",
      "Batch =  1875  Train Loss =  0.0030673007484603053  SSIM =  0.7800380040566126\n",
      "Batch =  1900  Train Loss =  0.00306565409085842  SSIM =  0.7801587147069605\n",
      "Batch =  1925  Train Loss =  0.0030688872028145236  SSIM =  0.780202478789664\n",
      "Batch =  1950  Train Loss =  0.003065770991864459  SSIM =  0.7802540241831388\n",
      "Batch =  1975  Train Loss =  0.0030722236830838428  SSIM =  0.7800309949660603\n",
      "Batch =  2000  Train Loss =  0.003075921242190816  SSIM =  0.779866472415626\n",
      "Batch =  2025  Train Loss =  0.0030739485292859306  SSIM =  0.7799433500236935\n",
      "Batch =  2050  Train Loss =  0.0030785701388178007  SSIM =  0.7797200619665587\n",
      "Batch =  2075  Train Loss =  0.003073459228378698  SSIM =  0.7800519013189408\n",
      "Batch =  2100  Train Loss =  0.003067787066358419  SSIM =  0.7800328631841001\n",
      "Batch =  2125  Train Loss =  0.0030802744928651545  SSIM =  0.7796448883922661\n",
      "Batch =  2150  Train Loss =  0.0030809836176854863  SSIM =  0.779641382560827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch =  2175  Train Loss =  0.0030947238768868407  SSIM =  0.778871737757976\n",
      "Batch =  2200  Train Loss =  0.003102096862620039  SSIM =  0.778642097208649\n",
      "Loader #:  0  Epoch:  7 /  2500  -  Time:  0:04:09.132578 s - Validation Loss:  0.0  - Train Loss:  0.003102096862620039  - train SSIM:  0.778642097208649  validation SSIM:  1713.0126138590276\n",
      "Batch =  1  Train Loss =  0.0034896358847618103  SSIM =  0.7317146956920624\n",
      "Batch =  25  Train Loss =  0.0028736126003786923  SSIM =  0.7915590351819992\n",
      "Batch =  50  Train Loss =  0.002777359586325474  SSIM =  0.7800050657987595\n",
      "Batch =  75  Train Loss =  0.002677100361712898  SSIM =  0.7844027398029964\n",
      "Batch =  100  Train Loss =  0.002814306407526601  SSIM =  0.7801142793893814\n",
      "Batch =  125  Train Loss =  0.0028487937869504094  SSIM =  0.7833125944137573\n",
      "Batch =  150  Train Loss =  0.002863559191270421  SSIM =  0.7836991451183954\n",
      "Batch =  175  Train Loss =  0.0028464900642367346  SSIM =  0.7846872872114181\n",
      "Batch =  200  Train Loss =  0.002904391955016763  SSIM =  0.7829908211529255\n",
      "Batch =  225  Train Loss =  0.0029721961954944872  SSIM =  0.7788890837960774\n",
      "Batch =  250  Train Loss =  0.002963357225351501  SSIM =  0.7800887277722359\n",
      "Batch =  275  Train Loss =  0.002985027635480616  SSIM =  0.7797811811078679\n",
      "Batch =  300  Train Loss =  0.0030353091046466337  SSIM =  0.7768041710058848\n",
      "Batch =  325  Train Loss =  0.0030424489618655153  SSIM =  0.7756745845537919\n",
      "Batch =  350  Train Loss =  0.0030569974995367896  SSIM =  0.7753281577995845\n",
      "Batch =  375  Train Loss =  0.003009079441234159  SSIM =  0.7773798065582911\n",
      "Batch =  400  Train Loss =  0.003035059096619079  SSIM =  0.7761545377224683\n",
      "Batch =  425  Train Loss =  0.003015169038074365  SSIM =  0.7767492679988637\n",
      "Batch =  450  Train Loss =  0.003038099354825034  SSIM =  0.775871306459109\n",
      "Batch =  475  Train Loss =  0.0030268061497726625  SSIM =  0.7768747953992141\n",
      "Batch =  500  Train Loss =  0.0030251153051794972  SSIM =  0.7766188164949417\n",
      "Batch =  525  Train Loss =  0.0029892925993217864  SSIM =  0.7789662675062815\n",
      "Batch =  550  Train Loss =  0.0030164800812649034  SSIM =  0.7780350285158916\n",
      "Batch =  575  Train Loss =  0.0030041383858517056  SSIM =  0.7791790781785612\n",
      "Batch =  600  Train Loss =  0.003011601532513547  SSIM =  0.7791313877142966\n",
      "Batch =  625  Train Loss =  0.0029818105431972074  SSIM =  0.7806419458329678\n",
      "Batch =  650  Train Loss =  0.002990496197084072  SSIM =  0.7808914113904422\n",
      "Batch =  675  Train Loss =  0.003007256278091248  SSIM =  0.7803097866751529\n",
      "Batch =  700  Train Loss =  0.003022012858896882  SSIM =  0.7798565952373403\n",
      "Batch =  725  Train Loss =  0.0030407898434526128  SSIM =  0.7791583484514006\n",
      "Batch =  750  Train Loss =  0.003083284110277115  SSIM =  0.7779568527638913\n",
      "Batch =  775  Train Loss =  0.0030920315282832412  SSIM =  0.7784131247670419\n",
      "Batch =  800  Train Loss =  0.0031167877162624792  SSIM =  0.7776324132550507\n",
      "Batch =  825  Train Loss =  0.0030949690125028914  SSIM =  0.7785196681907682\n",
      "Batch =  850  Train Loss =  0.0030866130957592224  SSIM =  0.7787513419372194\n",
      "Batch =  875  Train Loss =  0.0031008854210210434  SSIM =  0.7784945955531938\n",
      "Batch =  900  Train Loss =  0.003099704739854335  SSIM =  0.7777056143267287\n",
      "Batch =  925  Train Loss =  0.003114444550387939  SSIM =  0.7775616754792832\n",
      "Batch =  950  Train Loss =  0.0031125320877885983  SSIM =  0.7775633019757898\n",
      "Batch =  975  Train Loss =  0.003108856614861184  SSIM =  0.7773540826256459\n",
      "Batch =  1000  Train Loss =  0.0031125473253341626  SSIM =  0.7773234864398837\n",
      "Batch =  1025  Train Loss =  0.003110762654942824  SSIM =  0.7772439512174304\n",
      "Batch =  1050  Train Loss =  0.003096234366350386  SSIM =  0.7784359900937194\n",
      "Batch =  1075  Train Loss =  0.0030958771930742184  SSIM =  0.7787368495727695\n",
      "Batch =  1100  Train Loss =  0.003104946213147328  SSIM =  0.7781691986931996\n",
      "Batch =  1125  Train Loss =  0.003097984268524063  SSIM =  0.7783583868278398\n",
      "Batch =  1150  Train Loss =  0.003103721551154234  SSIM =  0.7775313076830429\n",
      "Batch =  1175  Train Loss =  0.0031027848745988525  SSIM =  0.7777425598907978\n",
      "Batch =  1200  Train Loss =  0.0030970204423041046  SSIM =  0.7777477192195753\n",
      "Batch =  1225  Train Loss =  0.0030914550520151816  SSIM =  0.7779201367375802\n",
      "Batch =  1250  Train Loss =  0.0030903853900148535  SSIM =  0.7781009341895581\n",
      "Batch =  1275  Train Loss =  0.003076692131546545  SSIM =  0.7785746037551001\n",
      "Batch =  1300  Train Loss =  0.003073021209554729  SSIM =  0.778417204670035\n",
      "Batch =  1325  Train Loss =  0.0030799728084491777  SSIM =  0.7783779114316095\n",
      "Batch =  1350  Train Loss =  0.0030826767981677592  SSIM =  0.7781291824524049\n",
      "Batch =  1375  Train Loss =  0.003091934282152744  SSIM =  0.77773464202339\n",
      "Batch =  1400  Train Loss =  0.0031000347969633627  SSIM =  0.7774526912346482\n",
      "Batch =  1425  Train Loss =  0.0030933605943165164  SSIM =  0.7780142113461829\n",
      "Batch =  1450  Train Loss =  0.0030876700523417796  SSIM =  0.7786820994420298\n",
      "Batch =  1475  Train Loss =  0.003075830932041646  SSIM =  0.7790421711236744\n",
      "Batch =  1500  Train Loss =  0.0030764825000951534  SSIM =  0.7792064315229654\n",
      "Batch =  1525  Train Loss =  0.003076838910356393  SSIM =  0.7789865301329582\n",
      "Batch =  1550  Train Loss =  0.0030826378788615036  SSIM =  0.778529454022646\n",
      "Batch =  1575  Train Loss =  0.003086367679532573  SSIM =  0.7783938045397637\n",
      "Batch =  1600  Train Loss =  0.003086743801150078  SSIM =  0.7783177260542289\n",
      "Batch =  1625  Train Loss =  0.0030763032244106466  SSIM =  0.7789209943963931\n",
      "Batch =  1650  Train Loss =  0.0030746986775987105  SSIM =  0.7789967832195036\n",
      "Batch =  1675  Train Loss =  0.0030851423302960734  SSIM =  0.778581815829028\n",
      "Batch =  1700  Train Loss =  0.0030782585628827182  SSIM =  0.7788630359882818\n",
      "Batch =  1725  Train Loss =  0.0030745780818667127  SSIM =  0.7789890756598418\n",
      "Batch =  1750  Train Loss =  0.003070547845156398  SSIM =  0.7789784993486745\n",
      "Batch =  1775  Train Loss =  0.003071135119903951  SSIM =  0.778949850961356\n",
      "Batch =  1800  Train Loss =  0.0030659141543098183  SSIM =  0.7794492882034845\n",
      "Batch =  1825  Train Loss =  0.003063460038265226  SSIM =  0.7795423075393455\n",
      "Batch =  1850  Train Loss =  0.0030624427546265638  SSIM =  0.7795840523089912\n",
      "Batch =  1875  Train Loss =  0.003066172724085239  SSIM =  0.7793191699783008\n",
      "Batch =  1900  Train Loss =  0.0030675484564409617  SSIM =  0.7791610947525816\n",
      "Batch =  1925  Train Loss =  0.003071267005477959  SSIM =  0.7788536297926655\n",
      "Batch =  1950  Train Loss =  0.003070568267206769  SSIM =  0.778698720202232\n",
      "Batch =  1975  Train Loss =  0.0030750653414052757  SSIM =  0.7785297853773153\n",
      "Batch =  2000  Train Loss =  0.0030820495434381884  SSIM =  0.7781226426847279\n",
      "Batch =  2025  Train Loss =  0.0030936595074254674  SSIM =  0.7776830837277718\n",
      "Batch =  2050  Train Loss =  0.003101510454910501  SSIM =  0.7775950042249226\n",
      "Batch =  2075  Train Loss =  0.003101169286844186  SSIM =  0.7777741449101861\n",
      "Batch =  2100  Train Loss =  0.003095617329035165  SSIM =  0.7782601406212364\n",
      "Batch =  2125  Train Loss =  0.003095120320582817  SSIM =  0.7781918887636241\n",
      "Batch =  2150  Train Loss =  0.0030963562876135  SSIM =  0.7779524774572184\n",
      "Batch =  2175  Train Loss =  0.0031051372001469455  SSIM =  0.7777281729209012\n",
      "Batch =  2200  Train Loss =  0.0031110543268600436  SSIM =  0.7773956868763674\n",
      "Loader #:  0  Epoch:  8 /  2500  -  Time:  0:04:13.427875 s - Validation Loss:  0.0  - Train Loss:  0.0031110543268600436  - train SSIM:  0.7773956868763674  validation SSIM:  1710.2705111280084\n",
      "Batch =  1  Train Loss =  0.002635481534525752  SSIM =  0.7735047340393066\n",
      "Batch =  25  Train Loss =  0.0033590284548699854  SSIM =  0.7702318507432938\n",
      "Batch =  50  Train Loss =  0.0033861245505977422  SSIM =  0.7697397750616074\n",
      "Batch =  75  Train Loss =  0.0033013298106379808  SSIM =  0.7734848308563232\n",
      "Batch =  100  Train Loss =  0.0034438707976369187  SSIM =  0.765313525646925\n",
      "Batch =  125  Train Loss =  0.003244622218888253  SSIM =  0.7750556874275207\n",
      "Batch =  150  Train Loss =  0.0033790245194298524  SSIM =  0.7692550185322762\n",
      "Batch =  175  Train Loss =  0.0033188168705341275  SSIM =  0.7693449794394629\n",
      "Batch =  200  Train Loss =  0.0032971987632481616  SSIM =  0.7693124019354581\n",
      "Batch =  225  Train Loss =  0.003321486455711743  SSIM =  0.7671687382459641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch =  250  Train Loss =  0.003271714201953728  SSIM =  0.770489084661007\n",
      "Batch =  275  Train Loss =  0.0032530897291144357  SSIM =  0.7718002804301002\n",
      "Batch =  300  Train Loss =  0.003266960070711017  SSIM =  0.7703640607992808\n",
      "Batch =  325  Train Loss =  0.0032672493650954073  SSIM =  0.7694063438819005\n",
      "Batch =  350  Train Loss =  0.0032992450481729714  SSIM =  0.7671142120446478\n",
      "Batch =  375  Train Loss =  0.0032714507275183376  SSIM =  0.7683396993478139\n",
      "Batch =  400  Train Loss =  0.0032384985545286325  SSIM =  0.7705017585679889\n",
      "Batch =  425  Train Loss =  0.003239193599883412  SSIM =  0.7700715032044579\n",
      "Batch =  450  Train Loss =  0.0032261210468843477  SSIM =  0.770528435740206\n",
      "Batch =  475  Train Loss =  0.003200655746806756  SSIM =  0.7711695428584752\n",
      "Batch =  500  Train Loss =  0.0032040384663851  SSIM =  0.7712117271721363\n",
      "Batch =  525  Train Loss =  0.003199968310571941  SSIM =  0.7718220379522869\n",
      "Batch =  550  Train Loss =  0.003204541736737486  SSIM =  0.7714675360376184\n",
      "Batch =  575  Train Loss =  0.003205750181754727  SSIM =  0.7717648614489514\n",
      "Batch =  600  Train Loss =  0.00320691193206585  SSIM =  0.7720319876323144\n",
      "Batch =  625  Train Loss =  0.0031942891847807913  SSIM =  0.7720638429880142\n",
      "Batch =  650  Train Loss =  0.0031794250013235095  SSIM =  0.7730753821363816\n",
      "Batch =  675  Train Loss =  0.003193237490184536  SSIM =  0.7729916454023785\n",
      "Batch =  700  Train Loss =  0.00318793579836243  SSIM =  0.7726563327014446\n",
      "Batch =  725  Train Loss =  0.0031676074306095214  SSIM =  0.7732655530345851\n",
      "Batch =  750  Train Loss =  0.003140536646436279  SSIM =  0.7748127965132395\n",
      "Batch =  775  Train Loss =  0.003149879604330166  SSIM =  0.7748438603839567\n",
      "Batch =  800  Train Loss =  0.0031700605869627906  SSIM =  0.7745094015263021\n",
      "Batch =  825  Train Loss =  0.0031400344418502893  SSIM =  0.7758465403679645\n",
      "Batch =  850  Train Loss =  0.0031269224598894224  SSIM =  0.7765767604989164\n",
      "Batch =  875  Train Loss =  0.003119043877747442  SSIM =  0.7770269349643162\n",
      "Batch =  900  Train Loss =  0.0031303978051356453  SSIM =  0.7761631317064166\n",
      "Batch =  925  Train Loss =  0.0031347547958894455  SSIM =  0.7766373176792184\n",
      "Batch =  950  Train Loss =  0.003115188993152084  SSIM =  0.7776014616732535\n",
      "Batch =  975  Train Loss =  0.003111693708387275  SSIM =  0.7782528888682524\n",
      "Batch =  1000  Train Loss =  0.0031189545131346677  SSIM =  0.7778951087705791\n",
      "Batch =  1025  Train Loss =  0.0031116917820522426  SSIM =  0.7782821976666043\n",
      "Batch =  1050  Train Loss =  0.003103935149902966  SSIM =  0.7779078583029054\n",
      "Batch =  1075  Train Loss =  0.003119130147225765  SSIM =  0.7773621364769547\n",
      "Batch =  1100  Train Loss =  0.003110212350915059  SSIM =  0.7778748370944099\n",
      "Batch =  1125  Train Loss =  0.0031078677060237775  SSIM =  0.7779310789472527\n",
      "Batch =  1150  Train Loss =  0.0031160898717707427  SSIM =  0.7778432602305775\n",
      "Batch =  1175  Train Loss =  0.0031311900567441704  SSIM =  0.7772989160044396\n",
      "Batch =  1200  Train Loss =  0.0031246551049601595  SSIM =  0.7775512208758543\n",
      "Batch =  1225  Train Loss =  0.003124350668311271  SSIM =  0.7773739955954406\n",
      "Batch =  1250  Train Loss =  0.0031207549448125063  SSIM =  0.7773043609589338\n",
      "Batch =  1275  Train Loss =  0.0031278482461622095  SSIM =  0.7771955891888516\n",
      "Batch =  1300  Train Loss =  0.0031272848872602203  SSIM =  0.7772660320624709\n",
      "Batch =  1325  Train Loss =  0.0031235301465384734  SSIM =  0.7775839611916047\n",
      "Batch =  1350  Train Loss =  0.0031356506942797247  SSIM =  0.7771275712908418\n",
      "Batch =  1375  Train Loss =  0.0031358068577564237  SSIM =  0.7771529399021105\n",
      "Batch =  1400  Train Loss =  0.0031398936418242686  SSIM =  0.7769744682924024\n",
      "Batch =  1425  Train Loss =  0.0031336086715383683  SSIM =  0.7772445247021683\n",
      "Batch =  1450  Train Loss =  0.003133502616968552  SSIM =  0.7772902527734123\n",
      "Batch =  1475  Train Loss =  0.0031384810174507546  SSIM =  0.7772434824477819\n",
      "Batch =  1500  Train Loss =  0.0031341490930644794  SSIM =  0.7773010762706399\n",
      "Batch =  1525  Train Loss =  0.003120284700903614  SSIM =  0.7778335336852269\n",
      "Batch =  1550  Train Loss =  0.003113390509820273  SSIM =  0.7781317256607355\n",
      "Batch =  1575  Train Loss =  0.0031016837045299036  SSIM =  0.7783575487870072\n",
      "Batch =  1600  Train Loss =  0.003100647765204485  SSIM =  0.7781478354963474\n",
      "Batch =  1625  Train Loss =  0.0031020096589524588  SSIM =  0.778023228340424\n",
      "Batch =  1650  Train Loss =  0.0030946069320656753  SSIM =  0.7782337773184884\n",
      "Batch =  1675  Train Loss =  0.003093523448628526  SSIM =  0.7782553911542714\n",
      "Batch =  1700  Train Loss =  0.0030981406938928343  SSIM =  0.7779993126738597\n",
      "Batch =  1725  Train Loss =  0.003098143640284737  SSIM =  0.7779829174604105\n",
      "Batch =  1750  Train Loss =  0.0030962508859645045  SSIM =  0.77827826263862\n",
      "Batch =  1775  Train Loss =  0.0031053038044426013  SSIM =  0.7777441542228343\n",
      "Batch =  1800  Train Loss =  0.003110949512661642  SSIM =  0.7775820653864907\n",
      "Batch =  1825  Train Loss =  0.0031048132859693507  SSIM =  0.7777996987410604\n",
      "Batch =  1850  Train Loss =  0.0031035257665427856  SSIM =  0.7776846583932638\n",
      "Batch =  1875  Train Loss =  0.0030995053275177874  SSIM =  0.7777630736887455\n",
      "Batch =  1900  Train Loss =  0.0030987135388912926  SSIM =  0.777659292901425\n",
      "Batch =  1925  Train Loss =  0.0030981683462567916  SSIM =  0.7777065589888529\n",
      "Batch =  1950  Train Loss =  0.0030882872078412523  SSIM =  0.7781153971014114\n",
      "Batch =  1975  Train Loss =  0.003096215100929471  SSIM =  0.7779239634150946\n",
      "Batch =  2000  Train Loss =  0.0030978562332165892  SSIM =  0.7778892316352576\n",
      "Batch =  2025  Train Loss =  0.003097288502052564  SSIM =  0.7780267189075182\n",
      "Batch =  2050  Train Loss =  0.0030989763722045166  SSIM =  0.7780775684972362\n",
      "Batch =  2075  Train Loss =  0.0030992500574737562  SSIM =  0.7782720164416066\n",
      "Batch =  2100  Train Loss =  0.0030917219856304917  SSIM =  0.7787813211179205\n",
      "Batch =  2125  Train Loss =  0.0030947938817690182  SSIM =  0.7787657768568572\n",
      "Batch =  2150  Train Loss =  0.0030921059340471404  SSIM =  0.7790695070094147\n",
      "Batch =  2175  Train Loss =  0.00309614965123467  SSIM =  0.7787466368164825\n",
      "Batch =  2200  Train Loss =  0.003095589930109616  SSIM =  0.7787066177194092\n",
      "Loader #:  0  Epoch:  9 /  2500  -  Time:  0:04:17.461837 s - Validation Loss:  0.0  - Train Loss:  0.003095589930109616  - train SSIM:  0.7787066177194092  validation SSIM:  1713.1545589827\n",
      "Batch =  1  Train Loss =  0.000885671703144908  SSIM =  0.9010566771030426\n",
      "Batch =  25  Train Loss =  0.0032625575142446904  SSIM =  0.7798599284887314\n",
      "Batch =  50  Train Loss =  0.002783769253292121  SSIM =  0.7968223541975021\n",
      "Batch =  75  Train Loss =  0.002921950145003696  SSIM =  0.78155342400074\n",
      "Batch =  100  Train Loss =  0.002984183743828908  SSIM =  0.7823660065233707\n",
      "Batch =  125  Train Loss =  0.0030032446128316222  SSIM =  0.7799821081161499\n",
      "Batch =  150  Train Loss =  0.002878013802304243  SSIM =  0.7875711778799693\n",
      "Batch =  175  Train Loss =  0.002875329575368336  SSIM =  0.7907025246960776\n",
      "Batch =  200  Train Loss =  0.0029620300568058157  SSIM =  0.7892414119094611\n",
      "Batch =  225  Train Loss =  0.0029850068300341565  SSIM =  0.7864247513479656\n",
      "Batch =  250  Train Loss =  0.0030055573695572093  SSIM =  0.7865117882490158\n",
      "Batch =  275  Train Loss =  0.0030120575956111267  SSIM =  0.7843713076548143\n",
      "Batch =  300  Train Loss =  0.0030654437701256635  SSIM =  0.7812320409715175\n",
      "Batch =  325  Train Loss =  0.0030672240750685047  SSIM =  0.7788289793179586\n",
      "Batch =  350  Train Loss =  0.002998881113515901  SSIM =  0.782544165466513\n",
      "Batch =  375  Train Loss =  0.0029990538163110615  SSIM =  0.7823949236075084\n",
      "Batch =  400  Train Loss =  0.0030204002546088305  SSIM =  0.7822157194465399\n",
      "Batch =  425  Train Loss =  0.002997940676012898  SSIM =  0.7832990038394928\n",
      "Batch =  450  Train Loss =  0.003002026552114532  SSIM =  0.7825681467851003\n",
      "Batch =  475  Train Loss =  0.003002197356269646  SSIM =  0.7825252420337576\n",
      "Batch =  500  Train Loss =  0.0029918627201695924  SSIM =  0.7824460211098194\n",
      "Batch =  525  Train Loss =  0.0029960848674333343  SSIM =  0.7821562614895049\n",
      "Batch =  550  Train Loss =  0.0029971291647102175  SSIM =  0.781269115859812\n",
      "Batch =  575  Train Loss =  0.0029803695407716315  SSIM =  0.7814945706077244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch =  600  Train Loss =  0.0029768216250522527  SSIM =  0.7821094790597757\n",
      "Batch =  625  Train Loss =  0.0029654431622009726  SSIM =  0.7816044093370438\n",
      "Batch =  650  Train Loss =  0.0029756873895754464  SSIM =  0.7807073315519553\n",
      "Batch =  675  Train Loss =  0.0029983419510622128  SSIM =  0.779312359823121\n",
      "Batch =  700  Train Loss =  0.0030117231496842576  SSIM =  0.7792375825132642\n",
      "Batch =  725  Train Loss =  0.002997830272652209  SSIM =  0.7802560262433413\n",
      "Batch =  750  Train Loss =  0.0029809717434303214  SSIM =  0.7803068728248278\n",
      "Batch =  775  Train Loss =  0.0029776357015925308  SSIM =  0.7806800186634064\n",
      "Batch =  800  Train Loss =  0.00297586015331035  SSIM =  0.7809019935876131\n",
      "Batch =  825  Train Loss =  0.0029799843797544863  SSIM =  0.7812115350455949\n",
      "Batch =  850  Train Loss =  0.002980474454063155  SSIM =  0.7816314440790345\n",
      "Batch =  875  Train Loss =  0.0029818507891281377  SSIM =  0.7815921046904155\n",
      "Batch =  900  Train Loss =  0.0029804066059619396  SSIM =  0.7815415028731029\n",
      "Batch =  925  Train Loss =  0.0029949219573314327  SSIM =  0.7806217970074834\n",
      "Batch =  950  Train Loss =  0.0029939435191427993  SSIM =  0.7806240545448504\n",
      "Batch =  975  Train Loss =  0.003003365850965612  SSIM =  0.7801454367698767\n",
      "Batch =  1000  Train Loss =  0.0030070448398182635  SSIM =  0.780031356498599\n",
      "Batch =  1025  Train Loss =  0.002995061943713553  SSIM =  0.7807746182563828\n",
      "Batch =  1050  Train Loss =  0.0029972925621301067  SSIM =  0.7805264429109438\n",
      "Batch =  1075  Train Loss =  0.0030001774561715985  SSIM =  0.7800616318403288\n",
      "Batch =  1100  Train Loss =  0.002994203548600622  SSIM =  0.7801878239620815\n",
      "Batch =  1125  Train Loss =  0.0030018314033513887  SSIM =  0.780165410346455\n",
      "Batch =  1150  Train Loss =  0.0030021023245543526  SSIM =  0.7804344869178275\n",
      "Batch =  1175  Train Loss =  0.0030322498064166174  SSIM =  0.7794142371162455\n",
      "Batch =  1200  Train Loss =  0.003039033494605974  SSIM =  0.7792608336980145\n",
      "Batch =  1225  Train Loss =  0.003037093404991723  SSIM =  0.7794939732186649\n",
      "Batch =  1250  Train Loss =  0.0030502094081486576  SSIM =  0.7791121774435044\n",
      "Batch =  1275  Train Loss =  0.0030469748647038953  SSIM =  0.7795277438210506\n",
      "Batch =  1300  Train Loss =  0.0030433865279617467  SSIM =  0.7797070008860184\n",
      "Batch =  1325  Train Loss =  0.0030420921593003925  SSIM =  0.7802815557088492\n",
      "Batch =  1350  Train Loss =  0.0030421748042378264  SSIM =  0.7807742029538861\n",
      "Batch =  1375  Train Loss =  0.0030352961878101765  SSIM =  0.7809372570189562\n",
      "Batch =  1400  Train Loss =  0.0030333555179926667  SSIM =  0.7811948388282742\n",
      "Batch =  1425  Train Loss =  0.003028388739504296  SSIM =  0.781523530681928\n",
      "Batch =  1450  Train Loss =  0.003023980240255257  SSIM =  0.7818088241795014\n",
      "Batch =  1475  Train Loss =  0.0030244607719004755  SSIM =  0.7818518992399766\n",
      "Batch =  1500  Train Loss =  0.003034126577927964  SSIM =  0.7816731108923753\n",
      "Batch =  1525  Train Loss =  0.0030484990654875846  SSIM =  0.7814870321555216\n",
      "Batch =  1550  Train Loss =  0.003039300082835384  SSIM =  0.7820152130915272\n",
      "Batch =  1575  Train Loss =  0.0030412966940673953  SSIM =  0.7819149264456734\n",
      "Batch =  1600  Train Loss =  0.0030558866600495093  SSIM =  0.7810839153081178\n",
      "Batch =  1625  Train Loss =  0.003061151575538903  SSIM =  0.7811887706701572\n",
      "Batch =  1650  Train Loss =  0.003064679183078238  SSIM =  0.7807490996971275\n",
      "Batch =  1675  Train Loss =  0.003074842778550558  SSIM =  0.7803632528657344\n",
      "Batch =  1700  Train Loss =  0.0030843500648573834  SSIM =  0.7797844900716754\n",
      "Batch =  1725  Train Loss =  0.0030816452702814444  SSIM =  0.7797901952525844\n",
      "Batch =  1750  Train Loss =  0.003070214461926038  SSIM =  0.7804712900178773\n",
      "Batch =  1775  Train Loss =  0.0030861358545166374  SSIM =  0.7801215845430401\n",
      "Batch =  1800  Train Loss =  0.003076976985151608  SSIM =  0.7803775328563319\n",
      "Batch =  1825  Train Loss =  0.0030671869372001733  SSIM =  0.7809002657785807\n",
      "Batch =  1850  Train Loss =  0.003067695874511579  SSIM =  0.7807412231210116\n",
      "Batch =  1875  Train Loss =  0.003068014002148993  SSIM =  0.7807026473283768\n",
      "Batch =  1900  Train Loss =  0.0030622639417829064  SSIM =  0.7811417700977702\n",
      "Batch =  1925  Train Loss =  0.0030663952544896997  SSIM =  0.7808604300254351\n",
      "Batch =  1950  Train Loss =  0.003064354196816193  SSIM =  0.7809555476292586\n",
      "Batch =  1975  Train Loss =  0.003076319463891449  SSIM =  0.7804917178501056\n",
      "Batch =  2000  Train Loss =  0.0030736631192339703  SSIM =  0.780598465681076\n",
      "Batch =  2025  Train Loss =  0.003074309114477436  SSIM =  0.7807247447452428\n",
      "Batch =  2050  Train Loss =  0.0030779212959204435  SSIM =  0.7806240390568244\n",
      "Batch =  2075  Train Loss =  0.0030704171259828606  SSIM =  0.7810794532586293\n",
      "Batch =  2100  Train Loss =  0.003082425385191094  SSIM =  0.7806330233333366\n",
      "Batch =  2125  Train Loss =  0.0030827230070580676  SSIM =  0.780650528685135\n",
      "Batch =  2150  Train Loss =  0.0030843310272402327  SSIM =  0.780712012989923\n",
      "Batch =  2175  Train Loss =  0.00308294445596068  SSIM =  0.7807154816902917\n",
      "Batch =  2200  Train Loss =  0.0030821233084928033  SSIM =  0.7803759369762107\n",
      "Loader #:  0  Epoch:  10 /  2500  -  Time:  0:04:28.318787 s - Validation Loss:  0.0  - Train Loss:  0.0030821233084928033  - train SSIM:  0.7803759369762107  validation SSIM:  1716.8270613476634\n",
      "Batch =  1  Train Loss =  0.000807778094895184  SSIM =  0.9421924352645874\n",
      "Batch =  25  Train Loss =  0.002949095239164308  SSIM =  0.8015647453069686\n",
      "Batch =  50  Train Loss =  0.002592200177605264  SSIM =  0.811182681620121\n",
      "Batch =  75  Train Loss =  0.0028436732365904995  SSIM =  0.8023517505327861\n",
      "Batch =  100  Train Loss =  0.003027141990023665  SSIM =  0.7942419224977493\n",
      "Batch =  125  Train Loss =  0.003078720247372985  SSIM =  0.7901403708457947\n",
      "Batch =  150  Train Loss =  0.003093644928885624  SSIM =  0.7902528025706609\n",
      "Batch =  175  Train Loss =  0.003093393875751644  SSIM =  0.789002222759383\n",
      "Batch =  200  Train Loss =  0.003069610758393537  SSIM =  0.7901650592684746\n",
      "Batch =  225  Train Loss =  0.003116969563998282  SSIM =  0.7899175165096919\n",
      "Batch =  250  Train Loss =  0.0031203221057076007  SSIM =  0.7914090579748154\n",
      "Batch =  275  Train Loss =  0.0031279373680114407  SSIM =  0.7907379754564979\n",
      "Batch =  300  Train Loss =  0.003117296825706338  SSIM =  0.791154190103213\n",
      "Batch =  325  Train Loss =  0.0031046966588697752  SSIM =  0.7911183732289534\n",
      "Batch =  350  Train Loss =  0.0030761675044362036  SSIM =  0.7922539195418358\n",
      "Batch =  375  Train Loss =  0.0030856486363336445  SSIM =  0.7920741561253866\n",
      "Batch =  400  Train Loss =  0.0030656150630966293  SSIM =  0.7926013331115246\n",
      "Batch =  425  Train Loss =  0.0031109154842081754  SSIM =  0.789207988521632\n",
      "Batch =  450  Train Loss =  0.0031198265869170428  SSIM =  0.7877197709348467\n",
      "Batch =  475  Train Loss =  0.0030995834247503234  SSIM =  0.7890818610316829\n",
      "Batch =  500  Train Loss =  0.003138341648504138  SSIM =  0.7868151304423809\n",
      "Batch =  525  Train Loss =  0.0031505527539134382  SSIM =  0.7860578767458598\n",
      "Batch =  550  Train Loss =  0.003182488878532736  SSIM =  0.7842475426738913\n",
      "Batch =  575  Train Loss =  0.0031920954212546347  SSIM =  0.7829331831050955\n",
      "Batch =  600  Train Loss =  0.003210604746321527  SSIM =  0.7816094775001208\n",
      "Batch =  625  Train Loss =  0.0031873712804634125  SSIM =  0.7822246461868286\n",
      "Batch =  650  Train Loss =  0.0031857416591195105  SSIM =  0.7820724100103745\n",
      "Batch =  675  Train Loss =  0.003168713825677211  SSIM =  0.7822624612737585\n",
      "Batch =  700  Train Loss =  0.0031779237180515857  SSIM =  0.7808338390929358\n",
      "Batch =  725  Train Loss =  0.003169309344415649  SSIM =  0.7810736910433604\n",
      "Batch =  750  Train Loss =  0.003170715806540102  SSIM =  0.7810360601743063\n",
      "Batch =  775  Train Loss =  0.0031854174380010415  SSIM =  0.7800725447554742\n",
      "Batch =  800  Train Loss =  0.003187163306138245  SSIM =  0.7797680191323161\n",
      "Batch =  825  Train Loss =  0.003149931109792581  SSIM =  0.7813267114306941\n",
      "Batch =  850  Train Loss =  0.003152956790975569  SSIM =  0.7810525859629407\n",
      "Batch =  875  Train Loss =  0.0031327194752437727  SSIM =  0.7816436296190534\n",
      "Batch =  900  Train Loss =  0.0031247677837058694  SSIM =  0.7820197506248951\n",
      "Batch =  925  Train Loss =  0.0031263881211005453  SSIM =  0.781999372453303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch =  950  Train Loss =  0.0031190612775236857  SSIM =  0.7823336828225538\n",
      "Batch =  975  Train Loss =  0.0031258048124623316  SSIM =  0.7818213746792231\n",
      "Batch =  1000  Train Loss =  0.0031232828889915256  SSIM =  0.7819427665472031\n",
      "Batch =  1025  Train Loss =  0.003114493347437507  SSIM =  0.7824488812685013\n",
      "Batch =  1050  Train Loss =  0.0031017793143733536  SSIM =  0.783002932710307\n",
      "Batch =  1075  Train Loss =  0.003135016529311881  SSIM =  0.7816911805923595\n",
      "Batch =  1100  Train Loss =  0.0031223856240235777  SSIM =  0.7820280502736568\n",
      "Batch =  1125  Train Loss =  0.0031072359836349884  SSIM =  0.7824186215930515\n",
      "Batch =  1150  Train Loss =  0.0031093924326579207  SSIM =  0.7822683750028195\n",
      "Batch =  1175  Train Loss =  0.003115233487469402  SSIM =  0.781755488451491\n",
      "Batch =  1200  Train Loss =  0.0031036904126085572  SSIM =  0.7824176789199313\n",
      "Batch =  1225  Train Loss =  0.003107174885262051  SSIM =  0.7819002103318974\n",
      "Batch =  1250  Train Loss =  0.003106102610810194  SSIM =  0.7819631242156029\n",
      "Batch =  1275  Train Loss =  0.0031135311822393253  SSIM =  0.7812396478652954\n",
      "Batch =  1300  Train Loss =  0.0031046168921770563  SSIM =  0.7813400953434981\n",
      "Batch =  1325  Train Loss =  0.003100693648656883  SSIM =  0.7813855266795968\n",
      "Batch =  1350  Train Loss =  0.0030916856163444376  SSIM =  0.7816087367689168\n",
      "Batch =  1375  Train Loss =  0.003082590556904589  SSIM =  0.7820239691951059\n",
      "Batch =  1400  Train Loss =  0.003073072226449897  SSIM =  0.7822250978329351\n",
      "Batch =  1425  Train Loss =  0.003080502688065743  SSIM =  0.7819804967809142\n",
      "Batch =  1450  Train Loss =  0.0030740140444611135  SSIM =  0.7821309488908997\n",
      "Batch =  1475  Train Loss =  0.003069670260400448  SSIM =  0.7819891842037945\n",
      "Batch =  1500  Train Loss =  0.003057470647259227  SSIM =  0.7827283197740714\n",
      "Batch =  1525  Train Loss =  0.003052079188551555  SSIM =  0.7828205093594848\n",
      "Batch =  1550  Train Loss =  0.003054050629566078  SSIM =  0.7823713960474538\n",
      "Batch =  1575  Train Loss =  0.003047212272178426  SSIM =  0.7823612945515012\n",
      "Batch =  1600  Train Loss =  0.003050772373562722  SSIM =  0.7822604328393936\n",
      "Batch =  1625  Train Loss =  0.0030555427830505112  SSIM =  0.7817833128250562\n",
      "Batch =  1650  Train Loss =  0.0030614061205856023  SSIM =  0.7814619132244226\n",
      "Batch =  1675  Train Loss =  0.0030724429817255167  SSIM =  0.7811147715440437\n",
      "Batch =  1700  Train Loss =  0.0030836994938727  SSIM =  0.7803849841478993\n",
      "Batch =  1725  Train Loss =  0.003085258570423795  SSIM =  0.7802905023443526\n",
      "Batch =  1750  Train Loss =  0.0030850179324375597  SSIM =  0.7801454889689173\n",
      "Batch =  1775  Train Loss =  0.003078891554360383  SSIM =  0.7802663834497963\n",
      "Batch =  1800  Train Loss =  0.003066230418990017  SSIM =  0.7808466759820779\n",
      "Batch =  1825  Train Loss =  0.003061281275681909  SSIM =  0.7810652363953525\n",
      "Batch =  1850  Train Loss =  0.003068629613962666  SSIM =  0.7809844207038751\n",
      "Batch =  1875  Train Loss =  0.0030735781556461006  SSIM =  0.7808089825550715\n",
      "Batch =  1900  Train Loss =  0.0030781147915312384  SSIM =  0.7807589594784536\n",
      "Batch =  1925  Train Loss =  0.003080616563349938  SSIM =  0.7805921328377414\n",
      "Batch =  1950  Train Loss =  0.003084536354752998  SSIM =  0.7804243512738209\n",
      "Batch =  1975  Train Loss =  0.003092646691786078  SSIM =  0.779854152581737\n",
      "Batch =  2000  Train Loss =  0.0030829489473981086  SSIM =  0.7805010691601783\n",
      "Batch =  2025  Train Loss =  0.0030783877590021555  SSIM =  0.7805868510129275\n",
      "Batch =  2050  Train Loss =  0.0030787588443135752  SSIM =  0.7806011394047883\n",
      "Batch =  2075  Train Loss =  0.00307242207721448  SSIM =  0.781095022783581\n",
      "Batch =  2100  Train Loss =  0.003076306660756624  SSIM =  0.7809524385294034\n",
      "Batch =  2125  Train Loss =  0.0030789205812273876  SSIM =  0.7808739805309212\n",
      "Batch =  2150  Train Loss =  0.0030740981768995392  SSIM =  0.7807515299614779\n",
      "Batch =  2175  Train Loss =  0.003073609740963077  SSIM =  0.7808331441005756\n",
      "Batch =  2200  Train Loss =  0.00307782390546477  SSIM =  0.7804963082972576\n",
      "Loader #:  0  Epoch:  11 /  2500  -  Time:  0:04:23.654443 s - Validation Loss:  0.0  - Train Loss:  0.00307782390546477  - train SSIM:  0.7804963082972576  validation SSIM:  1717.0918782539666\n",
      "Batch =  1  Train Loss =  0.0026675122790038586  SSIM =  0.8150845766067505\n",
      "Batch =  25  Train Loss =  0.0028425008337944744  SSIM =  0.790253438949585\n",
      "Batch =  50  Train Loss =  0.0027719041891396045  SSIM =  0.7935280001163483\n",
      "Batch =  75  Train Loss =  0.002864019994158298  SSIM =  0.7904027525583903\n",
      "Batch =  100  Train Loss =  0.003110352637595497  SSIM =  0.7810298067331314\n",
      "Batch =  125  Train Loss =  0.003090734848752618  SSIM =  0.7832662566900254\n",
      "Batch =  150  Train Loss =  0.0031555828677179913  SSIM =  0.7801671175161997\n",
      "Batch =  175  Train Loss =  0.0032090622216596133  SSIM =  0.7782788539784296\n",
      "Batch =  200  Train Loss =  0.0031695201099500993  SSIM =  0.7812390860915184\n",
      "Batch =  225  Train Loss =  0.0031604256310189763  SSIM =  0.7816639845238792\n",
      "Batch =  250  Train Loss =  0.003189226729096845  SSIM =  0.7782625095844269\n",
      "Batch =  275  Train Loss =  0.0031280971425373784  SSIM =  0.7807231524857607\n",
      "Batch =  300  Train Loss =  0.0030935557525178107  SSIM =  0.7821752405166627\n",
      "Batch =  325  Train Loss =  0.0030858253246137442  SSIM =  0.780402709704179\n",
      "Batch =  350  Train Loss =  0.0031048196623201614  SSIM =  0.7789603282298361\n",
      "Batch =  375  Train Loss =  0.003068075398526465  SSIM =  0.7801577107906341\n",
      "Batch =  400  Train Loss =  0.003102192484002444  SSIM =  0.7783201311528682\n",
      "Batch =  425  Train Loss =  0.003197849773842951  SSIM =  0.7754624077853034\n",
      "Batch =  450  Train Loss =  0.0031485965913290986  SSIM =  0.7767432425088353\n",
      "Batch =  475  Train Loss =  0.0031317560056739143  SSIM =  0.7773361858254985\n",
      "Batch =  500  Train Loss =  0.0031445864101988263  SSIM =  0.7774356592595577\n",
      "Batch =  525  Train Loss =  0.0031696128975489133  SSIM =  0.7777574242296673\n",
      "Batch =  550  Train Loss =  0.0031772904992844403  SSIM =  0.7771443284641613\n",
      "Batch =  575  Train Loss =  0.0031511629244778305  SSIM =  0.7774806074215018\n",
      "Batch =  600  Train Loss =  0.0031491962456008573  SSIM =  0.7778484729677438\n",
      "Batch =  625  Train Loss =  0.0031517719693947583  SSIM =  0.7776614879608155\n",
      "Batch =  650  Train Loss =  0.0031694981773258547  SSIM =  0.7773203772994188\n",
      "Batch =  675  Train Loss =  0.003167322604287485  SSIM =  0.777636517992726\n",
      "Batch =  700  Train Loss =  0.0031691752726328557  SSIM =  0.7777433061386858\n",
      "Batch =  725  Train Loss =  0.0032106108670042635  SSIM =  0.7760729015695638\n",
      "Batch =  750  Train Loss =  0.0031954448849816496  SSIM =  0.776520689090093\n",
      "Batch =  775  Train Loss =  0.0032022140208526604  SSIM =  0.7762891385055357\n",
      "Batch =  800  Train Loss =  0.0031736468573581077  SSIM =  0.7774008919484914\n",
      "Batch =  825  Train Loss =  0.0031623334497933022  SSIM =  0.778034281965458\n",
      "Batch =  850  Train Loss =  0.003168190772172666  SSIM =  0.7776517292988651\n",
      "Batch =  875  Train Loss =  0.003150279171166143  SSIM =  0.7778590718635491\n",
      "Batch =  900  Train Loss =  0.0031539027215007486  SSIM =  0.7778936618525121\n",
      "Batch =  925  Train Loss =  0.0031506191485410405  SSIM =  0.7778392985986696\n",
      "Batch =  950  Train Loss =  0.003144403414090017  SSIM =  0.7777106306701899\n",
      "Batch =  975  Train Loss =  0.003140215161554993  SSIM =  0.7778704409530529\n",
      "Batch =  1000  Train Loss =  0.0031291558275115677  SSIM =  0.778061440449208\n",
      "Batch =  1025  Train Loss =  0.0031208606218782868  SSIM =  0.7786877354579728\n",
      "Batch =  1050  Train Loss =  0.0031180051091637107  SSIM =  0.7785368267233883\n",
      "Batch =  1075  Train Loss =  0.0031321023724605003  SSIM =  0.7778453264090904\n",
      "Batch =  1100  Train Loss =  0.0031277316091158852  SSIM =  0.7779922406070612\n",
      "Batch =  1125  Train Loss =  0.003142583644741939  SSIM =  0.7773327666355504\n",
      "Batch =  1150  Train Loss =  0.0031157551628269217  SSIM =  0.7785688744647348\n",
      "Batch =  1175  Train Loss =  0.003111127412499008  SSIM =  0.7785639947621112\n",
      "Batch =  1200  Train Loss =  0.003103665857670421  SSIM =  0.7791134958559026\n",
      "Batch =  1225  Train Loss =  0.0031040209311426485  SSIM =  0.7790309122843402\n",
      "Batch =  1250  Train Loss =  0.003096877971489448  SSIM =  0.7792376411288977\n",
      "Batch =  1275  Train Loss =  0.003102904190914249  SSIM =  0.7789098595374939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch =  1300  Train Loss =  0.003091409479216404  SSIM =  0.7791423220789203\n",
      "Batch =  1325  Train Loss =  0.003097285915150764  SSIM =  0.778842470508139\n",
      "Batch =  1350  Train Loss =  0.003087262179437352  SSIM =  0.7792806511427517\n",
      "Batch =  1375  Train Loss =  0.003077221319936639  SSIM =  0.7797594768215309\n",
      "Batch =  1400  Train Loss =  0.0030720037049468372  SSIM =  0.7795191246244524\n",
      "Batch =  1425  Train Loss =  0.0030634617335326447  SSIM =  0.7796998240419647\n",
      "Batch =  1450  Train Loss =  0.0030753144965073543  SSIM =  0.7792451221341716\n",
      "Batch =  1475  Train Loss =  0.0030695437636005423  SSIM =  0.7796664479400142\n",
      "Batch =  1500  Train Loss =  0.003088959076364214  SSIM =  0.7787642786478003\n",
      "Batch =  1525  Train Loss =  0.0030927912834421045  SSIM =  0.7788211634095575\n",
      "Batch =  1550  Train Loss =  0.003084393329355085  SSIM =  0.7794797086883937\n",
      "Batch =  1575  Train Loss =  0.003089188546448621  SSIM =  0.7791244068292398\n",
      "Batch =  1600  Train Loss =  0.003091754588349431  SSIM =  0.7788950458611361\n",
      "Batch =  1625  Train Loss =  0.0030863420447836127  SSIM =  0.7793104881300376\n",
      "Batch =  1650  Train Loss =  0.0030856816879134964  SSIM =  0.779457345981941\n",
      "Batch =  1675  Train Loss =  0.003086646624958949  SSIM =  0.779564517040306\n",
      "Batch =  1700  Train Loss =  0.0030902467056876048  SSIM =  0.7796974139594857\n",
      "Batch =  1725  Train Loss =  0.003086050500371156  SSIM =  0.7800174298040244\n",
      "Batch =  1750  Train Loss =  0.0030956561214490128  SSIM =  0.7797380064619439\n",
      "Batch =  1775  Train Loss =  0.0030945081765842166  SSIM =  0.7799369877891641\n",
      "Batch =  1800  Train Loss =  0.003105087633351407  SSIM =  0.7796313502618836\n",
      "Batch =  1825  Train Loss =  0.003105459753216614  SSIM =  0.7798579589124411\n",
      "Batch =  1850  Train Loss =  0.0031081909815799695  SSIM =  0.7797738092795418\n",
      "Batch =  1875  Train Loss =  0.0031062974504195155  SSIM =  0.7799476054410139\n",
      "Batch =  1900  Train Loss =  0.0031029387886519545  SSIM =  0.7800117660882442\n",
      "Batch =  1925  Train Loss =  0.0031026107908752685  SSIM =  0.7799593974998245\n",
      "Batch =  1950  Train Loss =  0.0031055381204425476  SSIM =  0.7796763940098194\n",
      "Batch =  1975  Train Loss =  0.0030966712827529006  SSIM =  0.7800041993415053\n",
      "Batch =  2000  Train Loss =  0.003103129595445353  SSIM =  0.7796750840824097\n",
      "Batch =  2025  Train Loss =  0.003097202437800459  SSIM =  0.7799705152139987\n",
      "Batch =  2050  Train Loss =  0.0030930379196779986  SSIM =  0.7801933417650985\n",
      "Batch =  2075  Train Loss =  0.0030914244895761675  SSIM =  0.7801013824433447\n",
      "Batch =  2100  Train Loss =  0.0030907212291051457  SSIM =  0.7801947285217189\n",
      "Batch =  2125  Train Loss =  0.0030876384021728026  SSIM =  0.7805261812893783\n",
      "Batch =  2150  Train Loss =  0.00308816716828259  SSIM =  0.7803485579112934\n",
      "Batch =  2175  Train Loss =  0.0030832052134825774  SSIM =  0.7805114053498055\n",
      "Batch =  2200  Train Loss =  0.0030868321968922498  SSIM =  0.7798908430591903\n",
      "Loader #:  0  Epoch:  12 /  2500  -  Time:  0:04:10.621235 s - Validation Loss:  0.0  - Train Loss:  0.0030868321968922498  - train SSIM:  0.7798908430591903  validation SSIM:  1715.7598547302186\n",
      "Batch =  1  Train Loss =  0.0034949686378240585  SSIM =  0.8499046862125397\n",
      "Batch =  25  Train Loss =  0.00318208540789783  SSIM =  0.7780803579092026\n",
      "Batch =  50  Train Loss =  0.003221425540978089  SSIM =  0.7837215915322304\n",
      "Batch =  75  Train Loss =  0.0030868418433237822  SSIM =  0.7903854491313299\n",
      "Batch =  100  Train Loss =  0.0030967435377533548  SSIM =  0.785307701677084\n",
      "Batch =  125  Train Loss =  0.0030006538743618874  SSIM =  0.7872164175510407\n",
      "Batch =  150  Train Loss =  0.00297892108928257  SSIM =  0.7866119746367136\n",
      "Batch =  175  Train Loss =  0.0029818765780821976  SSIM =  0.7860794382435935\n",
      "Batch =  200  Train Loss =  0.0030024331396271008  SSIM =  0.7857489085197449\n",
      "Batch =  225  Train Loss =  0.003056055447216042  SSIM =  0.7852571687267886\n",
      "Batch =  250  Train Loss =  0.0030643573564011605  SSIM =  0.7856588967889547\n",
      "Batch =  275  Train Loss =  0.003029343751259148  SSIM =  0.785869808291847\n",
      "Batch =  300  Train Loss =  0.0029308936369488946  SSIM =  0.7908246419206262\n",
      "Batch =  325  Train Loss =  0.0029550946705365695  SSIM =  0.7910986108390184\n",
      "Batch =  350  Train Loss =  0.0029441277550566677  SSIM =  0.7900824344903231\n",
      "Batch =  375  Train Loss =  0.002992042525399787  SSIM =  0.7878820230464141\n",
      "Batch =  400  Train Loss =  0.003048682637272577  SSIM =  0.7858558223303408\n",
      "Batch =  425  Train Loss =  0.0030905201315866125  SSIM =  0.7844021625290899\n",
      "Batch =  450  Train Loss =  0.0030682816006851175  SSIM =  0.7857079375038545\n",
      "Batch =  475  Train Loss =  0.0030892896492610146  SSIM =  0.7852991625431337\n",
      "Batch =  500  Train Loss =  0.0030608124751306605  SSIM =  0.7861087380573154\n",
      "Batch =  525  Train Loss =  0.0030375819699561576  SSIM =  0.7866013221442699\n",
      "Batch =  550  Train Loss =  0.00304680629768303  SSIM =  0.7857202818922021\n",
      "Batch =  575  Train Loss =  0.0030591756928175606  SSIM =  0.7846441129433072\n",
      "Batch =  600  Train Loss =  0.0030356517868009784  SSIM =  0.785861048741887\n",
      "Batch =  625  Train Loss =  0.003033696951321326  SSIM =  0.7860609089314937\n",
      "Batch =  650  Train Loss =  0.003060873914845825  SSIM =  0.783854696676135\n",
      "Batch =  675  Train Loss =  0.0030476493831439358  SSIM =  0.783628347929981\n",
      "Batch =  700  Train Loss =  0.0030583209125948736  SSIM =  0.7827576484797256\n",
      "Batch =  725  Train Loss =  0.0030570882499209957  SSIM =  0.7820610916563149\n",
      "Batch =  750  Train Loss =  0.003032996640560062  SSIM =  0.783133297736446\n",
      "Batch =  775  Train Loss =  0.003035015414728062  SSIM =  0.7836336846265101\n",
      "Batch =  800  Train Loss =  0.0030360669316542043  SSIM =  0.7840556553890928\n",
      "Batch =  825  Train Loss =  0.0030535306672699694  SSIM =  0.7828286093789519\n",
      "Batch =  850  Train Loss =  0.003046433686206857  SSIM =  0.7832036504631533\n",
      "Batch =  875  Train Loss =  0.003048382420741421  SSIM =  0.7834738475893225\n",
      "Batch =  900  Train Loss =  0.0030292911015082306  SSIM =  0.7837869865157538\n",
      "Batch =  925  Train Loss =  0.0030293573017980955  SSIM =  0.784382855179342\n",
      "Batch =  950  Train Loss =  0.00303580569732922  SSIM =  0.7844406218238567\n",
      "Batch =  975  Train Loss =  0.0030373893241481616  SSIM =  0.7842977421520613\n",
      "Batch =  1000  Train Loss =  0.003030148594494676  SSIM =  0.7846641643755138\n",
      "Batch =  1025  Train Loss =  0.0030242644577207632  SSIM =  0.7844962993155166\n",
      "Batch =  1050  Train Loss =  0.0030168808559011225  SSIM =  0.784936423496831\n",
      "Batch =  1075  Train Loss =  0.0030195312996037565  SSIM =  0.7846399866668291\n",
      "Batch =  1100  Train Loss =  0.003040576202196958  SSIM =  0.7834420730410652\n",
      "Batch =  1125  Train Loss =  0.0030630159120644544  SSIM =  0.7820452822811074\n",
      "Batch =  1150  Train Loss =  0.0030805983804495316  SSIM =  0.7818832261115313\n",
      "Batch =  1175  Train Loss =  0.003079202133045096  SSIM =  0.7819072847765811\n",
      "Batch =  1200  Train Loss =  0.0030905978299294173  SSIM =  0.7807706306967884\n",
      "Batch =  1225  Train Loss =  0.0031019138962111188  SSIM =  0.7802547796617966\n",
      "Batch =  1250  Train Loss =  0.0031086532006156633  SSIM =  0.7795870570629835\n",
      "Batch =  1275  Train Loss =  0.003112855318388171  SSIM =  0.7791353496120257\n",
      "Batch =  1300  Train Loss =  0.0030981386430189908  SSIM =  0.7796826795631876\n",
      "Batch =  1325  Train Loss =  0.0031036103466846364  SSIM =  0.7792363483697738\n",
      "Batch =  1350  Train Loss =  0.003103354475169908  SSIM =  0.7790321098737143\n",
      "Batch =  1375  Train Loss =  0.003106260920419696  SSIM =  0.7789168148555539\n",
      "Batch =  1400  Train Loss =  0.0030965530279146542  SSIM =  0.7789261521265975\n",
      "Batch =  1425  Train Loss =  0.0031023955275406735  SSIM =  0.7785501189895889\n",
      "Batch =  1450  Train Loss =  0.003102091385065658  SSIM =  0.7785433209327788\n",
      "Batch =  1475  Train Loss =  0.0030907104145913083  SSIM =  0.7791068721398459\n",
      "Batch =  1500  Train Loss =  0.00309197387577539  SSIM =  0.7790224869822462\n",
      "Batch =  1525  Train Loss =  0.003096744260630479  SSIM =  0.7788172901922562\n",
      "Batch =  1550  Train Loss =  0.003092036608065809  SSIM =  0.7790443391256755\n",
      "Batch =  1575  Train Loss =  0.0030977308600845317  SSIM =  0.779083288129833\n",
      "Batch =  1600  Train Loss =  0.0030949275533203036  SSIM =  0.7793127847253345\n",
      "Batch =  1625  Train Loss =  0.0030853582872436025  SSIM =  0.7796587090744422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch =  1650  Train Loss =  0.003087055037948162  SSIM =  0.7793354682917848\n",
      "Batch =  1675  Train Loss =  0.0030866580706527596  SSIM =  0.7793829981493416\n",
      "Batch =  1700  Train Loss =  0.003092438823557185  SSIM =  0.7792763994524584\n",
      "Batch =  1725  Train Loss =  0.0030906257731249504  SSIM =  0.7792745630702247\n",
      "Batch =  1750  Train Loss =  0.003091289583967799  SSIM =  0.7791504060519593\n",
      "Batch =  1775  Train Loss =  0.0030920869200207575  SSIM =  0.7790041027585386\n",
      "Batch =  1800  Train Loss =  0.0030868350140756066  SSIM =  0.7791766882501543\n",
      "Batch =  1825  Train Loss =  0.0030881437788097895  SSIM =  0.7790474486575552\n",
      "Batch =  1850  Train Loss =  0.003081908559556846  SSIM =  0.7796113112951453\n",
      "Batch =  1875  Train Loss =  0.0030839756248965083  SSIM =  0.7795871953388055\n",
      "Batch =  1900  Train Loss =  0.00308560115859444  SSIM =  0.7792311363216293\n",
      "Batch =  1925  Train Loss =  0.0030850722308572256  SSIM =  0.779283864116901\n",
      "Batch =  1950  Train Loss =  0.0030938080772709023  SSIM =  0.7790651817256824\n",
      "Batch =  1975  Train Loss =  0.003101462749453023  SSIM =  0.7787603012670444\n",
      "Batch =  2000  Train Loss =  0.0030961005289263993  SSIM =  0.7789830352813005\n",
      "Batch =  2025  Train Loss =  0.00309328321433435  SSIM =  0.7792134828626374\n",
      "Batch =  2050  Train Loss =  0.0030972928097510637  SSIM =  0.7792338835611576\n",
      "Batch =  2075  Train Loss =  0.0031043489456596836  SSIM =  0.7787743339265686\n",
      "Batch =  2100  Train Loss =  0.00310386658352183  SSIM =  0.7788945294703756\n",
      "Batch =  2125  Train Loss =  0.003097546383038314  SSIM =  0.7793018347656026\n",
      "Batch =  2150  Train Loss =  0.0030973344658847307  SSIM =  0.7790409784469493\n",
      "Batch =  2175  Train Loss =  0.003090085654381339  SSIM =  0.7794308014916278\n",
      "Batch =  2200  Train Loss =  0.0030809654934786193  SSIM =  0.7798120201243596\n",
      "Loader #:  0  Epoch:  13 /  2500  -  Time:  0:04:10.591975 s - Validation Loss:  0.0  - Train Loss:  0.0030809654934786193  - train SSIM:  0.7798120201243596  validation SSIM:  1715.586444273591\n",
      "Batch =  1  Train Loss =  0.001230986206792295  SSIM =  0.8105310797691345\n",
      "Batch =  25  Train Loss =  0.002545716951135546  SSIM =  0.8028019386529922\n",
      "Batch =  50  Train Loss =  0.002488436498097144  SSIM =  0.8062749552726746\n",
      "Batch =  75  Train Loss =  0.0026161093028107037  SSIM =  0.8068553086121877\n",
      "Batch =  100  Train Loss =  0.002781294327287469  SSIM =  0.7989797939360141\n",
      "Batch =  125  Train Loss =  0.002804616199573502  SSIM =  0.7933895666599273\n",
      "Batch =  150  Train Loss =  0.0029848423179161423  SSIM =  0.7824710846940677\n",
      "Batch =  175  Train Loss =  0.00295050302403979  SSIM =  0.7840340579407555\n",
      "Batch =  200  Train Loss =  0.002970938646612922  SSIM =  0.780676232278347\n",
      "Batch =  225  Train Loss =  0.0029340056031166265  SSIM =  0.7829767889446683\n",
      "Batch =  250  Train Loss =  0.0029667115529300645  SSIM =  0.7821975176930428\n",
      "Batch =  275  Train Loss =  0.0029520238750741223  SSIM =  0.7837197324904529\n",
      "Batch =  300  Train Loss =  0.002986164492419145  SSIM =  0.7835673459370931\n",
      "Batch =  325  Train Loss =  0.002972757713063262  SSIM =  0.7851034169013683\n",
      "Batch =  350  Train Loss =  0.0029881292381157567  SSIM =  0.7834962821432523\n",
      "Batch =  375  Train Loss =  0.003019442413235083  SSIM =  0.7809363689025243\n",
      "Batch =  400  Train Loss =  0.0030453999073506567  SSIM =  0.7799501548707485\n",
      "Batch =  425  Train Loss =  0.0030168555019532933  SSIM =  0.7817065520146314\n",
      "Batch =  450  Train Loss =  0.0030324791780569488  SSIM =  0.7809009593725205\n",
      "Batch =  475  Train Loss =  0.003025140742844853  SSIM =  0.7811514272815303\n",
      "Batch =  500  Train Loss =  0.003073843187536113  SSIM =  0.7782857126891612\n",
      "Batch =  525  Train Loss =  0.003076658757026529  SSIM =  0.778215626960709\n",
      "Batch =  550  Train Loss =  0.0030745640125083313  SSIM =  0.7779025919532234\n",
      "Batch =  575  Train Loss =  0.003079866803980068  SSIM =  0.7781153836392838\n",
      "Batch =  600  Train Loss =  0.003048951739425926  SSIM =  0.7797968351530532\n",
      "Batch =  625  Train Loss =  0.003057307064253837  SSIM =  0.7789015057742595\n",
      "Batch =  650  Train Loss =  0.0030546022552656584  SSIM =  0.7787760293197173\n",
      "Batch =  675  Train Loss =  0.0030578065569150364  SSIM =  0.7797285762263669\n",
      "Batch =  700  Train Loss =  0.0030661893222298075  SSIM =  0.7783482526402389\n",
      "Batch =  725  Train Loss =  0.003051629004044587  SSIM =  0.7782208893987639\n",
      "Batch =  750  Train Loss =  0.0030498071532153214  SSIM =  0.778352728202939\n",
      "Batch =  775  Train Loss =  0.003036770686571066  SSIM =  0.7791970599803233\n",
      "Batch =  800  Train Loss =  0.003071781920752983  SSIM =  0.7787214571004734\n",
      "Batch =  825  Train Loss =  0.003056146528652982  SSIM =  0.7792835958572951\n",
      "Batch =  850  Train Loss =  0.003049329675388221  SSIM =  0.7799315300014089\n",
      "Batch =  875  Train Loss =  0.0030400371983414515  SSIM =  0.780203650282962\n",
      "Batch =  900  Train Loss =  0.0030312567957864506  SSIM =  0.78063912205812\n",
      "Batch =  925  Train Loss =  0.003044728938052808  SSIM =  0.7800373505055904\n",
      "Batch =  950  Train Loss =  0.0030321798889474992  SSIM =  0.7802400018627706\n",
      "Batch =  975  Train Loss =  0.0030239780430938714  SSIM =  0.7801623804103105\n",
      "Batch =  1000  Train Loss =  0.0030463305000885156  SSIM =  0.7800246608071029\n",
      "Batch =  1025  Train Loss =  0.003068437810700455  SSIM =  0.779209835365778\n",
      "Batch =  1050  Train Loss =  0.0030748405729814077  SSIM =  0.7787341385902393\n",
      "Batch =  1075  Train Loss =  0.0030750159171361174  SSIM =  0.7789065014726894\n",
      "Batch =  1100  Train Loss =  0.0030901551503682806  SSIM =  0.7787199563973329\n",
      "Batch =  1125  Train Loss =  0.0030808915339229216  SSIM =  0.7793095555139913\n",
      "Batch =  1150  Train Loss =  0.0031056359300577164  SSIM =  0.7784315731700348\n",
      "Batch =  1175  Train Loss =  0.0031197584640021694  SSIM =  0.7778401225519941\n",
      "Batch =  1200  Train Loss =  0.003128984825173878  SSIM =  0.7774290072514365\n",
      "Batch =  1225  Train Loss =  0.0031171069820102645  SSIM =  0.7780592307357156\n",
      "Batch =  1250  Train Loss =  0.0031221472925855778  SSIM =  0.7774927283138037\n",
      "Batch =  1275  Train Loss =  0.0031212552856338923  SSIM =  0.7775189538767525\n",
      "Batch =  1300  Train Loss =  0.0031085657552578664  SSIM =  0.7781320590096025\n",
      "Batch =  1325  Train Loss =  0.00311172756729955  SSIM =  0.7783862318717084\n",
      "Batch =  1350  Train Loss =  0.003110962544203546  SSIM =  0.7782536970768814\n",
      "Batch =  1375  Train Loss =  0.003124659346194345  SSIM =  0.7778268178240819\n",
      "Batch =  1400  Train Loss =  0.0031300138951025603  SSIM =  0.7774222820119134\n",
      "Batch =  1425  Train Loss =  0.0031259294700902774  SSIM =  0.7775908552737613\n",
      "Batch =  1450  Train Loss =  0.003128329837756986  SSIM =  0.7771073139510278\n",
      "Batch =  1475  Train Loss =  0.0031286942359884822  SSIM =  0.7772094194641558\n",
      "Batch =  1500  Train Loss =  0.0031240124155398614  SSIM =  0.7772856911097964\n",
      "Batch =  1525  Train Loss =  0.0031226868270686444  SSIM =  0.7773247238822648\n",
      "Batch =  1550  Train Loss =  0.003114389569203805  SSIM =  0.7775224770245052\n",
      "Batch =  1575  Train Loss =  0.0031185785722678584  SSIM =  0.7776544901941503\n",
      "Batch =  1600  Train Loss =  0.0031274127147389663  SSIM =  0.7775101772858761\n",
      "Batch =  1625  Train Loss =  0.0031325246268220676  SSIM =  0.7773377356322912\n",
      "Batch =  1650  Train Loss =  0.003131825243992463  SSIM =  0.7773959952118722\n",
      "Batch =  1675  Train Loss =  0.0031325820195853516  SSIM =  0.7770829939108286\n",
      "Batch =  1700  Train Loss =  0.003128453371626914  SSIM =  0.7772443923507543\n",
      "Batch =  1725  Train Loss =  0.0031293972984017073  SSIM =  0.777395475115897\n",
      "Batch =  1750  Train Loss =  0.003132112718935657  SSIM =  0.7769947249123028\n",
      "Batch =  1775  Train Loss =  0.003124392376917506  SSIM =  0.7773274796445605\n",
      "Batch =  1800  Train Loss =  0.003122941783868656  SSIM =  0.7775764683551258\n",
      "Batch =  1825  Train Loss =  0.0031205723893331133  SSIM =  0.777994690243512\n",
      "Batch =  1850  Train Loss =  0.003123525077763416  SSIM =  0.7779449789588516\n",
      "Batch =  1875  Train Loss =  0.0031151466807505736  SSIM =  0.7783373999834061\n",
      "Batch =  1900  Train Loss =  0.0031258938828097214  SSIM =  0.7777964841848926\n",
      "Batch =  1925  Train Loss =  0.0031250114827991353  SSIM =  0.7778659802365613\n",
      "Batch =  1950  Train Loss =  0.0031181533072137824  SSIM =  0.7778280424307554\n",
      "Batch =  1975  Train Loss =  0.0031177015149158344  SSIM =  0.7777678487647938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch =  2000  Train Loss =  0.0031069120233150897  SSIM =  0.7782062455564738\n",
      "Batch =  2025  Train Loss =  0.0031061212293499  SSIM =  0.7783116244975431\n",
      "Batch =  2050  Train Loss =  0.003107757278385845  SSIM =  0.7782331279865126\n",
      "Batch =  2075  Train Loss =  0.003102641662495794  SSIM =  0.778449426645256\n",
      "Batch =  2100  Train Loss =  0.003104677383020143  SSIM =  0.7786480883402483\n",
      "Batch =  2125  Train Loss =  0.0030945091961663874  SSIM =  0.7792159280075747\n",
      "Batch =  2150  Train Loss =  0.0030984855517149404  SSIM =  0.7789989973015563\n",
      "Batch =  2175  Train Loss =  0.003098878314178009  SSIM =  0.7790861517670511\n",
      "Batch =  2200  Train Loss =  0.0031058137339740905  SSIM =  0.7788160467892885\n",
      "Loader #:  0  Epoch:  14 /  2500  -  Time:  0:04:09.264203 s - Validation Loss:  0.0  - Train Loss:  0.0031058137339740905  - train SSIM:  0.7788160467892885  validation SSIM:  1713.3953029364347\n",
      "Batch =  1  Train Loss =  0.0012357706436887383  SSIM =  0.9006042182445526\n",
      "Batch =  25  Train Loss =  0.0026124796154908835  SSIM =  0.8090798044204712\n",
      "Batch =  50  Train Loss =  0.002680253582657315  SSIM =  0.8095346793532372\n",
      "Batch =  75  Train Loss =  0.002782439343864098  SSIM =  0.8042765973011653\n",
      "Batch =  100  Train Loss =  0.0029628301522461697  SSIM =  0.7942938524484634\n",
      "Batch =  125  Train Loss =  0.002982804587110877  SSIM =  0.7916841045618057\n",
      "Batch =  150  Train Loss =  0.002974506159662269  SSIM =  0.7892320173978805\n",
      "Batch =  175  Train Loss =  0.0030483678816485087  SSIM =  0.7863700187206268\n",
      "Batch =  200  Train Loss =  0.0030655831246986054  SSIM =  0.7841868465393782\n",
      "Batch =  225  Train Loss =  0.00308110162227725  SSIM =  0.7824605214595795\n",
      "Batch =  250  Train Loss =  0.0029938492239452896  SSIM =  0.7865621703863144\n",
      "Batch =  275  Train Loss =  0.003029674118628133  SSIM =  0.7838918601382863\n",
      "Batch =  300  Train Loss =  0.0030546208461843588  SSIM =  0.7834734985729058\n",
      "Batch =  325  Train Loss =  0.0030637713509629695  SSIM =  0.7831241550812355\n",
      "Batch =  350  Train Loss =  0.0031017414495831223  SSIM =  0.781957899757794\n",
      "Batch =  375  Train Loss =  0.0031292611142077174  SSIM =  0.7797230087518692\n",
      "Batch =  400  Train Loss =  0.003147504874832521  SSIM =  0.7786983672529459\n",
      "Batch =  425  Train Loss =  0.0031582813511041048  SSIM =  0.7776853791405173\n",
      "Batch =  450  Train Loss =  0.003178272333041403  SSIM =  0.7760014300545056\n",
      "Batch =  475  Train Loss =  0.003157337377380923  SSIM =  0.7766699110520513\n",
      "Batch =  500  Train Loss =  0.0031179450151685158  SSIM =  0.7774785774350166\n",
      "Batch =  525  Train Loss =  0.003141357301529275  SSIM =  0.7767451620953424\n",
      "Batch =  550  Train Loss =  0.0031277167253905315  SSIM =  0.7779426511038433\n",
      "Batch =  575  Train Loss =  0.003120382307578162  SSIM =  0.7780588513871898\n",
      "Batch =  600  Train Loss =  0.0031102771257428686  SSIM =  0.7781469008574883\n",
      "Batch =  625  Train Loss =  0.0030932716539828105  SSIM =  0.7787662718057633\n",
      "Batch =  650  Train Loss =  0.0031079174306628725  SSIM =  0.7772448100722753\n",
      "Batch =  675  Train Loss =  0.003106886234061137  SSIM =  0.777163547983876\n",
      "Batch =  700  Train Loss =  0.003079701718587395  SSIM =  0.7783013765513896\n",
      "Batch =  725  Train Loss =  0.003064727160080079  SSIM =  0.7789147980254272\n",
      "Batch =  750  Train Loss =  0.0030711021815077403  SSIM =  0.7790859510501226\n",
      "Batch =  775  Train Loss =  0.0030788450302638775  SSIM =  0.778844752023297\n",
      "Batch =  800  Train Loss =  0.003086240192842524  SSIM =  0.7783600191026926\n",
      "Batch =  825  Train Loss =  0.0030915940233513317  SSIM =  0.7781246596032922\n",
      "Batch =  850  Train Loss =  0.003093250393268296  SSIM =  0.7775853689102565\n",
      "Batch =  875  Train Loss =  0.0031073121525779634  SSIM =  0.7771878103869302\n",
      "Batch =  900  Train Loss =  0.0030962202427811765  SSIM =  0.7771965677870645\n",
      "Batch =  925  Train Loss =  0.003092365150884222  SSIM =  0.7772397021828471\n",
      "Batch =  950  Train Loss =  0.003072670401138327  SSIM =  0.7785309922852014\n",
      "Batch =  975  Train Loss =  0.0030624622100134356  SSIM =  0.7789440663502767\n",
      "Batch =  1000  Train Loss =  0.0030648801963106963  SSIM =  0.7787117397487163\n",
      "Batch =  1025  Train Loss =  0.00304694310257995  SSIM =  0.7793611748189461\n",
      "Batch =  1050  Train Loss =  0.0030642261191055047  SSIM =  0.7785331381928353\n",
      "Batch =  1075  Train Loss =  0.0030724039767614326  SSIM =  0.7776618936311367\n",
      "Batch =  1100  Train Loss =  0.0030917681056863247  SSIM =  0.7768759141320531\n",
      "Batch =  1125  Train Loss =  0.0030906972458405006  SSIM =  0.7768611096938451\n",
      "Batch =  1150  Train Loss =  0.0030904230705345983  SSIM =  0.7765456003728121\n",
      "Batch =  1175  Train Loss =  0.003093028170059118  SSIM =  0.7768269091844558\n",
      "Batch =  1200  Train Loss =  0.00309850682241328  SSIM =  0.7765572940930724\n",
      "Batch =  1225  Train Loss =  0.003098289247789914  SSIM =  0.7763002493308515\n",
      "Batch =  1250  Train Loss =  0.003099188738025259  SSIM =  0.776236033475399\n",
      "Batch =  1275  Train Loss =  0.0030873526264707943  SSIM =  0.7768887692222408\n",
      "Batch =  1300  Train Loss =  0.0030796518828607915  SSIM =  0.7772889572152725\n",
      "Batch =  1325  Train Loss =  0.003078043851946516  SSIM =  0.7775816195191078\n",
      "Batch =  1350  Train Loss =  0.0030778228606774334  SSIM =  0.77785538252857\n",
      "Batch =  1375  Train Loss =  0.0030947781362337993  SSIM =  0.7769195528355511\n",
      "Batch =  1400  Train Loss =  0.0030877317097890355  SSIM =  0.777270843865616\n",
      "Batch =  1425  Train Loss =  0.003085430916939993  SSIM =  0.7772696609873521\n",
      "Batch =  1450  Train Loss =  0.003098814174591098  SSIM =  0.7765312136350007\n",
      "Batch =  1475  Train Loss =  0.003092811629225468  SSIM =  0.7768509873293213\n",
      "Batch =  1500  Train Loss =  0.0031028106331456606  SSIM =  0.7764721963405609\n",
      "Batch =  1525  Train Loss =  0.0030974692398048633  SSIM =  0.7769497684084001\n",
      "Batch =  1550  Train Loss =  0.0031035746620299534  SSIM =  0.7768967849208462\n",
      "Batch =  1575  Train Loss =  0.0030921001007433034  SSIM =  0.7771377949676816\n",
      "Batch =  1600  Train Loss =  0.003096362546348246  SSIM =  0.7771785289328546\n",
      "Batch =  1625  Train Loss =  0.0030917297690891874  SSIM =  0.7776252436546179\n",
      "Batch =  1650  Train Loss =  0.0030853020005055113  SSIM =  0.7780973886991992\n",
      "Batch =  1675  Train Loss =  0.0030840100880726172  SSIM =  0.7780926871922479\n",
      "Batch =  1700  Train Loss =  0.003069901596412391  SSIM =  0.7789991837126368\n",
      "Batch =  1725  Train Loss =  0.003065477680158464  SSIM =  0.7792239077972329\n",
      "Batch =  1750  Train Loss =  0.0030654781509989073  SSIM =  0.779453500032425\n",
      "Batch =  1775  Train Loss =  0.003078623444045251  SSIM =  0.7789210855037394\n",
      "Batch =  1800  Train Loss =  0.0030727162133229687  SSIM =  0.7789455873684751\n",
      "Batch =  1825  Train Loss =  0.00307542183627821  SSIM =  0.7788494176244083\n",
      "Batch =  1850  Train Loss =  0.0030743438556375033  SSIM =  0.7784280741939673\n",
      "Batch =  1875  Train Loss =  0.0030711068466460953  SSIM =  0.7788834422210852\n",
      "Batch =  1900  Train Loss =  0.003075834014835939  SSIM =  0.7788746377845344\n",
      "Batch =  1925  Train Loss =  0.0030797373425282006  SSIM =  0.778399988879244\n",
      "Batch =  1950  Train Loss =  0.0030826482789752144  SSIM =  0.7785477911298856\n",
      "Batch =  1975  Train Loss =  0.003085334577650728  SSIM =  0.7785404499290111\n",
      "Batch =  2000  Train Loss =  0.003088606620556675  SSIM =  0.7784820512998849\n",
      "Batch =  2025  Train Loss =  0.0030877055450814006  SSIM =  0.7787238514000251\n",
      "Batch =  2050  Train Loss =  0.0030849655353884418  SSIM =  0.7789215007878658\n",
      "Batch =  2075  Train Loss =  0.0030829394383759924  SSIM =  0.7791151651291244\n",
      "Batch =  2100  Train Loss =  0.003088134841001149  SSIM =  0.7789782204177408\n",
      "Batch =  2125  Train Loss =  0.0030883102089659695  SSIM =  0.7790208058304646\n",
      "Batch =  2150  Train Loss =  0.003090532942956617  SSIM =  0.7790210362454486\n",
      "Batch =  2175  Train Loss =  0.003089207644338837  SSIM =  0.7789578627940567\n",
      "Batch =  2200  Train Loss =  0.0030850482118231328  SSIM =  0.7793546796336093\n",
      "Loader #:  0  Epoch:  15 /  2500  -  Time:  0:04:16.234619 s - Validation Loss:  0.0  - Train Loss:  0.0030850482118231328  - train SSIM:  0.7793546796336093  validation SSIM:  1714.5802951939404\n",
      "Batch =  1  Train Loss =  0.002217809669673443  SSIM =  0.8093157410621643\n",
      "Batch =  25  Train Loss =  0.0025933590228669344  SSIM =  0.7794977134466171\n",
      "Batch =  50  Train Loss =  0.002779519263422117  SSIM =  0.7866814342141152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch =  75  Train Loss =  0.0028841786684157948  SSIM =  0.7854289493958155\n",
      "Batch =  100  Train Loss =  0.002808906235732138  SSIM =  0.7890642014145851\n",
      "Batch =  125  Train Loss =  0.002839019351406023  SSIM =  0.7849629414081574\n",
      "Batch =  150  Train Loss =  0.002907514341835243  SSIM =  0.7816949301958084\n",
      "Batch =  175  Train Loss =  0.002950042761097263  SSIM =  0.7802728175265449\n",
      "Batch =  200  Train Loss =  0.0030185347357473804  SSIM =  0.7776013704389334\n",
      "Batch =  225  Train Loss =  0.0030432726110383454  SSIM =  0.7775490064091153\n",
      "Batch =  250  Train Loss =  0.00303743092273362  SSIM =  0.7793477480411529\n",
      "Batch =  275  Train Loss =  0.002968073976226151  SSIM =  0.7827562207525427\n",
      "Batch =  300  Train Loss =  0.003059631379728671  SSIM =  0.7801860929528872\n",
      "Batch =  325  Train Loss =  0.0031454424378283036  SSIM =  0.7770981231561074\n",
      "Batch =  350  Train Loss =  0.0031026256076958298  SSIM =  0.7773273464185851\n",
      "Batch =  375  Train Loss =  0.0031108097650576383  SSIM =  0.7772704650958379\n",
      "Batch =  400  Train Loss =  0.0031097694046184187  SSIM =  0.7778714638575912\n",
      "Batch =  425  Train Loss =  0.003114343983710141  SSIM =  0.7774740854782217\n",
      "Batch =  450  Train Loss =  0.0031122173730465065  SSIM =  0.7770204035441081\n",
      "Batch =  475  Train Loss =  0.0031295075482615318  SSIM =  0.7769173196742409\n",
      "Batch =  500  Train Loss =  0.003121361430909019  SSIM =  0.7762093401849269\n",
      "Batch =  525  Train Loss =  0.00312578120407471  SSIM =  0.7764397829771041\n",
      "Batch =  550  Train Loss =  0.0031385217024415563  SSIM =  0.7761864345182072\n",
      "Batch =  575  Train Loss =  0.0031762601171478467  SSIM =  0.7749895523682885\n",
      "Batch =  600  Train Loss =  0.003197813505200126  SSIM =  0.7738637256125609\n",
      "Batch =  625  Train Loss =  0.003191674077510834  SSIM =  0.7740580130338669\n",
      "Batch =  650  Train Loss =  0.0031874473524378397  SSIM =  0.7742996046634821\n",
      "Batch =  675  Train Loss =  0.003185671150481708  SSIM =  0.7748854995215381\n",
      "Batch =  700  Train Loss =  0.003185073982063581  SSIM =  0.7746875598600933\n",
      "Batch =  725  Train Loss =  0.003182169334613718  SSIM =  0.7746400445905225\n",
      "Batch =  750  Train Loss =  0.003167010043767126  SSIM =  0.7750933492382367\n",
      "Batch =  775  Train Loss =  0.0031672598262535287  SSIM =  0.776250067757022\n",
      "Batch =  800  Train Loss =  0.0031662914753360382  SSIM =  0.7760861966200173\n",
      "Batch =  825  Train Loss =  0.003171020654880329  SSIM =  0.7756433088851697\n",
      "Batch =  850  Train Loss =  0.0031732454283251977  SSIM =  0.775392392400433\n",
      "Batch =  875  Train Loss =  0.0031597892055454267  SSIM =  0.7757508649315152\n",
      "Batch =  900  Train Loss =  0.003166793189415734  SSIM =  0.776043689681424\n",
      "Batch =  925  Train Loss =  0.003183084079394797  SSIM =  0.7752992025259379\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m G_optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(G\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#D_optimizer = torch.optim.Adam(D.parameters(), lr=lr, weight_decay=1e-4)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m losses_D, losses_G, D, G \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_JSGANO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_optimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mtrain_JSGANO\u001b[0;34m(D, G, train_data, epochs, D_optim, G_optim, scheduler)\u001b[0m\n\u001b[1;32m     96\u001b[0m ssim_batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ssim_individual\u001b[38;5;241m/\u001b[39my\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     97\u001b[0m loss \u001b[38;5;241m=\u001b[39m W_loss\n\u001b[0;32m---> 98\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m loss_G \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()            \n\u001b[1;32m    100\u001b[0m G_optim\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/depot/bera89/apps/IDEAS_hviswan/lib/python3.8/site-packages/torch/_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    248\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    249\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    254\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 255\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/bera89/apps/IDEAS_hviswan/lib/python3.8/site-packages/torch/autograd/__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 147\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#D = torch.load('/depot/bera89/data/hviswan/GANO_DISC300.pt')\n",
    "#D.eval()\n",
    "#G = torch.load('/depot/bera89/data/hviswan/UNO_2f_vimeo_huge_rgb20.pt')\n",
    "#G = torch.load('/depot/bera89/data/hviswan/UNO_2f_vimeo_complex_norm_rgb400.pt')\n",
    "#G = torch.load('/depot/bera89/data/hviswan/UNO_2f_vimeo_vgg_rgb100.pt')\n",
    "#G = torch.load('/depot/bera89/data/hviswan/UNO_2f_vimeo_lowres_rgb300.pt')\n",
    "#G = torch.load('/depot/bera89/data/hviswan/UNO_2f_vimeo_lowres_vgg_rgb_huge940.pt')\n",
    "#summary(G, input_size=(2,10, 3,85,85))\n",
    "G.train()\n",
    "device = 'cuda:11'\n",
    "#G = torch.load('/depot/bera89/data/hviswan/UNO_2f_vimeo_full50.pt')\n",
    "#D.train()\n",
    "G.train()\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "#D_optimizer = torch.optim.Adam(D.parameters(), lr=lr, weight_decay=1e-4)\n",
    "losses_D, losses_G, D, G = train_JSGANO(None, G, None, epochs, None, G_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400068b6",
   "metadata": {
    "id": "400068b6"
   },
   "source": [
    "ite = 2\n",
    "s = '~/GANO/Figures/GANO_GRF/{}GRF_HistogramGANO.pdf'.format(ite)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e521bfc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "executionInfo": {
     "elapsed": 2412,
     "status": "error",
     "timestamp": 1677864857289,
     "user": {
      "displayName": "Hrishikesh Viswanath",
      "userId": "01021904601979213475"
     },
     "user_tz": 300
    },
    "id": "3e521bfc",
    "outputId": "f424a1af-0f0b-4871-c703-a262896349bc"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "#import pylab as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "#from skimage.metrics import peak_signal_noise_ratio as PSNR\n",
    "from math import log10, sqrt\n",
    "myloss = torch.nn.L1Loss()\n",
    "def PSNR(original, compressed):\n",
    "    mse = np.mean((original - compressed) ** 2)\n",
    "    if(mse == 0):  # MSE is zero means no noise is present in the signal .\n",
    "                  # Therefore PSNR have no importance.\n",
    "        return 100\n",
    "    max_pixel = 255.0\n",
    "    psnr = 20 * log10(max_pixel / sqrt(mse))\n",
    "    return psnr\n",
    "def mse(imageA, imageB):\n",
    "    # the 'Mean Squared Error' between the two images is the sum of the squared difference between the two images\n",
    "    mse_error = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "    mse_error /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    return mse_error\n",
    "#model_loaded = torch.load('/depot/bera89/data/hviswan/UNO_2f_vimeo_rgb595.pt')\n",
    "base_G = torch.load('/depot/bera89/data/hviswan/UNO_2f_vimeo_lowres_rgb300.pt')\n",
    "complex_G = torch.load('/depot/bera89/data/hviswan/UNO_2f_vimeo_lowres_vgg_rgb_huge940.pt')\n",
    "backSub = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "#x_train = torch.load('/scratch/gilbreth/hviswan/full_res_vimeo_rgb.pt')\n",
    "#y_train = torch.load('/scratch/gilbreth/hviswan/full_res_gt_rgb.pt')\n",
    "#x_train = torch.permute(x_train, (1, 0,2,3,4))\n",
    "#print(x_train.shape)\n",
    "#ntrain = x_train.shape[1]\n",
    "#y_train = y_train.reshape(ntrain, x_train.shape[2], x_train.shape[3], x_train.shape[4])\n",
    "\n",
    "#y_normalizer = MinMaxNormalizer(y_train)\n",
    "#x_normalizer = MinMaxNormalizer(x_train)\n",
    "#x_train = x_normalizer.encode(x_train)\n",
    "#y_train = y_normalizer.encode(y_train)\n",
    "\n",
    "#test_data = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train[0], x_train[1], y_train), batch_size=1, shuffle=True)\n",
    "complex_G.eval()\n",
    "base_G.eval()\n",
    "#test_data, y_normalizer = LoadDataBatches(0, batches=10, isNormalized=True, isTrain=False)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "input_l = x_test[0][71].reshape(1, 100, 100).cpu().detach().numpy()\n",
    "input_r = x_test[1][71].reshape(1, 100, 100).cpu().detach().numpy()\n",
    "input = torch.from_numpy(np.asarray([input_l,input_r]).astype(np.float32))\n",
    "\n",
    "\n",
    "\n",
    "model_out = model_loaded(input.cuda())\n",
    "model_out = model_out.reshape(100,100).cpu().detach().numpy()\n",
    "output = y_test[71].reshape(100,100).cpu().detach().numpy()\n",
    "#model_out = y_normalizer.decode(model_out).cpu().detach().numpy()\n",
    "model_out = model_out.reshape(100,100)\n",
    "plt.imshow(model_out)\n",
    "plt.show()\n",
    "plt.imsave('/depot/bera89/data/hviswan/vimeo_generated.png', model_out)\n",
    "plt.imshow( output)\n",
    "plt.show()\n",
    "plt.imsave('/depot/bera89/data/hviswan/vimeo_groundTruth.png', output)\n",
    "#x_train = torch.permute(x_train, (0,2,3,1))\n",
    "#input = torch.permute(input, (0,3,1,2))\n",
    "#input1 = ((input[0][0]+input[0][1])/2.0).reshape(100,100).cpu().detach().numpy()\n",
    "input1 = ((input_l+ input_r)/2.0).reshape(100, 100)\n",
    "plt.imshow( input1)\n",
    "plt.imsave('/depot/bera89/data/hviswan/vimeo_mean.png', input1)\n",
    "plt.imsave('/depot/bera89/data/hviswan/vimeo_left.png', input_l.reshape(100,100))\n",
    "plt.imsave('/depot/bera89/data/hviswan/vimeo_right.png', input_r.reshape(100,100))\n",
    "plt.show()\n",
    "\n",
    "print(ssim(model_out, output))\n",
    "print(ssim(input1, output))\n",
    "print(ssim(model_out, input1))\n",
    "print(mse(model_out, output))\n",
    "print(mse(input1, output))\n",
    "print(mse(model_out, input1))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "train_data, y_normalizer = LoadDataBatches(0, isNormalized=True, batches=2, isTrain=False)\n",
    "loss_D = 0.0\n",
    "loss_G = 0.0\n",
    "ssim_G = 0.0\n",
    "train_counter = 0\n",
    "ssim_batch=0\n",
    "psnr_G = 0.0\n",
    "counter = 0.0\n",
    "print_counter = 0\n",
    "for xl,xr,y in train_data:\n",
    "    train_counter += 1\n",
    "    mean = (xl[0]+xr[0])/2.0\n",
    "    mean = y_normalizer.decode(mean)\n",
    "    mean = mean.cpu().detach().numpy()\n",
    "    xl = xl.cpu().detach().numpy()\n",
    "    xr = xr.cpu().detach().numpy()\n",
    "    x = torch.from_numpy(np.asarray([xl,xr]).astype(np.float32))\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    base_x_out = base_G(x)\n",
    "    x = torch.stack((base_x_out, base_x_out))\n",
    "    x_syn = complex_G(x).reshape(x.shape[1], x.shape[2],x.shape[3],x.shape[4])\n",
    "    y = y.reshape(y.shape[0], y.shape[1],y.shape[2],y.shape[3])\n",
    "    x_syn = y_normalizer.decode(x_syn)\n",
    "    y = y_normalizer.decode(y)\n",
    "    #y2 = y_normalizer.encode(y).detach().cpu().numpy()\n",
    "    #x2 = y_normalizer.encode(x_syn).detach().cpu().numpy()\n",
    "    W_loss = myloss(x_syn, y).item()\n",
    "    y = y.detach().cpu().numpy()\n",
    "    x_syn = x_syn.detach().cpu().numpy()\n",
    "    ssval = ssim(x_syn[0], y[0], multichannel=True)\n",
    "    #print(\"SSVAL = \", ssval)\n",
    "    \n",
    "    if(ssval>0.75 and print_counter<20):\n",
    "        mean = cv2.cvtColor(mean, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(mean.astype('uint8'))\n",
    "        plt.show()\n",
    "        \n",
    "        x_syn_show = cv2.cvtColor(x_syn[0], cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(x_syn_show.astype('uint8'))\n",
    "        plt.show()\n",
    "        gt = cv2.cvtColor(y[0], cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(gt.astype('uint8'))\n",
    "        plt.show()\n",
    "        print(\"SSIM MEANvSYN \", ssim(mean, x_syn[0], multichannel=True))\n",
    "        print(\"SSIM MEANvEXP \", ssim(mean, y[0], multichannel=True))\n",
    "        print(\"SSIM SYNvEXP \", ssim(x_syn[0], y[0], multichannel=True))\n",
    "        print(\"SSIM = \", ssim(x_syn[0], y[0], multichannel=True))\n",
    "        print_counter += 1\n",
    "    #if(x.shape[1]>=7):\n",
    "    ssim_batch += ssim(x_syn[0], y[0], multichannel=True)\n",
    "    ssim_G = ssim_batch\n",
    "    #loss = loss_D_real + loss_D_syn\n",
    "    loss = W_loss\n",
    "    #print(loss.shape)\n",
    "    #loss.backward()\n",
    "    loss_G += loss\n",
    "    psnr_G += PSNR(x_syn, y)\n",
    "    if(train_counter %5 == 0):\n",
    "        print(\"Batch = \",train_counter,\" Train Loss = \", loss_G/train_counter, \" SSIM = \", ssim_batch/(train_counter-1), \" PSNR = \", psnr_G/train_counter)\n",
    "losses_D = loss_D / train_counter\n",
    "losses_G = loss_G / train_counter\n",
    "ssims_G = ssim_G / train_counter\n",
    "print(losses_G)\n",
    "print(ssims_G)\n",
    "print(psnr_G /train_counter)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-colon",
   "metadata": {
    "id": "limiting-colon"
   },
   "outputs": [],
   "source": [
    "\"\"\"plt.plot(np.arange(epochs), losses_D, c='k', label='D')\n",
    "plt.plot(np.arange(epochs), losses_G, c='b', label='G')\n",
    "plt.legend()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.show()\"\"\"\n",
    "print(losses_G)\n",
    "print(ssims_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb4c790",
   "metadata": {
    "id": "7eb4c790"
   },
   "source": [
    "# DISFA+ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ecda50",
   "metadata": {
    "id": "30ecda50"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "#import pylab as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "#from skimage.metrics import peak_signal_noise_ratio as PSNR\n",
    "from math import log10, sqrt\n",
    "def PSNR(original, compressed):\n",
    "    mse = np.mean((original - compressed) ** 2)\n",
    "    if(mse == 0):  # MSE is zero means no noise is present in the signal .\n",
    "                  # Therefore PSNR have no importance.\n",
    "        return 100\n",
    "    max_pixel = 255.0\n",
    "    psnr = 10 * log10(max_pixel / sqrt(mse))\n",
    "    return psnr\n",
    "def mse(imageA, imageB):\n",
    "    # the 'Mean Squared Error' between the two images is the sum of the squared difference between the two images\n",
    "    mse_error = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "    mse_error /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    return mse_error\n",
    "model_loaded = torch.load('/depot/bera89/data/hviswan/UNO_2f_vimeo_full60.pt')\n",
    "model_loaded.eval()\n",
    "x_train = torch.load('../Fall22 IDEAS/x_train.pt')\n",
    "y_train = torch.load('../Fall22 IDEAS/y_train.pt')\n",
    "x_test = torch.load('../Fall22 IDEAS/x_test.pt')\n",
    "y_test = torch.load('../Fall22 IDEAS/y_test.pt')\n",
    "x_train = torch.permute(x_train, (1,0,2,3))\n",
    "x_test = torch.permute(x_test, (1,0,2,3))\n",
    "y_normalizer = MinMaxNormalizer(y_train)\n",
    "x_normalizer = MinMaxNormalizer(x_train)\n",
    "#x_train = x_normalizer.encode(x_train)\n",
    "#x_train = y_normalizer.encode(y_train)\n",
    "y_normalizer = MinMaxNormalizer(y_test)\n",
    "x_normalizer = MinMaxNormalizer(x_test)\n",
    "#x_test = x_normalizer.encode(x_test)\n",
    "#x_test = y_normalizer.encode(y_test)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train[0], x_train[1], y_train), batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test[0], x_test[1], y_test), batch_size=10, shuffle=True)\n",
    "input_l = x_test[0][12].reshape(1, 100, 100).cpu().detach().numpy()\n",
    "input_r = x_test[1][12].reshape(1, 100, 100).cpu().detach().numpy()\n",
    "input = torch.from_numpy(np.asarray([input_l,input_r]).astype(np.float32))\n",
    "\n",
    "\n",
    "\n",
    "model_out = model_loaded(input.cuda())\n",
    "model_out = model_out.reshape(100,100).cpu().detach().numpy()\n",
    "output = y_test[12].reshape(100,100).cpu().detach().numpy()\n",
    "#model_out = y_normalizer.decode(model_out).cpu().detach().numpy()\n",
    "model_out = model_out.reshape(100,100)\n",
    "plt.imshow( model_out)\n",
    "plt.show()\n",
    "plt.imsave('/depot/bera89/data/hviswan/disfa_generated.png', model_out)\n",
    "plt.imshow( output)\n",
    "plt.show()\n",
    "plt.imsave('/depot/bera89/data/hviswan/disfa_groundTruth.png', output)\n",
    "#x_train = torch.permute(x_train, (0,2,3,1))\n",
    "#input = torch.permute(input, (0,3,1,2))\n",
    "#input1 = ((input[0][0]+input[0][1])/2.0).reshape(100,100).cpu().detach().numpy()\n",
    "input1 = ((input_l+ input_r)/2.0).reshape(100, 100)\n",
    "plt.imshow( input1)\n",
    "plt.show()\n",
    "plt.imsave('/depot/bera89/data/hviswan/disfa_mean.png', input1)\n",
    "plt.imsave('/depot/bera89/data/hviswan/disfa_left.png', input_l.reshape(100,100))\n",
    "plt.imsave('/depot/bera89/data/hviswan/disfa_right.png', input_r.reshape(100,100))\n",
    "print(ssim(model_out, output))\n",
    "print(ssim(input1, output))\n",
    "print(ssim(model_out, input1))\n",
    "print(mse(model_out, output))\n",
    "print(mse(input1, output))\n",
    "print(mse(model_out, input1))\n",
    "print(PSNR(model_out, output))\n",
    "print(PSNR(input1, output))\n",
    "print(PSNR(model_out, input1))\n",
    "\n",
    "loss_D = 0.0\n",
    "loss_G = 0.0\n",
    "ssim_G = 0.0\n",
    "train_counter = 0\n",
    "ssim_batch=0\n",
    "psnr_G = 0.0\n",
    "for xl,xr,y in test_loader:\n",
    "    train_counter += 1\n",
    "    xl = xl.cpu().detach().numpy()\n",
    "    xr = xr.cpu().detach().numpy()\n",
    "    x = torch.from_numpy(np.asarray([xl,xr]).astype(np.float32))\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "\n",
    "    x_syn = model_loaded(x).reshape(x.shape[1], x.shape[2],x.shape[3],x.shape[4])\n",
    "    y = y.reshape(x.shape[1], 100, 100)\n",
    "    y2 = y_normalizer.encode(y).detach().cpu().numpy()\n",
    "    x2 = y_normalizer.encode(x_syn).detach().cpu().numpy()\n",
    "\n",
    "    y = y.detach().cpu().numpy()\n",
    "    x_syn = x_syn.detach().cpu().numpy()\n",
    "    W_loss = myloss(x_syn, y).item()\n",
    "    if(x.shape[1]>=7):\n",
    "        ssim_batch += ssim(x_syn, y, multichannel=True)\n",
    "        ssim_G = ssim_batch\n",
    "\n",
    "    loss = W_loss\n",
    "    psnr_G += PSNR(x2, y2)\n",
    "\n",
    "    loss_G += loss\n",
    "\n",
    "    if(train_counter %10 == 0):\n",
    "        print(\"Batch = \",train_counter,\" Train Loss = \", loss_G/train_counter, \" SSIM = \", ssim_batch/(train_counter))\n",
    "losses_D = loss_D / train_counter\n",
    "losses_G = loss_G / train_counter\n",
    "ssims_G = ssim_G / train_counter\n",
    "print(losses_G)\n",
    "print(ssims_G)\n",
    "print(psnr_G / train_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2109180e",
   "metadata": {
    "id": "2109180e"
   },
   "source": [
    "# DAVIS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a188d89",
   "metadata": {
    "id": "0a188d89"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "#import pylab as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "#from skimage.metrics import peak_signal_noise_ratio as PSNR\n",
    "from math import log10, sqrt\n",
    "def PSNR(original, compressed):\n",
    "    mse = np.mean((original - compressed) ** 2)\n",
    "    if(mse == 0):  # MSE is zero means no noise is present in the signal .\n",
    "                  # Therefore PSNR have no importance.\n",
    "        return 100\n",
    "    max_pixel = np.max(original)\n",
    "    psnr = 20 * log10(max_pixel / sqrt(mse))\n",
    "    return psnr\n",
    "def mse(imageA, imageB):\n",
    "    # the 'Mean Squared Error' between the two images is the sum of the squared difference between the two images\n",
    "    mse_error = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "    mse_error /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    return mse_error\n",
    "model_loaded = torch.load('/depot/bera89/data/hviswan/UNO_2f_vimeo_full60.pt')\n",
    "model_loaded.eval()\n",
    "x_train = torch.load('../Fall22 IDEAS/x_train_davis.pt')\n",
    "y_train = torch.load('../Fall22 IDEAS/y_train_davis.pt')\n",
    "x_test = torch.load('../Fall22 IDEAS/x_test_davis.pt')\n",
    "y_test = torch.load('../Fall22 IDEAS/y_test_davis.pt')\n",
    "x_train = torch.permute(x_train, (1,0,2,3))\n",
    "x_test = torch.permute(x_test, (1,0,2,3))\n",
    "y_normalizer = MinMaxNormalizer(y_train)\n",
    "x_normalizer = MinMaxNormalizer(x_train)\n",
    "x_train = x_normalizer.encode(x_train)\n",
    "y_train = y_normalizer.encode(y_train)\n",
    "y_normalizer = MinMaxNormalizer(y_test)\n",
    "x_normalizer = MinMaxNormalizer(x_test)\n",
    "x_test = x_normalizer.encode(x_test)\n",
    "y_test = y_normalizer.encode(y_test)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train[0], x_train[1], y_train), batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test[0], x_test[1], y_test), batch_size=10, shuffle=True)\n",
    "input_l = x_test[0][11].reshape(1, 100, 100).cpu().detach().numpy()\n",
    "input_r = x_test[1][11].reshape(1, 100, 100).cpu().detach().numpy()\n",
    "input = torch.from_numpy(np.asarray([input_l,input_r]).astype(np.float32))\n",
    "\n",
    "\n",
    "\n",
    "model_out = model_loaded(input.cuda())\n",
    "model_out = model_out.reshape(100,100).cpu().detach().numpy()\n",
    "output = y_test[11].reshape(100,100).cpu().detach().numpy()\n",
    "#model_out = y_normalizer.decode(model_out).cpu().detach().numpy()\n",
    "model_out = model_out.reshape(100,100)\n",
    "plt.imshow( model_out)\n",
    "plt.show()\n",
    "plt.imsave('/depot/bera89/data/hviswan/davis_generated.png', model_out)\n",
    "plt.imshow( output)\n",
    "plt.show()\n",
    "plt.imsave('/depot/bera89/data/hviswan/davis_groundTruth.png', output)\n",
    "#x_train = torch.permute(x_train, (0,2,3,1))\n",
    "#input = torch.permute(input, (0,3,1,2))\n",
    "#input1 = ((input[0][0]+input[0][1])/2.0).reshape(100,100).cpu().detach().numpy()\n",
    "input1 = ((input_l+ input_r)/2.0).reshape(100, 100)\n",
    "plt.imshow( input1)\n",
    "plt.show()\n",
    "plt.imsave('/depot/bera89/data/hviswan/davis_mean.png', input1)\n",
    "plt.imsave('/depot/bera89/data/hviswan/davis_left.png', input_l.reshape(100,100))\n",
    "plt.imsave('/depot/bera89/data/hviswan/davis_right.png', input_r.reshape(100,100))\n",
    "print(ssim(model_out, output))\n",
    "print(ssim(input1, output))\n",
    "print(ssim(model_out, input1))\n",
    "print(mse(model_out, output))\n",
    "print(mse(input1, output))\n",
    "print(mse(model_out, input1))\n",
    "\n",
    "\n",
    "loss_D = 0.0\n",
    "loss_G = 0.0\n",
    "ssim_G = 0.0\n",
    "train_counter = 0\n",
    "ssim_batch=0\n",
    "psnr_G = 0.0\n",
    "for xl,xr,y in train_loader:\n",
    "    train_counter += 1\n",
    "    xl = xl.cpu().detach().numpy()\n",
    "    xr = xr.cpu().detach().numpy()\n",
    "    x = torch.from_numpy(np.asarray([xl,xr]).astype(np.float32))\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    #D_optimizer.zero_grad()\n",
    "    #D_optimizer.zero_grad()\n",
    "    #G_optim.zero_grad()\n",
    "    #print(y.shape)\n",
    "    #y_real = D(y)\n",
    "    #print(y_real.shape)\n",
    "    #loss_D_real = fn_loss(y_real, torch.ones_like(y_real).cuda())\n",
    "    #print(x.shape)\n",
    "    x_syn = model_loaded(x).reshape(x.shape[1], 100,100)\n",
    "    y = y.reshape(x.shape[1], 100, 100)\n",
    "    #print(x_syn.shape)\n",
    "    #print(x.shape)\n",
    "    #print(y.shape)\n",
    "    #print(x_syn.shape)\n",
    "    #print(y.shape)\n",
    "    #print(x_syn.shape)\n",
    "    #print(y.shape)\n",
    "    #print(x.shape)\n",
    "    #print(\"___\")\n",
    "    #print(x_syn.shape)\n",
    "    #y_syn = D(x_syn)\n",
    "    #print(y_syn.shape)\n",
    "    #loss_D_syn = fn_loss(y_syn, torch.zeros_like(y_syn).cuda())\n",
    "    y2 = y_normalizer.encode(y).detach().cpu().numpy()\n",
    "    x2 = y_normalizer.encode(x_syn).detach().cpu().numpy()\n",
    "    y = y.detach().cpu().numpy()\n",
    "    x_syn = x_syn.detach().cpu().numpy()\n",
    "    W_loss = mse(x_syn, y)\n",
    "    if(x.shape[1]>=7):\n",
    "        ssim_batch += ssim(x_syn, y)\n",
    "        ssim_G = ssim_batch\n",
    "    #loss = loss_D_real + loss_D_syn\n",
    "    loss = W_loss\n",
    "    #print(loss.shape)\n",
    "    #loss.backward()\n",
    "    loss_G += loss\n",
    "    #loss_D += loss.item()\n",
    "    psnr_G += PSNR(x_syn, y)       \n",
    "    #D_optim.step()\n",
    "            \n",
    "    #x_syn = G(x)\n",
    "    #y_syn = D(x_syn)\n",
    "            \n",
    "    #loss = fn_loss(y_syn, torch.ones_like(y_syn).cuda())\n",
    "    #loss = myloss(y, x_syn)\n",
    "    #loss.backward()\n",
    "    #loss_G += loss.item()\n",
    "            \n",
    "    #G_optim.step()\n",
    "    if(train_counter %50 == 0):\n",
    "        print(\"Batch = \",train_counter,\" Train Loss = \", loss_G/train_counter, \" SSIM = \", ssim_batch/(train_counter))\n",
    "losses_D = loss_D / train_counter\n",
    "losses_G = loss_G / train_counter\n",
    "ssims_G = ssim_G / train_counter\n",
    "print(losses_G)\n",
    "print(ssims_G)\n",
    "print(psnr_G / (train_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b725e105",
   "metadata": {
    "id": "b725e105"
   },
   "source": [
    "# UCF101 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1049ec",
   "metadata": {
    "id": "8a1049ec"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "#import pylab as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "#from skimage.metrics import peak_signal_noise_ratio as PSNR\n",
    "from math import log10, sqrt\n",
    "def PSNR(original, compressed):\n",
    "    mse = np.mean((original - compressed) ** 2)\n",
    "    if(mse == 0):  # MSE is zero means no noise is present in the signal .\n",
    "                  # Therefore PSNR have no importance.\n",
    "        return 100\n",
    "    max_pixel = 255.0\n",
    "    psnr = 10 * log10(max_pixel / sqrt(mse))\n",
    "    return psnr\n",
    "def mse(imageA, imageB):\n",
    "    # the 'Mean Squared Error' between the two images is the sum of the squared difference between the two images\n",
    "    mse_error = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "    mse_error /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    return mse_error\n",
    "model_loaded = torch.load('/depot/bera89/data/hviswan/UNO_2f_vimeo_full60.pt')\n",
    "model_loaded.eval()\n",
    "x_train = torch.load('../Fall22 IDEAS/x_train_ucf.pt')\n",
    "y_train = torch.load('../Fall22 IDEAS/y_train_ucf.pt')\n",
    "x_test = torch.load('../Fall22 IDEAS/x_test_ucf.pt')\n",
    "y_test = torch.load('../Fall22 IDEAS/y_test_ucf.pt')\n",
    "x_train = torch.permute(x_train, (1,0,2,3))\n",
    "x_test = torch.permute(x_test, (1,0,2,3))\n",
    "y_normalizer = MinMaxNormalizer(y_train)\n",
    "x_normalizer = MinMaxNormalizer(x_train)\n",
    "x_train = x_normalizer.encode(x_train)\n",
    "y_train = y_normalizer.encode(y_train)\n",
    "y_normalizer = MinMaxNormalizer(y_test)\n",
    "x_normalizer = MinMaxNormalizer(x_test)\n",
    "x_test = x_normalizer.encode(x_test)\n",
    "y_test = y_normalizer.encode(y_test)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train[0], x_train[1], y_train), batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test[0], x_test[1], y_test), batch_size=10, shuffle=True)\n",
    "input_l = x_test[0][23].reshape(1, 100, 100).cpu().detach().numpy()\n",
    "input_r = x_test[1][23].reshape(1, 100, 100).cpu().detach().numpy()\n",
    "input = torch.from_numpy(np.asarray([input_l,input_r]).astype(np.float32))\n",
    "\n",
    "\n",
    "\n",
    "model_out = model_loaded(input.cuda())\n",
    "model_out = model_out.reshape(100,100).cpu().detach().numpy()\n",
    "output = y_test[23].reshape(100,100).cpu().detach().numpy()\n",
    "#model_out = y_normalizer.decode(model_out).cpu().detach().numpy()\n",
    "model_out = model_out.reshape(100,100)\n",
    "plt.imshow( model_out)\n",
    "plt.show()\n",
    "plt.imsave('/depot/bera89/data/hviswan/ucf_generated.png', model_out)\n",
    "plt.imshow( output)\n",
    "plt.show()\n",
    "plt.imsave('/depot/bera89/data/hviswan/ucf_groundTruth.png', output)\n",
    "#x_train = torch.permute(x_train, (0,2,3,1))\n",
    "#input = torch.permute(input, (0,3,1,2))\n",
    "#input1 = ((input[0][0]+input[0][1])/2.0).reshape(100,100).cpu().detach().numpy()\n",
    "input1 = ((input_l+ input_r)/2.0).reshape(100, 100)\n",
    "plt.imshow( input1)\n",
    "plt.show()\n",
    "plt.imsave('/depot/bera89/data/hviswan/ucf_mean.png', input1)\n",
    "plt.imsave('/depot/bera89/data/hviswan/ucf_left.png', input_l.reshape(100,100))\n",
    "plt.imsave('/depot/bera89/data/hviswan/ucf_right.png', input_r.reshape(100,100))\n",
    "print(ssim(model_out, output))\n",
    "print(ssim(input1, output))\n",
    "print(ssim(model_out, input1))\n",
    "print(mse(model_out, output))\n",
    "print(mse(input1, output))\n",
    "print(mse(model_out, input1))\n",
    "\n",
    "loss_D = 0.0\n",
    "loss_G = 0.0\n",
    "ssim_G = 0.0\n",
    "train_counter = 0\n",
    "ssim_batch=0\n",
    "psnr_G = 0.0\n",
    "for xl,xr,y in train_loader:\n",
    "    train_counter += 1\n",
    "    xl = xl.cpu().detach().numpy()\n",
    "    xr = xr.cpu().detach().numpy()\n",
    "    x = torch.from_numpy(np.asarray([xl,xr]).astype(np.float32))\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    #D_optimizer.zero_grad()\n",
    "    #D_optimizer.zero_grad()\n",
    "    #G_optim.zero_grad()\n",
    "    #print(y.shape)\n",
    "    #y_real = D(y)\n",
    "    #print(y_real.shape)\n",
    "    #loss_D_real = fn_loss(y_real, torch.ones_like(y_real).cuda())\n",
    "    #print(x.shape)\n",
    "    x_syn = model_loaded(x).reshape(x.shape[1], 100,100)\n",
    "    y = y.reshape(x.shape[1], 100, 100)\n",
    "    y2 = y_normalizer.encode(y)\n",
    "    x2 = y_normalizer.encode(x_syn)\n",
    "    #print(x_syn.shape)\n",
    "    #print(x.shape)\n",
    "    #print(y.shape)\n",
    "    #print(x_syn.shape)\n",
    "    #print(y.shape)\n",
    "    #print(x_syn.shape)\n",
    "    #print(y.shape)\n",
    "    #print(x.shape)\n",
    "    #print(\"___\")\n",
    "    #print(x_syn.shape)\n",
    "    #y_syn = D(x_syn)\n",
    "    #print(y_syn.shape)\n",
    "    #loss_D_syn = fn_loss(y_syn, torch.zeros_like(y_syn).cuda())\n",
    "    y = y.detach().cpu().numpy()\n",
    "    x_syn = x_syn.detach().cpu().numpy()\n",
    "    W_loss = mse(x_syn, y)\n",
    "    psnr_G += PSNR(x_syn, y)\n",
    "    if(x.shape[1]>=7):\n",
    "        ssim_batch += ssim(x_syn, y)\n",
    "        ssim_G = ssim_batch\n",
    "    #loss = loss_D_real + loss_D_syn\n",
    "    loss = W_loss\n",
    "    #print(loss.shape)\n",
    "    #loss.backward()\n",
    "    loss_G += loss\n",
    "    #loss_D += loss.item()\n",
    "            \n",
    "    #D_optim.step()\n",
    "            \n",
    "    #x_syn = G(x)\n",
    "    #y_syn = D(x_syn)\n",
    "            \n",
    "    #loss = fn_loss(y_syn, torch.ones_like(y_syn).cuda())\n",
    "    #loss = myloss(y, x_syn)\n",
    "    #loss.backward()\n",
    "    #loss_G += loss.item()\n",
    "            \n",
    "    #G_optim.step()\n",
    "    if(train_counter %50 == 0):\n",
    "        print(\"Batch = \",train_counter,\" Train Loss = \", loss_G/train_counter, \" SSIM = \", ssim_batch/(train_counter))\n",
    "losses_D = loss_D / train_counter\n",
    "losses_G = loss_G / train_counter\n",
    "ssims_G = ssim_G / train_counter\n",
    "print(losses_G)\n",
    "print(ssims_G)\n",
    "print(psnr_G / (train_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7988bbb",
   "metadata": {
    "id": "d7988bbb"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "model_loaded = torch.load('/depot/bera89/data/hviswan/UNO_2f_vimeo_full60.pt')\n",
    "model_loaded.eval()\n",
    "left = torch.load('../Fall22 IDEAS/left_tig.pt')\n",
    "right = torch.load('../Fall22 IDEAS/right_tig.pt')\n",
    "print(left.shape)\n",
    "#image_1_l = left[0].detach().cpu().numpy()\n",
    "#image_1_r = right[0].detach().cpu().numpy()\n",
    "\n",
    "image_1_l = torch.permute(left, (1,2,0)).detach().cpu().numpy()\n",
    "image_1_r = torch.permute(right, (1,2,0)).detach().cpu().numpy()\n",
    "\n",
    "slicer =  torch.from_numpy(np.asarray([image_1_l, image_1_l]).astype(np.float32)).reshape(2, 1, 700, 500, 1)\n",
    "print(\"HERE \", slicer.shape)\n",
    "x_normalizer = MinMaxNormalizer(slicer)\n",
    "#slicer = x_normalizer.encode(slicer)\n",
    "model_out = model_loaded(slicer.cuda()).reshape(700, 500).cpu().detach().numpy()\n",
    "plt.style.use('grayscale')\n",
    "plt.imshow(model_out)\n",
    "print(model_out.shape)\n",
    "plt.imsave('../Fall22 IDEAS/output1tig.png', model_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e32073",
   "metadata": {
    "id": "b2e32073"
   },
   "source": [
    "# DAVIS RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4847aa",
   "metadata": {
    "id": "1f4847aa"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def mse(imageA, imageB):\n",
    "    # the 'Mean Squared Error' between the two images is the sum of the squared difference between the two images\n",
    "    mse_error = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "    mse_error /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    return mse_error\n",
    "x_train = torch.load('../Fall22 IDEAS/x_train_davis_rgb.pt')\n",
    "y_train = torch.load('../Fall22 IDEAS/y_train_davis_rgb.pt')\n",
    "x_test = torch.load('../Fall22 IDEAS/x_test_davis_rgb.pt')\n",
    "y_test = torch.load('../Fall22 IDEAS/y_test_davis_rgb.pt')\n",
    "x_train = torch.permute(x_train, (1, 0,2,3,4))\n",
    "x_test = torch.permute(x_test, (1, 0,2,3,4))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(x_train[0][0].shape)\n",
    "ntrain = x_train.shape[1]\n",
    "ntest = x_test.shape[1]\n",
    "y_train = y_train.reshape(ntrain, 100, 100,3)\n",
    "y_test = y_test.reshape(ntest, 100, 100,3)\n",
    "dim = (100, 100)\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train[0], x_train[1], y_train), batch_size=1, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test[0], x_test[1], y_test), batch_size=1, shuffle=True)\n",
    "loss_D = 0.0\n",
    "loss_G = 0.0\n",
    "ssim_G = 0.0\n",
    "train_counter = 0\n",
    "ssim_batch=0\n",
    "psnr_G = 0.0\n",
    "model_loaded = torch.load('/depot/bera89/data/hviswan/UNO_2f_davis_rgb5.pt')\n",
    "model_loaded.eval()\n",
    "input_l = x_train[0][12].reshape(1, 100, 100,3).cpu().detach().numpy()\n",
    "input_r = x_train[1][12].reshape(1, 100, 100,3).cpu().detach().numpy()\n",
    "output = y_train[12].reshape(100,100,3).cpu().detach().numpy()\n",
    "\n",
    "input = torch.from_numpy(np.asarray([input_l,input_r]).astype(np.float32))\n",
    "model_out = G(input.cuda()).reshape(100,100,3).cpu().detach().numpy()\n",
    "print(\"OUTSHAPE = \", model_out.shape)\n",
    "plt.imshow((model_out * 255).astype(np.uint8))\n",
    "plt.show()\n",
    "plt.imshow((output * 255).astype(np.uint8))\n",
    "plt.show()\n",
    "\n",
    "for xl,xr,y in train_loader:\n",
    "    train_counter += 1\n",
    "    xl = xl.cpu().detach().numpy()\n",
    "    xr = xr.cpu().detach().numpy()\n",
    "    x = torch.from_numpy(np.asarray([xl,xr]).astype(np.float32))\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    x_syn = G(x).reshape(100,100,3).detach().cpu().numpy()\n",
    "    y = y.reshape(100,100,3).detach().cpu().numpy()\n",
    "    W_loss = mse(x_syn, y)\n",
    "    ssim_batch += ssim(x_syn, y, multichannel=True)\n",
    "    ssim_G = ssim_batch\n",
    "    loss = W_loss\n",
    "    loss_G += loss\n",
    "    if(train_counter %50 == 0 or train_counter == 1):\n",
    "        print(\"Batch = \",train_counter,\" Train Loss = \", loss_G/train_counter, \" SSIM = \", ssim_batch/(train_counter))\n",
    "print(loss_G/train_counter)\n",
    "print(ssim_batch/train_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6779cbdd",
   "metadata": {
    "id": "6779cbdd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (My IDEAS_hviswan Kernel)",
   "language": "python",
   "name": "ideas_hviswan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
