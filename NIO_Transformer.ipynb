{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "575ff432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/depot/bera89/apps/IDEAS_hviswan/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import logging\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "from copy import Error, deepcopy\n",
    "from re import S\n",
    "from numpy.lib.arraypad import pad\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "import torch.fft\n",
    "from torch.nn.modules.container import Sequential\n",
    "#from main_afnonet import get_args\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04d1e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(in_chan=2, up_dim=32):\n",
    "    \"\"\"\n",
    "        Kernel network apply on grid\n",
    "    \"\"\"\n",
    "    layers = nn.Sequential(\n",
    "                nn.Linear(in_chan, up_dim, bias=True), torch.nn.GELU(),\n",
    "                nn.Linear(up_dim, up_dim, bias=True), torch.nn.GELU(),\n",
    "                nn.Linear(up_dim, 1, bias=False)\n",
    "            )\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "121d604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dim1, dim2,modes1 = None, modes2 = None):\n",
    "        super(SpectralConv2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "        in_channels = int(in_channels)\n",
    "        out_channels = int(out_channels)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.dim1 = dim1 #output dimensions\n",
    "        self.dim2 = dim2\n",
    "        #self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "        #self.dropout = nn.Dropout(p=0.42)\n",
    "        if modes1 is not None:\n",
    "            self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "            self.modes2 = modes2\n",
    "        else:\n",
    "            self.modes1 = dim1//2 + 1 #if not given take the highest number of modes can be taken\n",
    "            self.modes2 = dim2//2 \n",
    "        self.scale = (1 / (2*in_channels))**(1.0/2.0)\n",
    "        self.weights1 = nn.Parameter(self.scale * (torch.randn(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat)))\n",
    "        self.weights2 = nn.Parameter(self.scale * (torch.randn(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat)))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul2d(self, input, weights):\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "\n",
    "    def forward(self, x, dim1 = None,dim2 = None):\n",
    "        if dim1 is not None:\n",
    "            self.dim1 = dim1\n",
    "            self.dim2 = dim2\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfft2(x)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels,  self.dim1, self.dim2//2 + 1 , dtype=torch.cfloat, device=x.device)\n",
    "        #print(\"Out FT Shape = \", out_ft.shape)\n",
    "        #print(\"x_ft Shape = \", x_ft.shape)\n",
    "        #print(\"Modes1 = \", self.modes1, \" Modes2 = \", self.modes2, \" self.dim1 = \", self.dim1, \" self.dim2//2 + 1 = \", self.dim2//2+1)\n",
    "        out_ft[:, :, :self.modes1, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfft2(out_ft, s=(self.dim1, self.dim2))\n",
    "        #x = self.dropout(x)\n",
    "        #x = self.batch_norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class pointwise_op(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel,dim1, dim2):\n",
    "        super(pointwise_op,self).__init__()\n",
    "        self.conv = nn.Conv2d(int(in_channel), int(out_channel), 1)\n",
    "        self.dim1 = int(dim1)\n",
    "        self.dim2 = int(dim2)\n",
    "        #self.alpha_drop = nn.AlphaDropout(p=0.42)\n",
    "        #self.layer_norm = nn.LayerNorm([int(out_channel), self.dim1, self.dim2])\n",
    "        #self.batch_norm = nn.BatchNorm2d(int(out_channel))\n",
    "        #self.upsample = nn.Upsample(size=[self.dim1, self.dim2], mode='bicubic', align_corners=True)\n",
    "\n",
    "    def forward(self,x, dim1 = None, dim2 = None):\n",
    "        if dim1 is None:\n",
    "            dim1 = self.dim1\n",
    "            dim2 = self.dim2\n",
    "        x_out = self.conv(x)\n",
    "        x_out = torch.nn.functional.interpolate(x_out, size = (dim1, dim2),mode = 'bicubic',align_corners=True)\n",
    "        #x_out = self.upsample(x_out)\n",
    "        #x_out = self.alpha_drop(x_out)\n",
    "        #x_out = self.layer_norm(x_out)\n",
    "        #x_out = self.batch_norm(x_out)\n",
    "        return x_out\n",
    "\n",
    "class UNO_vanilla(nn.Module):\n",
    "    def __init__(self, in_d_co_domain, d_co_domain, pad = 0, factor = 16/4):\n",
    "        super(UNO_vanilla, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the coefficient function and locations (a(x, y), x, y)\n",
    "        input shape: (batchsize, x=s, y=s, c=3)\n",
    "        output: the solution \n",
    "        output shape: (batchsize, x=s, y=s, c=1)\n",
    "        \"\"\"\n",
    "        self.in_d_co_domain = in_d_co_domain # input channel\n",
    "        self.d_co_domain = d_co_domain \n",
    "        self.factor = factor\n",
    "        self.factor2 = factor/4\n",
    "        self.padding = pad  # pad the domain if input is non-periodic\n",
    "\n",
    "        self.fc0 = nn.Linear(self.in_d_co_domain, self.d_co_domain) # input channel is 3: (a(x, y), x, y)\n",
    "\n",
    "        self.conv0 = SpectralConv2d(self.d_co_domain, 4*factor*self.d_co_domain, 16, 16, 42, 42)\n",
    "\n",
    "        self.conv1 = SpectralConv2d(4*factor*self.d_co_domain, 8*factor*self.d_co_domain, 16, 16, 21,21)\n",
    "\n",
    "        self.conv2 = SpectralConv2d(8*factor*self.d_co_domain, 16*factor*self.d_co_domain, 8, 8,10,10)\n",
    "        \n",
    "        self.conv2_1 = SpectralConv2d(16*factor*self.d_co_domain, 32*factor*self.d_co_domain, 4, 4,5,5)\n",
    "        \n",
    "        self.conv2_9 = SpectralConv2d(32*factor*self.d_co_domain, 16*factor*self.d_co_domain, 8, 8,5,5)\n",
    "    \n",
    "\n",
    "        self.conv3 = SpectralConv2d(32*factor*self.d_co_domain, 8*factor*self.d_co_domain, 16, 16,10,10)\n",
    "\n",
    "        self.conv4 = SpectralConv2d(16*factor*self.d_co_domain, 4*factor*self.d_co_domain, 32, 32,21,21)\n",
    "\n",
    "        self.conv5 = SpectralConv2d(8*factor*self.d_co_domain, self.d_co_domain, 48, 48,42,42) # will be reshaped\n",
    "\n",
    "        self.w0 = pointwise_op(self.d_co_domain,4*factor*self.d_co_domain,75, 75) #\n",
    "        \n",
    "        self.w1 = pointwise_op(4*factor*self.d_co_domain, 8*factor*self.d_co_domain, 50, 50) #\n",
    "        \n",
    "        self.w2 = pointwise_op(8*factor*self.d_co_domain, 16*factor*self.d_co_domain, 25, 25) #\n",
    "        \n",
    "        self.w2_1 = pointwise_op(16*factor*self.d_co_domain, 32*factor*self.d_co_domain, 12, 12)\n",
    "\n",
    "        self.w2_9 = pointwise_op(32*factor*self.d_co_domain, 16*factor*self.d_co_domain, 25, 25)\n",
    "        \n",
    "        self.w3 = pointwise_op(32*factor*self.d_co_domain, 8*factor*self.d_co_domain, 50, 50) #\n",
    "        \n",
    "        self.w4 = pointwise_op(16*factor*self.d_co_domain, 4*factor*self.d_co_domain, 75, 75)\n",
    "        \n",
    "        self.w5 = pointwise_op(8*factor*self.d_co_domain, self.d_co_domain, 100, 100) # will be reshaped\n",
    "\n",
    "        self.fc1 = nn.Linear(2*self.d_co_domain, 4*self.d_co_domain)\n",
    "        self.fc2 = nn.Linear(4*self.d_co_domain, self.d_co_domain)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        grid = self.get_grid(x[0].shape, x.device)\n",
    "        x = x.permute(0,2,3,1)\n",
    "\n",
    "        x_fc0 = self.fc0(x)\n",
    "\n",
    "        x_fc0 = F.gelu(x_fc0)\n",
    "        \n",
    "        x_fc0 = x_fc0.permute(0, 3, 1, 2)\n",
    "        \n",
    "        \n",
    "        x_fc0 = F.pad(x_fc0, [0,self.padding, 0,self.padding])\n",
    "        \n",
    "        D1,D2 = x_fc0.shape[-2],x_fc0.shape[-1]\n",
    "        \n",
    "        x1_c0 = self.conv0(x_fc0,int(D1*self.factor2),int(D2*self.factor2))\n",
    "        x2_c0 = self.w0(x_fc0,int(D1*self.factor2),int(D2*self.factor2))\n",
    "        x_c0 = x1_c0 + x2_c0\n",
    "        x_c0 = F.gelu(x_c0)\n",
    "\n",
    "        x1_c1 = self.conv1(x_c0 ,D1//2,D2//2)\n",
    "        x2_c1 = self.w1(x_c0 ,D1//2,D2//2)\n",
    "        x_c1 = x1_c1 + x2_c1\n",
    "        x_c1 = F.gelu(x_c1)\n",
    "\n",
    "        x1_c2 = self.conv2(x_c1 ,D1//4,D2//4)\n",
    "        x2_c2 = self.w2(x_c1 ,D1//4,D2//4)\n",
    "        x_c2 = x1_c2 + x2_c2\n",
    "        x_c2 = F.gelu(x_c2 )\n",
    "\n",
    "        x1_c2_1 = self.conv2_1(x_c2,D1//8,D2//8)\n",
    "        x2_c2_1 = self.w2_1(x_c2,D1//8,D2//8)\n",
    "        x_c2_1 = x1_c2_1 + x2_c2_1\n",
    "        x_c2_1 = F.gelu(x_c2_1)\n",
    "\n",
    "        \n",
    "        x1_c2_9 = self.conv2_9(x_c2_1,D1//4,D2//4)\n",
    "        x2_c2_9 = self.w2_9(x_c2_1,D1//4,D2//4)\n",
    "        x_c2_9 = x1_c2_9 + x2_c2_9\n",
    "        x_c2_9 = F.gelu(x_c2_9)\n",
    "        x_c2_9 = torch.cat([x_c2_9, x_c2], dim=1)\n",
    "\n",
    "        x1_c3 = self.conv3(x_c2_9,D1//2,D2//2)\n",
    "        x2_c3 = self.w3(x_c2_9,D1//2,D2//2)\n",
    "        x_c3 = x1_c3 + x2_c3\n",
    "        x_c3 = F.gelu(x_c3)\n",
    "        x_c3 = torch.cat([x_c3, x_c1], dim=1)\n",
    "\n",
    "        x1_c4 = self.conv4(x_c3,int(D1*self.factor2),int(D2*self.factor2))\n",
    "        x2_c4 = self.w4(x_c3,int(D1*self.factor2),int(D2*self.factor2))\n",
    "        x_c4 = x1_c4 + x2_c4\n",
    "        x_c4 = F.gelu(x_c4)\n",
    "        x_c4 = torch.cat([x_c4, x_c0], dim=1)\n",
    "\n",
    "        x1_c5 = self.conv5(x_c4,D1,D2)\n",
    "        x2_c5 = self.w5(x_c4,D1,D2)\n",
    "        x_c5 = x1_c5 + x2_c5\n",
    "        x_c5 = F.gelu(x_c5)\n",
    "\n",
    "\n",
    "        x_c5 = torch.cat([x_c5, x_fc0], dim=1)\n",
    "        if self.padding!=0:\n",
    "            x_c5 = x_c5[..., :-self.padding, :-self.padding]\n",
    "\n",
    "        x_c5 = x_c5.permute(0, 2, 3, 1)\n",
    "        \n",
    "        x_fc1 = self.fc1(x_c5)\n",
    "        x_fc1 = F.gelu(x_fc1)\n",
    "        \n",
    "        x_out = self.fc2(x_fc1)\n",
    "        x_out = x_out.permute(0,3,1,2)\n",
    "        x_out = F.gelu(x_out)\n",
    "        return x_out\n",
    "    \n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d02df15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNO(nn.Module):\n",
    "    def __init__(self, in_d_co_domain, d_co_domain, pad = 0, factor = 16/4):\n",
    "        super(UNO, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the coefficient function and locations (a(x, y), x, y)\n",
    "        input shape: (batchsize, x=s, y=s, c=3)\n",
    "        output: the solution \n",
    "        output shape: (batchsize, x=s, y=s, c=1)\n",
    "        \"\"\"\n",
    "        self.in_d_co_domain = in_d_co_domain # input channel\n",
    "        self.d_co_domain = d_co_domain \n",
    "        self.factor = factor\n",
    "        self.factor2 = factor/4\n",
    "        self.padding = pad  # pad the domain if input is non-periodic\n",
    "        self.reduceColor = nn.Conv2d(3, 1, kernel_size=1)\n",
    "        self.interpolation = nn.Conv2d(3, 3, kernel_size=1)\n",
    "\n",
    "        self.fc0 = nn.Linear(self.in_d_co_domain, self.d_co_domain) # input channel is 3: (a(x, y), x, y)\n",
    "\n",
    "        self.conv0 = SpectralConv2d(self.d_co_domain, 4*factor*self.d_co_domain, 16, 16, 42, 42)\n",
    "\n",
    "        self.conv1 = SpectralConv2d(4*factor*self.d_co_domain, 8*factor*self.d_co_domain, 16, 16, 21,21)\n",
    "\n",
    "        self.conv2 = SpectralConv2d(8*factor*self.d_co_domain, 16*factor*self.d_co_domain, 8, 8,10,10)\n",
    "        \n",
    "        self.conv2_1 = SpectralConv2d(16*factor*self.d_co_domain, 32*factor*self.d_co_domain, 4, 4,5,5)\n",
    "        \n",
    "        self.conv2_9 = SpectralConv2d(32*factor*self.d_co_domain, 16*factor*self.d_co_domain, 8, 8,5,5)\n",
    "    \n",
    "\n",
    "        self.conv3 = SpectralConv2d(32*factor*self.d_co_domain, 8*factor*self.d_co_domain, 16, 16,10,10)\n",
    "\n",
    "        self.conv4 = SpectralConv2d(16*factor*self.d_co_domain, 4*factor*self.d_co_domain, 32, 32,21,21)\n",
    "\n",
    "        self.conv5 = SpectralConv2d(8*factor*self.d_co_domain, self.d_co_domain, 48, 48,42,42) # will be reshaped\n",
    "\n",
    "        self.w0 = pointwise_op(self.d_co_domain,4*factor*self.d_co_domain,75, 75) #\n",
    "        \n",
    "        self.w1 = pointwise_op(4*factor*self.d_co_domain, 8*factor*self.d_co_domain, 50, 50) #\n",
    "        \n",
    "        self.w2 = pointwise_op(8*factor*self.d_co_domain, 16*factor*self.d_co_domain, 25, 25) #\n",
    "        \n",
    "        self.w2_1 = pointwise_op(16*factor*self.d_co_domain, 32*factor*self.d_co_domain, 12, 12)\n",
    "        \"\"\"\n",
    "        #VAE Code begin\n",
    "        self.linearFlatten1 = nn.Linear(384*12*12, 128)\n",
    "        self.linearFlatten2 = nn.Linear(128, 4)\n",
    "        self.linearFlatten3 = nn.Linear(128, 4)\n",
    "        self.softmax1 = nn.Softmax(dim=1)\n",
    "        self.N = torch.distributions.Normal(0, 10)\n",
    "        self.N.loc = self.N.loc.cuda()\n",
    "        self.N.scale = self.N.scale.cuda()\n",
    "        self.kl = 0\n",
    "        self.linearUnflatten = nn.Linear(4, 128)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.linearUnflatten2 = nn.Linear(128, 384*12*12)\n",
    "        self.relu2 = nn.ReLU(True)\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(384, 12, 12))\n",
    "        #VAE Code End\n",
    "        \"\"\"\n",
    "        self.w2_9 = pointwise_op(32*factor*self.d_co_domain, 16*factor*self.d_co_domain, 25, 25)\n",
    "        \n",
    "        self.w3 = pointwise_op(32*factor*self.d_co_domain, 8*factor*self.d_co_domain, 50, 50) #\n",
    "        \n",
    "        self.w4 = pointwise_op(16*factor*self.d_co_domain, 4*factor*self.d_co_domain, 75, 75)\n",
    "        \n",
    "        self.w5 = pointwise_op(8*factor*self.d_co_domain, self.d_co_domain, 100, 100) # will be reshaped\n",
    "\n",
    "        self.fc1 = nn.Linear(2*self.d_co_domain, 4*self.d_co_domain)\n",
    "        self.fc2 = nn.Linear(4*self.d_co_domain, 3)\n",
    "        self.increaseColor = nn.Conv2d(1, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        grid = self.get_grid(x[0].shape, x.device)\n",
    "        #print(grid.shape)\n",
    "        \n",
    "        #print(x[0].shape)\n",
    "        #x_l = torch.cat((x[0], grid), dim=-1).cuda()\n",
    "        #x_r = torch.cat((x[1], grid), dim=-1).cuda()\n",
    "\n",
    "        x_l = x[0].permute(0,3,1,2)\n",
    "        x_r = x[1].permute(0,3,1,2)\n",
    "        #print(x_l.shape)\n",
    "        x1 = self.interpolation(x_l)\n",
    "        x2 = self.interpolation(x_r)\n",
    "        ##print(x1.shape)\n",
    "        x = x1 + x2\n",
    "        x3 = x\n",
    "        #x = self.reduceColor(x)\n",
    "        x = x.permute(0,2,3,1)\n",
    "        #x = torch.cat((x, grid), dim=-1).cuda()\n",
    "        #print(x.shape)\n",
    "        x_fc0 = self.fc0(x)\n",
    "\n",
    "        x_fc0 = F.gelu(x_fc0)\n",
    "        \n",
    "        x_fc0 = x_fc0.permute(0, 3, 1, 2)\n",
    "        \n",
    "        \n",
    "        x_fc0 = F.pad(x_fc0, [0,self.padding, 0,self.padding])\n",
    "        \n",
    "        D1,D2 = x_fc0.shape[-2],x_fc0.shape[-1]\n",
    "        \n",
    "        x1_c0 = self.conv0(x_fc0,int(D1*self.factor2),int(D2*self.factor2))\n",
    "        x2_c0 = self.w0(x_fc0,int(D1*self.factor2),int(D2*self.factor2))\n",
    "        x_c0 = x1_c0 + x2_c0\n",
    "        x_c0 = F.gelu(x_c0)\n",
    "\n",
    "        x1_c1 = self.conv1(x_c0 ,D1//2,D2//2)\n",
    "        x2_c1 = self.w1(x_c0 ,D1//2,D2//2)\n",
    "        x_c1 = x1_c1 + x2_c1\n",
    "        x_c1 = F.gelu(x_c1)\n",
    "\n",
    "        x1_c2 = self.conv2(x_c1 ,D1//4,D2//4)\n",
    "        x2_c2 = self.w2(x_c1 ,D1//4,D2//4)\n",
    "        x_c2 = x1_c2 + x2_c2\n",
    "        x_c2 = F.gelu(x_c2 )\n",
    "\n",
    "        x1_c2_1 = self.conv2_1(x_c2,D1//8,D2//8)\n",
    "        x2_c2_1 = self.w2_1(x_c2,D1//8,D2//8)\n",
    "        x_c2_1 = x1_c2_1 + x2_c2_1\n",
    "        x_c2_1 = F.gelu(x_c2_1)\n",
    "        \"\"\"\n",
    "        #Variational Autoencoder part - Extract mu and sigma\n",
    "        x_c2_1 = torch.flatten(x_c2_1, start_dim=1)\n",
    "        x_c2_1 = self.linearFlatten1(x_c2_1)\n",
    "        mu = self.linearFlatten2(x_c2_1)\n",
    "        sigma = self.linearFlatten3(x_c2_1)\n",
    "        std = torch.exp(0.5*sigma)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = sigma*self.N.sample(mu.shape) + mu\n",
    "        #z = mu + std*eps\n",
    "\n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n",
    "        z = self.linearUnflatten(z)\n",
    "        z = self.linearUnflatten2(z)\n",
    "        x_c2_1 = self.unflatten(z)\n",
    "        #print(\"x_c2_1 shape = \", x_c2_1.shape)\n",
    "        #Variational Autoencoder -end\n",
    "        \"\"\"\n",
    "        \n",
    "        x1_c2_9 = self.conv2_9(x_c2_1,D1//4,D2//4)\n",
    "        x2_c2_9 = self.w2_9(x_c2_1,D1//4,D2//4)\n",
    "        x_c2_9 = x1_c2_9 + x2_c2_9\n",
    "        x_c2_9 = F.gelu(x_c2_9)\n",
    "        x_c2_9 = torch.cat([x_c2_9, x_c2], dim=1)\n",
    "\n",
    "        x1_c3 = self.conv3(x_c2_9,D1//2,D2//2)\n",
    "        x2_c3 = self.w3(x_c2_9,D1//2,D2//2)\n",
    "        x_c3 = x1_c3 + x2_c3\n",
    "        x_c3 = F.gelu(x_c3)\n",
    "        x_c3 = torch.cat([x_c3, x_c1], dim=1)\n",
    "\n",
    "        x1_c4 = self.conv4(x_c3,int(D1*self.factor2),int(D2*self.factor2))\n",
    "        x2_c4 = self.w4(x_c3,int(D1*self.factor2),int(D2*self.factor2))\n",
    "        x_c4 = x1_c4 + x2_c4\n",
    "        x_c4 = F.gelu(x_c4)\n",
    "        x_c4 = torch.cat([x_c4, x_c0], dim=1)\n",
    "\n",
    "        x1_c5 = self.conv5(x_c4,D1,D2)\n",
    "        x2_c5 = self.w5(x_c4,D1,D2)\n",
    "        x_c5 = x1_c5 + x2_c5\n",
    "        x_c5 = F.gelu(x_c5)\n",
    "\n",
    "\n",
    "        x_c5 = torch.cat([x_c5, x_fc0], dim=1)\n",
    "        if self.padding!=0:\n",
    "            x_c5 = x_c5[..., :-self.padding, :-self.padding]\n",
    "\n",
    "        x_c5 = x_c5.permute(0, 2, 3, 1)\n",
    "        \n",
    "        x_fc1 = self.fc1(x_c5)\n",
    "        x_fc1 = F.gelu(x_fc1)\n",
    "        \n",
    "        x_out = self.fc2(x_fc1)\n",
    "        \n",
    "        #x3 = torch.permute(x3, (0, 2,3,1))\n",
    "        x3 = (x_l+x_r)/2\n",
    "        #print(x3.shape)\n",
    "        x3 = torch.permute(x3, (0,2,3,1))\n",
    "        #x_fin = x_out + x3\n",
    "        #x_fin = torch.permute(x_fin, (0, 3, 1, 2))\n",
    "        #x_out = self.interpolation(x_fin)\n",
    "        #x_out = torch.permute(x_out, (0, 3, 1, 2))\n",
    "        #print(x_out.shape)\n",
    "        #x_out = self.increaseColor(x_out)\n",
    "        return x_out\n",
    "    \n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8950192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "UNO_vanilla                              [10, 1, 100, 100]         --\n",
       "├─Linear: 1-1                            [10, 100, 100, 1]         3\n",
       "├─SpectralConv2d: 1-2                    [10, 16, 100, 100]        56,448\n",
       "├─pointwise_op: 1-3                      [10, 16, 100, 100]        --\n",
       "│    └─Conv2d: 2-1                       [10, 16, 100, 100]        32\n",
       "├─SpectralConv2d: 1-4                    [10, 32, 50, 50]          451,584\n",
       "├─pointwise_op: 1-5                      [10, 32, 50, 50]          --\n",
       "│    └─Conv2d: 2-2                       [10, 32, 100, 100]        544\n",
       "├─SpectralConv2d: 1-6                    [10, 64, 25, 25]          409,600\n",
       "├─pointwise_op: 1-7                      [10, 64, 25, 25]          --\n",
       "│    └─Conv2d: 2-3                       [10, 64, 50, 50]          2,112\n",
       "├─SpectralConv2d: 1-8                    [10, 128, 12, 12]         409,600\n",
       "├─pointwise_op: 1-9                      [10, 128, 12, 12]         --\n",
       "│    └─Conv2d: 2-4                       [10, 128, 25, 25]         8,320\n",
       "├─SpectralConv2d: 1-10                   [10, 64, 25, 25]          409,600\n",
       "├─pointwise_op: 1-11                     [10, 64, 25, 25]          --\n",
       "│    └─Conv2d: 2-5                       [10, 64, 12, 12]          8,256\n",
       "├─SpectralConv2d: 1-12                   [10, 32, 50, 50]          819,200\n",
       "├─pointwise_op: 1-13                     [10, 32, 50, 50]          --\n",
       "│    └─Conv2d: 2-6                       [10, 32, 25, 25]          4,128\n",
       "├─SpectralConv2d: 1-14                   [10, 16, 100, 100]        903,168\n",
       "├─pointwise_op: 1-15                     [10, 16, 100, 100]        --\n",
       "│    └─Conv2d: 2-7                       [10, 16, 50, 50]          1,040\n",
       "├─SpectralConv2d: 1-16                   [10, 1, 100, 100]         112,896\n",
       "├─pointwise_op: 1-17                     [10, 1, 100, 100]         --\n",
       "│    └─Conv2d: 2-8                       [10, 1, 100, 100]         33\n",
       "├─Linear: 1-18                           [10, 100, 100, 4]         12\n",
       "├─Linear: 1-19                           [10, 100, 100, 1]         5\n",
       "==========================================================================================\n",
       "Total params: 3,596,581\n",
       "Trainable params: 3,596,581\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.84\n",
       "==========================================================================================\n",
       "Input size (MB): 0.80\n",
       "Forward/backward pass size (MB): 115.81\n",
       "Params size (MB): 28.67\n",
       "Estimated Total Size (MB): 145.29\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNONet = UNO_vanilla(2,1)\n",
    "summary(UNONet, input_size=(10, 2, 100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba91e21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NIO(nn.Module):\n",
    "    def __init__(self, in_d_co_domain, d_co_domain, pad = 0, factor = 10/4):\n",
    "        super(NIO, self).__init__()\n",
    "        self.in_d_co_domain = in_d_co_domain # input channel\n",
    "        self.d_co_domain = d_co_domain \n",
    "        self.factor = factor\n",
    "        self.factor2 = factor/4\n",
    "        self.interpolation = nn.Conv2d(3, 3, kernel_size=1)\n",
    "        self.UNO_block_1 = UNO(self.in_d_co_domain, d_co_domain, pad=0, factor=16/4)\n",
    "        self.UNO_block_2 = UNO(self.in_d_co_domain, d_co_domain, pad=0, factor=16/4)\n",
    "        self.UNO_block_3 = UNO(self.in_d_co_domain, d_co_domain, pad=0, factor=16/4)\n",
    "        self.UNO_block_4 = UNO(self.in_d_co_domain, d_co_domain, pad=0, factor=16/4)\n",
    "        #self.UNO_block_5 = UNO(self.in_d_co_domain, d_co_domain, pad=0, factor=20/4)\n",
    "    def forward(self, x):\n",
    "        uno_1_out = self.UNO_block_1(x)\n",
    "        #print(uno_1_out.shape)\n",
    "        #print(x[0].shape)\n",
    "        #print(x.shape)\n",
    "        x_int_l = self.interpolation(x[0].permute(0,3,1,2))\n",
    "        x_int_r = self.interpolation(x[1].permute(0,3,1,2))\n",
    "        x_int = x_int_l + x_int_r\n",
    "        x_int = torch.permute(x_int, (0,2,3,1))\n",
    "        y = torch.stack((x_int, uno_1_out))\n",
    "        \n",
    "        #y = x[0] + uno_1_out\n",
    "        #print(\"Y Shape = \", y.shape)\n",
    "        uno_2_out = self.UNO_block_2(y)\n",
    "        y = torch.stack((x_int, uno_2_out))\n",
    "        #y = x[1] + uno_2_out\n",
    "        uno_3_out = self.UNO_block_3(y)\n",
    "        y = torch.stack((x_int, uno_3_out))\n",
    "        uno_4_out = self.UNO_block_4(x)\n",
    "        #y = torch.stack((x_int, uno_4_out))\n",
    "        #x = self.UNO_block_5(y)\n",
    "        return uno_4_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b41a694",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodicPad2d(nn.Module):\n",
    "    \"\"\" \n",
    "        pad longitudinal (left-right) circular \n",
    "        and pad latitude (top-bottom) with zeros\n",
    "    \"\"\"\n",
    "    def __init__(self, pad_width):\n",
    "       super(PeriodicPad2d, self).__init__()\n",
    "       self.pad_width = pad_width\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pad left and right circular\n",
    "        out = F.pad(x, (self.pad_width, self.pad_width, 0, 0), mode=\"circular\") \n",
    "        # pad top and bottom zeros\n",
    "        out = F.pad(out, (0, 0, self.pad_width, self.pad_width), mode=\"constant\", value=0) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca1ad167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxNormalizer:\n",
    "    def __init__(self, batch):\n",
    "        self.batch = batch\n",
    "        self.min = batch.min()\n",
    "        self.max = batch.max()\n",
    "    def encode(self, batch): # min max normalization -> [0,1]\n",
    "        encoded = (batch - self.min) / (self.max - self.min)\n",
    "        return encoded \n",
    "    def decode(self, batch): # returns to standard values -> [a,b]\n",
    "        decoded = batch * (self.max - self.min) + self.min\n",
    "        return decoded \n",
    "    def cuda(self):\n",
    "        self.min = self.min.cuda()\n",
    "        self.max = self.max.cuda()\n",
    "    def cpu(self):\n",
    "        self.min = self.min.cpu()\n",
    "        self.max = self.max.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59734267",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73f934dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFNO2D(nn.Module):\n",
    "    def __init__(self, hidden_size, num_blocks=8, sparsity_threshold=0.01, hard_thresholding_fraction=1, hidden_size_factor=1):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_blocks == 0, f\"hidden_size {hidden_size} should be divisble by num_blocks {num_blocks}\"\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sparsity_threshold = sparsity_threshold\n",
    "        self.num_blocks = num_blocks\n",
    "        self.block_size = self.hidden_size // self.num_blocks\n",
    "        self.hard_thresholding_fraction = hard_thresholding_fraction\n",
    "        self.hidden_size_factor = hidden_size_factor\n",
    "        self.scale = 0.02\n",
    "\n",
    "        self.w1 = nn.Parameter(self.scale * torch.randn(2, self.num_blocks, self.block_size, self.block_size * self.hidden_size_factor))\n",
    "        self.b1 = nn.Parameter(self.scale * torch.randn(2, self.num_blocks, self.block_size * self.hidden_size_factor))\n",
    "        self.w2 = nn.Parameter(self.scale * torch.randn(2, self.num_blocks, self.block_size * self.hidden_size_factor, self.block_size))\n",
    "        self.b2 = nn.Parameter(self.scale * torch.randn(2, self.num_blocks, self.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        bias = x\n",
    "\n",
    "        dtype = x.dtype\n",
    "        x = x.float()\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        x = torch.fft.rfft2(x, dim=(1, 2), norm=\"ortho\")\n",
    "        x = x.reshape(B, H, W // 2 + 1, self.num_blocks, self.block_size)\n",
    "\n",
    "        o1_real = torch.zeros([B, H, W // 2 + 1, self.num_blocks, self.block_size * self.hidden_size_factor], device=x.device)\n",
    "        o1_imag = torch.zeros([B, H, W // 2 + 1, self.num_blocks, self.block_size * self.hidden_size_factor], device=x.device)\n",
    "        o2_real = torch.zeros(x.shape, device=x.device)\n",
    "        o2_imag = torch.zeros(x.shape, device=x.device)\n",
    "\n",
    "\n",
    "        total_modes = H // 2 + 1\n",
    "        kept_modes = int(total_modes * self.hard_thresholding_fraction)\n",
    "\n",
    "        o1_real[:, total_modes-kept_modes:total_modes+kept_modes, :kept_modes] = F.relu(\n",
    "            torch.einsum('...bi,bio->...bo', x[:, total_modes-kept_modes:total_modes+kept_modes, :kept_modes].real, self.w1[0]) - \\\n",
    "            torch.einsum('...bi,bio->...bo', x[:, total_modes-kept_modes:total_modes+kept_modes, :kept_modes].imag, self.w1[1]) + \\\n",
    "            self.b1[0]\n",
    "        )\n",
    "\n",
    "        o1_imag[:, total_modes-kept_modes:total_modes+kept_modes, :kept_modes] = F.relu(\n",
    "            torch.einsum('...bi,bio->...bo', x[:, total_modes-kept_modes:total_modes+kept_modes, :kept_modes].imag, self.w1[0]) + \\\n",
    "            torch.einsum('...bi,bio->...bo', x[:, total_modes-kept_modes:total_modes+kept_modes, :kept_modes].real, self.w1[1]) + \\\n",
    "            self.b1[1]\n",
    "        )\n",
    "\n",
    "        o2_real[:, total_modes-kept_modes:total_modes+kept_modes, :kept_modes]  = (\n",
    "            torch.einsum('...bi,bio->...bo', o1_real[:, total_modes-kept_modes:total_modes+kept_modes, :kept_modes], self.w2[0]) - \\\n",
    "            torch.einsum('...bi,bio->...bo', o1_imag[:, total_modes-kept_modes:total_modes+kept_modes, :kept_modes], self.w2[1]) + \\\n",
    "            self.b2[0]\n",
    "        )\n",
    "\n",
    "        o2_imag[:, total_modes-kept_modes:total_modes+kept_modes, :kept_modes]  = (\n",
    "            torch.einsum('...bi,bio->...bo', o1_imag[:, total_modes-kept_modes:total_modes+kept_modes, :kept_modes], self.w2[0]) + \\\n",
    "            torch.einsum('...bi,bio->...bo', o1_real[:, total_modes-kept_modes:total_modes+kept_modes, :kept_modes], self.w2[1]) + \\\n",
    "            self.b2[1]\n",
    "        )\n",
    "\n",
    "        x = torch.stack([o2_real, o2_imag], dim=-1)\n",
    "        x = F.softshrink(x, lambd=self.sparsity_threshold)\n",
    "        x = torch.view_as_complex(x)\n",
    "        x = x.reshape(B, H, W // 2 + 1, C)\n",
    "        x = torch.fft.irfft2(x, s=(H, W), dim=(1,2), norm=\"ortho\")\n",
    "        x = x.type(dtype)\n",
    "\n",
    "        return x + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ca5cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            mlp_ratio=4.,\n",
    "            drop=0.,\n",
    "            drop_path=0.,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            double_skip=True,\n",
    "            num_blocks=8,\n",
    "            sparsity_threshold=0.01,\n",
    "            hard_thresholding_fraction=1.0\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.filter = AFNO2D(dim, num_blocks, sparsity_threshold, hard_thresholding_fraction) \n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        #self.drop_path = nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        self.double_skip = double_skip\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.filter(x)\n",
    "\n",
    "        if self.double_skip:\n",
    "            x = x + residual\n",
    "            residual = x\n",
    "\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.drop_path(x)\n",
    "        x = x + residual\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bb7aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecipNet(nn.Module):\n",
    "    def __init__(self, params, backbone):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.patch_size = (params.patch_size, params.patch_size)\n",
    "        self.in_chans = params.N_in_channels\n",
    "        self.out_chans = params.N_out_channels\n",
    "        self.backbone = backbone\n",
    "        self.ppad = PeriodicPad2d(1)\n",
    "        self.conv = nn.Conv2d(self.out_chans, self.out_chans, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.ppad(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2df82954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pointwise_op(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel,dim1, dim2):\n",
    "        super(pointwise_op,self).__init__()\n",
    "        self.conv = nn.Conv2d(int(in_channel), int(out_channel), 1)\n",
    "        self.dim1 = int(dim1)\n",
    "        self.dim2 = int(dim2)\n",
    "        #self.alpha_drop = nn.AlphaDropout(p=0.42)\n",
    "        #self.layer_norm = nn.LayerNorm([int(out_channel), self.dim1, self.dim2])\n",
    "        #self.batch_norm = nn.BatchNorm2d(int(out_channel))\n",
    "        #self.upsample = nn.Upsample(size=[self.dim1, self.dim2], mode='bicubic', align_corners=True)\n",
    "\n",
    "    def forward(self,x, dim1 = None, dim2 = None):\n",
    "        if dim1 is None:\n",
    "            dim1 = self.dim1\n",
    "            dim2 = self.dim2\n",
    "        x_out = self.conv(x)\n",
    "        x_out = torch.nn.functional.interpolate(x_out, size = (dim1, dim2),mode = 'bicubic',align_corners=True)\n",
    "        #x_out = self.upsample(x_out)\n",
    "        #x_out = self.alpha_drop(x_out)\n",
    "        #x_out = self.layer_norm(x_out)\n",
    "        #x_out = self.batch_norm(x_out)\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af995735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFNONet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size=(720, 1440),\n",
    "            patch_size=(16, 16),\n",
    "            in_chans=3,\n",
    "            out_chans=3,\n",
    "            embed_dim=768,\n",
    "            depth=12,\n",
    "            mlp_ratio=4.,\n",
    "            drop_rate=0.,\n",
    "            drop_path_rate=0.,\n",
    "            num_blocks=16,\n",
    "            sparsity_threshold=0.01,\n",
    "            hard_thresholding_fraction=1.0,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        #self.params = params\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = (patch_size[0], patch_size[1])\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.num_blocks = num_blocks \n",
    "        norm_layer = partial(nn.LayerNorm, eps=1e-6)\n",
    "\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=self.patch_size, in_chans=self.in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "\n",
    "        self.h = img_size[0] // self.patch_size[0]\n",
    "        self.w = img_size[1] // self.patch_size[1]\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(dim=embed_dim, mlp_ratio=mlp_ratio, drop=drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
    "            num_blocks=self.num_blocks, sparsity_threshold=sparsity_threshold, hard_thresholding_fraction=hard_thresholding_fraction) \n",
    "        for i in range(depth)])\n",
    "\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        self.head = nn.Linear(embed_dim, self.out_chans*self.patch_size[0]*self.patch_size[1], bias=False)\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        x = x.reshape(B, self.h, self.w, self.embed_dim)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        x = rearrange(\n",
    "            x,\n",
    "            \"b h w (p1 p2 c_out) -> b c_out (h p1) (w p2)\",\n",
    "            p1=self.patch_size[0],\n",
    "            p2=self.patch_size[1],\n",
    "            h=self.img_size[0] // self.patch_size[0],\n",
    "            w=self.img_size[1] // self.patch_size[1],\n",
    "        )\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=(224, 224), patch_size=(16, 16), in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dadb0826",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NIO_transformer(nn.Module):\n",
    "    def __init__(self, img_size=(100,100), patch_size=(2,2), in_chans=3, stack_factor=2, num_classes=0,embed_dim=768, depth=4,\n",
    "                 mlp_ratio=4., width=100, height=100, mixing_type='sa'):\n",
    "        super().__init__()\n",
    "        #self.interpolation_l = nn.Conv2d(3, 3, kernel_size=1)\n",
    "        #self.interpolation_r = nn.Conv2d(3,3,kernel_size=1)\n",
    "        self.embed_dim = embed_dim\n",
    "        #self.AF_transformer_red = AFNONet(img_size=img_size, patch_size=patch_size, in_chans=2, out_chans=1, depth=1)\n",
    "        #self.AF_transformer_green = AFNONet(img_size=img_size, patch_size=patch_size, in_chans=2, out_chans=1, depth=1)\n",
    "        #self.AF_transformer_blue = AFNONet(img_size=img_size, patch_size=patch_size, in_chans=2, out_chans=1, depth=1)\n",
    "        self.AF_transformer = AFNONet(img_size=img_size, patch_size=patch_size, in_chans=6, out_chans=3, depth=depth)\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.UNO_block_red = UNO_vanilla(2, 1, pad=0, factor=16/4)\n",
    "        self.UNO_block_green = UNO_vanilla(2, 1, pad=0, factor=16/4)\n",
    "        self.UNO_block_blue = UNO_vanilla(2, 1, pad=0, factor=16/4)\n",
    "        self.UNO_block_merge = UNO(3, 3, pad=0, factor=16/4)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        self.output_dims = in_chans*width*height\n",
    "        #self.unflatten = nn.Unflatten(1, (3,16,16))\n",
    "        self.deconv1 = nn.ConvTranspose2d(3, 3, 3,stride=1, padding=1)\n",
    "        #self.alpha_drop1 = nn.AlphaDropout(p=0.42)\n",
    "        #self.batch_norm1 = nn.BatchNorm2d(3)\n",
    "        self.deconv2 = nn.ConvTranspose2d(3, 3, 4,stride=2, padding=1)\n",
    "        #self.alpha_drop2 = nn.AlphaDropout(p=0.42)\n",
    "        #self.batch_norm2 = nn.BatchNorm2d(3)\n",
    "        self.deconv3 = nn.ConvTranspose2d(3, 3, 2,stride=2, padding=7)\n",
    "        #self.alpha_drop3 = nn.AlphaDropout(p=0.42)\n",
    "        #self.batch_norm3 = nn.BatchNorm2d(3)\n",
    "        self.deconv4 = nn.ConvTranspose2d(3, 3, 2,stride=2, padding=0)\n",
    "        #self.alpha_drop4 = nn.AlphaDropout(p=0.42)\n",
    "        #self.batch_norm4 = nn.BatchNorm2d(3)\n",
    "        self.pointwise1 = pointwise_op(3, 3, self.height, self.width)\n",
    "        self.pointwise2 = pointwise_op(3, 3, self.height, self.width)\n",
    "        self.pointwise3 = pointwise_op(3,3, self.height, self.width)\n",
    "        self.pointwise4 = pointwise_op(3, 3, self.height, self.width)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.height = x.shape[2]\n",
    "        self.width = x.shape[3]\n",
    "        #self.output_dims = self.height*self.width*x.shape[4]\n",
    "        x_l = x[0]\n",
    "        x_r = x[1]\n",
    "        \n",
    "        x_l_colors = x_l.permute(1,0,2,3)\n",
    "        x_r_colors = x_r.permute(1,0,2,3)\n",
    "        \n",
    "        #print(x_l_colors.shape)\n",
    "        x_l_red = x_l_colors[0]\n",
    "        x_l_green = x_l_colors[1]\n",
    "        x_l_blue = x_l_colors[2]\n",
    "        \n",
    "        x_r_red = x_r_colors[0]\n",
    "        x_r_green = x_r_colors[1]\n",
    "        x_r_blue = x_r_colors[2]\n",
    "        \n",
    "        x_red = torch.stack((x_l_red, x_r_red)).permute(1,0,2,3)\n",
    "        x_green = torch.stack((x_l_green, x_r_green)).permute(1,0,2,3)\n",
    "        x_blue = torch.stack((x_l_blue, x_r_blue)).permute(1,0,2,3)\n",
    "        \n",
    "        #print(x_red.shape)\n",
    "        #print(x_green.shape)\n",
    "        #print(x_blue.shape)\n",
    "\n",
    "        #x = torch.cat((x_l, x_r), dim=1)\n",
    "        #x = self.AF_transformer(x)\n",
    "        #r = self.UNO_block_red(x_red)\n",
    "        #g = self.UNO_block_green(x_green)\n",
    "        #b = self.UNO_block_blue(x_blue)\n",
    "        #uno_x = torch.cat((r,g,b), dim=1)\n",
    "        \n",
    "        \n",
    "        x = torch.cat((x_l, x_r), dim=1)\n",
    "        transfo_x = self.AF_transformer(x)\n",
    "        #x_concat = 0.0001*uno_x + transfo_x\n",
    "        #x_out = self.UNO_block_merge(x_concat)\n",
    "        #x_concat = transfo_x\n",
    "        \n",
    "        \"\"\"\n",
    "        x = self.deconv1(x)\n",
    "        x = self.pointwise1(x, self.height//8, self.width//8)\n",
    "        #x = torch.nn.functional.interpolate(x, size = (self.output_dims//8, self.output_dims//8),mode = 'bicubic',align_corners=True)\n",
    "        #x = self.alpha_drop1(x)\n",
    "        #x = self.batch_norm1(x)\n",
    "        x = self.deconv2(x)\n",
    "        x = self.pointwise2(x, self.height//4, self.width//4)\n",
    "        #x = torch.nn.functional.interpolate(x, size=(self.output_dims//4, self.output_dims//4), mode = 'bicubic',align_corners=True)\n",
    "        #x = self.alpha_drop2(x)\n",
    "        #x = self.batch_norm2(x)\n",
    "        x = self.deconv3(x)\n",
    "        x = self.pointwise3(x, self.height//2, self.width//2)\n",
    "        #x = torch.nn.functional.interpolate(x, size=(self.output_dims//2, self.output_dims//2), mode = 'bicubic',align_corners=True)\n",
    "        #x = self.alpha_drop3(x)\n",
    "        #x = self.batch_norm3(x)\n",
    "        x = self.deconv4(x)\n",
    "        x = self.pointwise4(x, self.height, self.width)\n",
    "        #x = torch.nn.functional.interpolate(x, size=(self.output_dims, self.output_dims), mode = 'bicubic',align_corners=True)\n",
    "        #x = self.alpha_drop4(x)\n",
    "        #x = self.batch_norm4(x)\n",
    "        #x = self.pointwise(x, self.height, self.width)\n",
    "        \"\"\"\n",
    "        #print(x.shape)\n",
    "        return transfo_x\n",
    "    def sub_mean(self, x):\n",
    "        mean = x.mean(2, keepdim=True).mean(3, keepdim=True)\n",
    "        x -= mean\n",
    "        return x, mean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "153aec6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "NIO_transformer                          [10, 3, 256, 256]         --\n",
       "├─AFNONet: 1-1                           [10, 3, 256, 256]         12,593,664\n",
       "│    └─PatchEmbed: 2-1                   [10, 16384, 768]          --\n",
       "│    │    └─Conv2d: 3-1                  [10, 768, 128, 128]       19,200\n",
       "│    └─Dropout: 2-2                      [10, 16384, 768]          --\n",
       "│    └─ModuleList: 2-3                   --                        --\n",
       "│    │    └─Block: 3-2                   [10, 128, 128, 768]       4,876,032\n",
       "│    │    └─Block: 3-3                   [10, 128, 128, 768]       4,876,032\n",
       "│    │    └─Block: 3-4                   [10, 128, 128, 768]       4,876,032\n",
       "│    │    └─Block: 3-5                   [10, 128, 128, 768]       4,876,032\n",
       "├─UNO_vanilla: 1-2                       --                        --\n",
       "│    └─Linear: 2-4                       --                        3\n",
       "│    └─SpectralConv2d: 2-5               --                        56,448\n",
       "│    └─SpectralConv2d: 2-6               --                        451,584\n",
       "│    └─SpectralConv2d: 2-7               --                        409,600\n",
       "│    └─SpectralConv2d: 2-8               --                        409,600\n",
       "│    └─SpectralConv2d: 2-9               --                        409,600\n",
       "│    └─SpectralConv2d: 2-10              --                        819,200\n",
       "│    └─SpectralConv2d: 2-11              --                        903,168\n",
       "│    └─SpectralConv2d: 2-12              --                        112,896\n",
       "│    └─pointwise_op: 2-13                --                        --\n",
       "│    │    └─Conv2d: 3-6                  --                        32\n",
       "│    └─pointwise_op: 2-14                --                        --\n",
       "│    │    └─Conv2d: 3-7                  --                        544\n",
       "│    └─pointwise_op: 2-15                --                        --\n",
       "│    │    └─Conv2d: 3-8                  --                        2,112\n",
       "│    └─pointwise_op: 2-16                --                        --\n",
       "│    │    └─Conv2d: 3-9                  --                        8,320\n",
       "│    └─pointwise_op: 2-17                --                        --\n",
       "│    │    └─Conv2d: 3-10                 --                        8,256\n",
       "│    └─pointwise_op: 2-18                --                        --\n",
       "│    │    └─Conv2d: 3-11                 --                        4,128\n",
       "│    └─pointwise_op: 2-19                --                        --\n",
       "│    │    └─Conv2d: 3-12                 --                        1,040\n",
       "│    └─pointwise_op: 2-20                --                        --\n",
       "│    │    └─Conv2d: 3-13                 --                        33\n",
       "│    └─Linear: 2-21                      --                        12\n",
       "│    └─Linear: 2-22                      --                        5\n",
       "├─UNO_vanilla: 1-3                       --                        --\n",
       "│    └─Linear: 2-23                      --                        3\n",
       "│    └─SpectralConv2d: 2-24              --                        56,448\n",
       "│    └─SpectralConv2d: 2-25              --                        451,584\n",
       "│    └─SpectralConv2d: 2-26              --                        409,600\n",
       "│    └─SpectralConv2d: 2-27              --                        409,600\n",
       "│    └─SpectralConv2d: 2-28              --                        409,600\n",
       "│    └─SpectralConv2d: 2-29              --                        819,200\n",
       "│    └─SpectralConv2d: 2-30              --                        903,168\n",
       "│    └─SpectralConv2d: 2-31              --                        112,896\n",
       "│    └─pointwise_op: 2-32                --                        --\n",
       "│    │    └─Conv2d: 3-14                 --                        32\n",
       "│    └─pointwise_op: 2-33                --                        --\n",
       "│    │    └─Conv2d: 3-15                 --                        544\n",
       "│    └─pointwise_op: 2-34                --                        --\n",
       "│    │    └─Conv2d: 3-16                 --                        2,112\n",
       "│    └─pointwise_op: 2-35                --                        --\n",
       "│    │    └─Conv2d: 3-17                 --                        8,320\n",
       "│    └─pointwise_op: 2-36                --                        --\n",
       "│    │    └─Conv2d: 3-18                 --                        8,256\n",
       "│    └─pointwise_op: 2-37                --                        --\n",
       "│    │    └─Conv2d: 3-19                 --                        4,128\n",
       "│    └─pointwise_op: 2-38                --                        --\n",
       "│    │    └─Conv2d: 3-20                 --                        1,040\n",
       "│    └─pointwise_op: 2-39                --                        --\n",
       "│    │    └─Conv2d: 3-21                 --                        33\n",
       "│    └─Linear: 2-40                      --                        12\n",
       "│    └─Linear: 2-41                      --                        5\n",
       "├─UNO_vanilla: 1-4                       --                        --\n",
       "│    └─Linear: 2-42                      --                        3\n",
       "│    └─SpectralConv2d: 2-43              --                        56,448\n",
       "│    └─SpectralConv2d: 2-44              --                        451,584\n",
       "│    └─SpectralConv2d: 2-45              --                        409,600\n",
       "│    └─SpectralConv2d: 2-46              --                        409,600\n",
       "│    └─SpectralConv2d: 2-47              --                        409,600\n",
       "│    └─SpectralConv2d: 2-48              --                        819,200\n",
       "│    └─SpectralConv2d: 2-49              --                        903,168\n",
       "│    └─SpectralConv2d: 2-50              --                        112,896\n",
       "│    └─pointwise_op: 2-51                --                        --\n",
       "│    │    └─Conv2d: 3-22                 --                        32\n",
       "│    └─pointwise_op: 2-52                --                        --\n",
       "│    │    └─Conv2d: 3-23                 --                        544\n",
       "│    └─pointwise_op: 2-53                --                        --\n",
       "│    │    └─Conv2d: 3-24                 --                        2,112\n",
       "│    └─pointwise_op: 2-54                --                        --\n",
       "│    │    └─Conv2d: 3-25                 --                        8,320\n",
       "│    └─pointwise_op: 2-55                --                        --\n",
       "│    │    └─Conv2d: 3-26                 --                        8,256\n",
       "│    └─pointwise_op: 2-56                --                        --\n",
       "│    │    └─Conv2d: 3-27                 --                        4,128\n",
       "│    └─pointwise_op: 2-57                --                        --\n",
       "│    │    └─Conv2d: 3-28                 --                        1,040\n",
       "│    └─pointwise_op: 2-58                --                        --\n",
       "│    │    └─Conv2d: 3-29                 --                        33\n",
       "│    └─Linear: 2-59                      --                        12\n",
       "│    └─Linear: 2-60                      --                        5\n",
       "├─UNO: 1-5                               --                        --\n",
       "│    └─Conv2d: 2-61                      --                        4\n",
       "│    └─Conv2d: 2-62                      --                        12\n",
       "│    └─Linear: 2-63                      --                        12\n",
       "│    └─SpectralConv2d: 2-64              --                        508,032\n",
       "│    └─SpectralConv2d: 2-65              --                        4,064,256\n",
       "│    └─SpectralConv2d: 2-66              --                        3,686,400\n",
       "│    └─SpectralConv2d: 2-67              --                        3,686,400\n",
       "│    └─SpectralConv2d: 2-68              --                        3,686,400\n",
       "│    └─SpectralConv2d: 2-69              --                        7,372,800\n",
       "│    └─SpectralConv2d: 2-70              --                        8,128,512\n",
       "│    └─SpectralConv2d: 2-71              --                        1,016,064\n",
       "│    └─pointwise_op: 2-72                --                        --\n",
       "│    │    └─Conv2d: 3-30                 --                        192\n",
       "│    └─pointwise_op: 2-73                --                        --\n",
       "│    │    └─Conv2d: 3-31                 --                        4,704\n",
       "│    └─pointwise_op: 2-74                --                        --\n",
       "│    │    └─Conv2d: 3-32                 --                        18,624\n",
       "│    └─pointwise_op: 2-75                --                        --\n",
       "│    │    └─Conv2d: 3-33                 --                        74,112\n",
       "│    └─pointwise_op: 2-76                --                        --\n",
       "│    │    └─Conv2d: 3-34                 --                        73,920\n",
       "│    └─pointwise_op: 2-77                --                        --\n",
       "│    │    └─Conv2d: 3-35                 --                        36,960\n",
       "│    └─pointwise_op: 2-78                --                        --\n",
       "│    │    └─Conv2d: 3-36                 --                        9,264\n",
       "│    └─pointwise_op: 2-79                --                        --\n",
       "│    │    └─Conv2d: 3-37                 --                        291\n",
       "│    └─Linear: 2-80                      --                        84\n",
       "│    └─Linear: 2-81                      --                        39\n",
       "│    └─Conv2d: 2-82                      --                        6\n",
       "├─AFNONet: 1                             --                        --\n",
       "│    └─Linear: 2-83                      [10, 128, 128, 12]        9,216\n",
       "==========================================================================================\n",
       "Total params: 75,273,823\n",
       "Trainable params: 75,273,823\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 3.33\n",
       "==========================================================================================\n",
       "Input size (MB): 15.73\n",
       "Forward/backward pass size (MB): 33234.62\n",
       "Params size (MB): 422.22\n",
       "Estimated Total Size (MB): 33672.56\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NIO_Af = AFNONet(img_size=100, patch_size=15, in_chans = 3, num_classes=0)\n",
    "NIO_Af = NIO_transformer(img_size=(256,256))\n",
    "summary(NIO_Af, input_size=(2,10,  3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "413aa960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np\n",
    "import random\n",
    "def LoadDataBatches(batchNumber, batches=4, isNormalized=True, isTrain=True, percent=1):\n",
    "    if(isTrain):\n",
    "        x_train = torch.load('/scratch/gilbreth/hviswan/x_train_vimeo_256_diff_type.pt')\n",
    "        y_train = torch.load('/scratch/gilbreth/hviswan/y_train_vimeo_256_diff_type.pt')\n",
    "        ntrain = x_train.shape[0]\n",
    "        \n",
    "        ntrain_slice = int(ntrain*percent)\n",
    "        if(ntrain_slice < ntrain-30):\n",
    "            start_index = min(0, random.randint(0, ntrain-ntrain_slice-30))\n",
    "        else:\n",
    "            start_index = 0\n",
    "        x_train = x_train[0+start_index:start_index+ntrain_slice]\n",
    "        x_train = torch.permute(x_train, (1, 0,2,3,4))\n",
    "        print(x_train.shape)\n",
    "        #ntrain = x_train.shape[1]\n",
    "        y_train = y_train.reshape(ntrain, x_train.shape[2], x_train.shape[3], x_train.shape[4])\n",
    "        y_train = y_train[0+start_index:ntrain_slice+start_index]\n",
    "        print(x_train.shape)\n",
    "        print(y_train.shape)\n",
    "        if(isNormalized):\n",
    "            y_normalizer = MinMaxNormalizer(y_train)\n",
    "            x_normalizer = MinMaxNormalizer(x_train)\n",
    "            x_train = x_normalizer.encode(x_train)\n",
    "            y_train = y_normalizer.encode(y_train)\n",
    "        train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train[0], x_train[1], y_train), batch_size=batches, shuffle=True)\n",
    "        if(isNormalized):\n",
    "            return train_loader, y_normalizer\n",
    "        return train_loader\n",
    "    else:\n",
    "        x_test = torch.load('/scratch/gilbreth/hviswan/x_test_vimeo_256_diff_type.pt')\n",
    "        y_test = torch.load('/scratch/gilbreth/hviswan/y_test_vimeo_256_diff_type.pt')\n",
    "        ntest = x_test.shape[0]\n",
    "        #x_test = x_test[0: 500]\n",
    "        x_test = torch.permute(x_test, (1, 0,2,3,4))\n",
    "        \n",
    "        y_test = y_test.reshape(ntest, x_test.shape[2], x_test.shape[3], x_test.shape[4])\n",
    "        #y_test = y_test[0:500]\n",
    "        print(x_test.shape)\n",
    "        print(y_test.shape)\n",
    "        if(isNormalized):\n",
    "            y_normalizer = MinMaxNormalizer(y_test)\n",
    "            x_normalizer = MinMaxNormalizer(x_test)\n",
    "            x_test = x_normalizer.encode(x_test)\n",
    "            y_test = y_normalizer.encode(y_test)\n",
    "        test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test[0], x_test[1], y_test), batch_size=batches, shuffle=True)\n",
    "        if(isNormalized):\n",
    "            return test_loader, y_normalizer\n",
    "        return test_loader\n",
    "#LoadDataBatches(2, batches=10, isNormalized=False, percent=1)\n",
    "#LoadDataBatches(2, batches=10, isNormalized=False, percent=1, isTrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "330abf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class MeanShift(nn.Conv2d):\n",
    "    def __init__(self, rgb_mean, rgb_std, sign=-1):\n",
    "        super(MeanShift, self).__init__(3, 3, kernel_size=1)\n",
    "        std = torch.Tensor(rgb_std)\n",
    "        self.weight.data = torch.eye(3).view(3, 3, 1, 1)\n",
    "        self.weight.data.div_(std.view(3, 1, 1, 1))\n",
    "        self.bias.data = sign * torch.Tensor(rgb_mean)\n",
    "        self.bias.data.div_(std)\n",
    "        self.requires_grad = False\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, loss_type):\n",
    "        super(VGG, self).__init__()\n",
    "        vgg_features = models.vgg19(pretrained=True).features\n",
    "        modules = [m.cuda() for m in vgg_features]\n",
    "        conv_index = loss_type\n",
    "        if conv_index == '22':\n",
    "            self.vgg = nn.Sequential(*modules[:8])\n",
    "        elif conv_index == '33':\n",
    "            self.vgg = nn.Sequential(*modules[:16])\n",
    "        elif conv_index == '44':\n",
    "            self.vgg = nn.Sequential(*modules[:26])\n",
    "        elif conv_index == '54':\n",
    "            self.vgg = nn.Sequential(*modules[:35])\n",
    "        elif conv_index == 'P':\n",
    "            self.vgg = nn.ModuleList([\n",
    "                nn.Sequential(*modules[:8]),\n",
    "                nn.Sequential(*modules[8:16]),\n",
    "                nn.Sequential(*modules[16:26]),\n",
    "                nn.Sequential(*modules[26:35])\n",
    "            ])\n",
    "        self.vgg = nn.DataParallel(self.vgg).cuda()\n",
    "\n",
    "        vgg_mean = (0.485, 0.456, 0.406)\n",
    "        vgg_std = (0.229, 0.224, 0.225)\n",
    "        self.sub_mean = MeanShift(vgg_mean, vgg_std)\n",
    "        self.vgg.requires_grad = False\n",
    "        # self.criterion = nn.L1Loss()\n",
    "        self.conv_index = conv_index\n",
    "\n",
    "    def forward(self, sr, hr):\n",
    "        def _forward(x):\n",
    "            x = x.cpu()\n",
    "            x = self.sub_mean(x)\n",
    "            x = self.vgg(x)\n",
    "            return x.cuda()\n",
    "        def _forward_all(x):\n",
    "            feats = []\n",
    "            x = x.cpu()\n",
    "            x = self.sub_mean(x)\n",
    "            for module in self.vgg.module:\n",
    "                x = module(x.cuda())\n",
    "                feats.append(x.cuda())\n",
    "            return feats\n",
    "\n",
    "        if self.conv_index == 'P':\n",
    "            vgg_sr_feats = _forward_all(sr)\n",
    "            with torch.no_grad():\n",
    "                vgg_hr_feats = _forward_all(hr.detach())\n",
    "            loss = 0\n",
    "            for i in range(len(vgg_sr_feats)):\n",
    "                loss_f = F.mse_loss(vgg_sr_feats[i], vgg_hr_feats[i])\n",
    "                #print(loss_f)\n",
    "                loss += loss_f\n",
    "            #print()\n",
    "        else:\n",
    "            vgg_sr = _forward(sr)\n",
    "            with torch.no_grad():\n",
    "                vgg_hr = _forward(hr.detach())\n",
    "            loss = F.mse_loss(vgg_sr, vgg_hr)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "245bf2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as time\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import torchvision.transforms.functional as FF\n",
    "import cv2\n",
    "import PIL\n",
    "myloss = torch.nn.MSELoss()\n",
    "l1loss = torch.nn.L1Loss()\n",
    "kldivloss = torch.nn.KLDivLoss()\n",
    "vggloss = VGG('22')\n",
    "RHO = 0.05\n",
    "BETA = 0.01\n",
    "def train_NIO_transformer(D, G, train_data, epochs, D_optim, G_optim, scheduler=None):\n",
    "    losses_D = np.zeros(epochs)\n",
    "    losses_G = np.zeros(epochs)\n",
    "    ssims_G = np.zeros(epochs)\n",
    "    #train_data = LoadDataBatches(0, isNormalized=False, batches=10, isTrain=True, percent=0.2)\n",
    "    train_data = LoadDataBatches(0, isNormalized=False, batches=2, isTrain=True)\n",
    "    #base_G = torch.load('/depot/bera89/data/hviswan/UNO_2f_vimeo_vgg_rgb150.pt')\n",
    "    #complex_G = torch.load('/depot/bera89/data/hviswan/UNO_2f_vimeo_huge_rgb1600.pt')\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(G_optim, factor=0.2, patience=5, mode='min')\n",
    "    for i in range(1, epochs+1):\n",
    "        t1 = time.now()\n",
    "        loss_D = 0.0\n",
    "        loss_G = 0.0\n",
    "        ssim_G = 0.0\n",
    "        train_counter = 0\n",
    "        ssim_batch=0\n",
    "        for j in range(1):\n",
    "            #del train_data\n",
    "            #train_data = LoadDataBatches(j, batches=10)\n",
    "            for xl,xr,y in train_data:\n",
    "                train_counter += 1\n",
    "                temp = 0.05*xl + 0.1*xr\n",
    "                \n",
    "                xl = xl.cpu().detach().numpy()\n",
    "                y = y.cpu().detach().numpy()\n",
    "                #xr = xr.cpu().detach().numpy()\n",
    "                x = torch.from_numpy(np.asarray([xl,y]).astype(np.float32))\n",
    "                xr = xr.cuda()\n",
    "                x = x.cuda()\n",
    "                #y = y.cuda()\n",
    "                G_optim.zero_grad()\n",
    "                #base_x_out = base_G(x).cuda()\n",
    "                #base_x_out = torch.permute(base_x_out, (0,3, 1, 2))\n",
    "                #x_avg = torch.from_numpy(np.asarray((xl+xr)/2.0).astype(np.float32)).cuda()\n",
    "                #x = torch.stack((base_x_out, base_x_out))\n",
    "                #complex_x_out = complex_G(x).reshape(x.shape[1], x.shape[2],x.shape[3],x.shape[4])\n",
    "                #x = torch.stack((complex_x_out, complex_x_out))\n",
    "                #x = torch.permute(complex_x_out, (0, 3, 1, 2))\n",
    "                #xl_in = torch.permute(xl_in, (0,3,1,2))\n",
    "                #base_x_gen = base_G(x).permute(0,3,1,2)\n",
    "                x_syn = G(x)\n",
    "                #x_syn = x_syn #+ 0.001*base_x_gen\n",
    "                #x_syn_colors = torch.permute(x_syn, (1,0,2,3))\n",
    "                #x_r = x_syn_colors[0]\n",
    "                #x_b = x_syn_colors[2]\n",
    "                #x_g = x_syn_colors[1]\n",
    "                \n",
    "                #x_syn = torch.cat((x_r, x_g, x_b), dim=1)\n",
    "                #print(x_syn.shape)\n",
    "                #x_syn = torch.permute(x_syn, (0, 2, 3, 1))\n",
    "                #x_syn = x_syn + temp.cuda()\n",
    "                #x_syn = y_normalizer.decode(x_syn)\n",
    "                #y = y_normalizer.decode(y)\n",
    "                #x_syn2 = torch.permute(x_syn, (0, 3, 1, 2)).type(torch.float32)\n",
    "                #x_syn2 = x_syn2.detach().cpu().numpy()\n",
    "                #for k in range(x_syn2.shape[0]):\n",
    "                    #x_syn2[k] = FF.adjust_saturation(x_syn2[k], 1.5)\n",
    "                #x_syn2 = torch.from_numpy(x_syn2.astype(np.float32))\n",
    "                #x_syn2 = x_syn\n",
    "                y2 = torch.permute(xr, (0, 2, 3,1)).type(torch.float32)\n",
    "                #y2_colors = torch.permute(y2, (1,0,2,3))\n",
    "                #y2_r = y2_colors[0]\n",
    "                #y2_g = y2_colors[1]\n",
    "                #y2_b = y2_colors[2]\n",
    "                \n",
    "                x_syn2 = torch.permute(x_syn, (0, 2, 3, 1))\n",
    "                y = xr.reshape(xr.shape[0], xr.shape[1],xr.shape[2],xr.shape[3])\n",
    "               # W_loss = l1loss(x_r.type(torch.float32), y2_r.type(torch.float32)) + l1loss(x_g.type(torch.float32), y2_g.type(torch.float32))+ l1loss(x_b.type(torch.float32), y2_b.type(torch.float32))\n",
    "                \n",
    "                W_loss = l1loss(x_syn.type(torch.float32), y.type(torch.float32)) #+ 0.001*vggloss(x_syn.type(torch.float32), y2.type(torch.float32))#+ 0.01*torch.abs(kldivloss(x_syn2.type(torch.float32), y2.type(torch.float32)))\n",
    "                ssim_value = W_loss.item()\n",
    "                ssim_individual = 0.0\n",
    "                for k in range(y.shape[0]):\n",
    "                    ssim_individual += ssim(x_syn2[k].detach().cpu().numpy(), y2[k].detach().cpu().numpy(), multichannel=True)\n",
    "                \n",
    "                ssim_batch += ssim_individual/y.shape[0]\n",
    "                loss = W_loss\n",
    "                loss.backward()\n",
    "                loss_G += loss.item() \n",
    "                torch.nn.utils.clip_grad_norm_(G.parameters(), 0.1)\n",
    "                G_optim.step()\n",
    "                #scheduler.step()\n",
    "                if(train_counter %100 == 0 or train_counter == 1):\n",
    "                    print(\"Batch = \",train_counter,\" Train Loss = \", loss_G/train_counter, \" SSIM = \", ssim_batch/(train_counter))\n",
    "            losses_D[i] = loss_D / train_counter\n",
    "            losses_G[i] = loss_G / train_counter\n",
    "            ssims_G[i] = ssim_batch / train_counter\n",
    "            t2 = time.now()\n",
    "            test_counter = 1\n",
    "            \"\"\"\n",
    "            test_counter = 0\n",
    "            ssim_batch = 0\n",
    "            for xl, xr, y in validation_data:\n",
    "                temp = 0.05*xl + 0.1*xr\n",
    "                xl = xl.cpu().detach().numpy()\n",
    "                xr = xr.cpu().detach().numpy()\n",
    "                x = torch.from_numpy(np.asarray([xl,xr]).astype(np.float32))\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "                x_syn = G(x).cuda()\n",
    "                x_syn = x_syn + temp.cuda()\n",
    "                x_syn = y_normalizer_test.decode(x_syn)\n",
    "                y = y_normalizer_test.decode(y)\n",
    "                x_syn2 = torch.permute(x_syn, (0, 3, 1, 2)).type(torch.float32)\n",
    "                y2 = torch.permute(y, (0, 3, 1,2)).type(torch.float32)\n",
    "                y = y.reshape(y.shape[0], y.shape[1],y.shape[2],y.shape[3])\n",
    "                W_loss = huberloss(x_syn2.type(torch.float32), y2.type(torch.float32))*10000\n",
    "                ssim_individual = 0.0\n",
    "                for k in range(y.shape[0]):\n",
    "                    ssim_individual += ssim(x_syn[k].detach().cpu().numpy(), y[k].detach().cpu().numpy(), multichannel=True)\n",
    "                \n",
    "                ssim_batch += ssim_individual/y.shape[0]\n",
    "                loss = W_loss\n",
    "                loss.backward()\n",
    "                G_optim.step()\n",
    "                loss_D += loss.item()\n",
    "                test_counter += 1\n",
    "                if(test_counter %50 == 0 or test_counter == 1):\n",
    "                    print(\"Batch = \",test_counter,\" Validation Loss = \", loss_D/test_counter, \" SSIM = \", ssim_batch/(test_counter))\n",
    "            losses_D[i] = loss_D/test_counter\n",
    "            \"\"\"\n",
    "            print(\"Loader #: \", j, \" Epoch: \", i, \"/ \",epochs,\" -  Time: \", t2-t1, \"s - Validation Loss: \", losses_D[i], \" - Train Loss: \", losses_G[i], \" - train SSIM: \", ssims_G[i], \" validation SSIM: \", ssim_batch/test_counter)\n",
    "        if(i%20 == 0):\n",
    "            torch.save(G, '/depot/bera89/data/hviswan/NIO_transformer'+str(i)+'.pt')\n",
    "            \n",
    "    return losses_D, losses_G, D, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1748da79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4400, 3, 256, 256])\n",
      "torch.Size([2, 4400, 3, 256, 256])\n",
      "torch.Size([4400, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40483/1814897442.py:85: FutureWarning: `multichannel` is a deprecated argument name for `structural_similarity`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  ssim_individual += ssim(x_syn2[k].detach().cpu().numpy(), y2[k].detach().cpu().numpy(), multichannel=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch =  1  Train Loss =  1.9852361679077148  SSIM =  -2.3378030164167285e-05\n",
      "Batch =  100  Train Loss =  2.0262391632795334  SSIM =  0.002629652093024788\n",
      "Batch =  200  Train Loss =  1.281402186602354  SSIM =  0.012921692827115408\n",
      "Batch =  300  Train Loss =  0.9808120762308439  SSIM =  0.027834878926407403\n",
      "Batch =  400  Train Loss =  0.7877435138449073  SSIM =  0.055349041271903446\n",
      "Batch =  500  Train Loss =  0.6662572989314794  SSIM =  0.08037478417458738\n",
      "Batch =  600  Train Loss =  0.5761757799734671  SSIM =  0.1190946121011145\n",
      "Batch =  700  Train Loss =  0.5109575137283121  SSIM =  0.1488050001748091\n",
      "Batch =  800  Train Loss =  0.45988248120527714  SSIM =  0.17990669568955653\n",
      "Batch =  900  Train Loss =  0.41930727440863846  SSIM =  0.20561411690754805\n",
      "Batch =  1000  Train Loss =  0.385874004624784  SSIM =  0.2315097665894906\n",
      "Batch =  1100  Train Loss =  0.35818545007908886  SSIM =  0.2597158741371993\n",
      "Batch =  1200  Train Loss =  0.3350719873793423  SSIM =  0.28280007028611837\n",
      "Batch =  1300  Train Loss =  0.3144816925835151  SSIM =  0.3069271832042688\n",
      "Batch =  1400  Train Loss =  0.2967757018522493  SSIM =  0.329731576328042\n",
      "Batch =  1500  Train Loss =  0.28140073776741825  SSIM =  0.35041657592580583\n",
      "Batch =  1600  Train Loss =  0.26792046185582874  SSIM =  0.37005009380379567\n",
      "Batch =  1700  Train Loss =  0.25592118023730376  SSIM =  0.38722281148307347\n",
      "Batch =  1800  Train Loss =  0.24514820748008787  SSIM =  0.40346227822184455\n",
      "Batch =  1900  Train Loss =  0.2356265469807151  SSIM =  0.4174416706748375\n",
      "Batch =  2000  Train Loss =  0.22676032816432418  SSIM =  0.4313048761559715\n",
      "Batch =  2100  Train Loss =  0.21876192289182828  SSIM =  0.443717415602263\n",
      "Batch =  2200  Train Loss =  0.21136460201560775  SSIM =  0.45563069626672614\n",
      "Loader #:  0  Epoch:  1 /  300  -  Time:  0:05:58.677930 s - Validation Loss:  0.0  - Train Loss:  0.21136460201560775  - train SSIM:  0.45563069626672614  validation SSIM:  1002.3875317867975\n",
      "Batch =  1  Train Loss =  0.04126397520303726  SSIM =  0.8102366924285889\n",
      "Batch =  100  Train Loss =  0.0544938132353127  SSIM =  0.7161035524308681\n",
      "Batch =  200  Train Loss =  0.05525874531827867  SSIM =  0.7104542850703001\n",
      "Batch =  300  Train Loss =  0.05520247978468736  SSIM =  0.7126624574760596\n",
      "Batch =  400  Train Loss =  0.05437662253156304  SSIM =  0.7163149689137935\n",
      "Batch =  500  Train Loss =  0.05513422477990389  SSIM =  0.7132528533786535\n",
      "Batch =  600  Train Loss =  0.05518793740930657  SSIM =  0.7129576715951165\n",
      "Batch =  700  Train Loss =  0.055060566727604184  SSIM =  0.7133682457144771\n",
      "Batch =  800  Train Loss =  0.054950851290486755  SSIM =  0.713497959850356\n",
      "Batch =  900  Train Loss =  0.05453978991756837  SSIM =  0.7152834977292353\n",
      "Batch =  1000  Train Loss =  0.05427316297404468  SSIM =  0.7171184099391102\n",
      "Batch =  1100  Train Loss =  0.05476950281045653  SSIM =  0.7142822056399151\n",
      "Batch =  1200  Train Loss =  0.05445744802554448  SSIM =  0.7160740554518997\n",
      "Batch =  1300  Train Loss =  0.054226190181305776  SSIM =  0.716952108058792\n",
      "Batch =  1400  Train Loss =  0.05413401006587914  SSIM =  0.7173644489129739\n",
      "Batch =  1500  Train Loss =  0.05392177241668105  SSIM =  0.7187521752094229\n",
      "Batch =  1600  Train Loss =  0.05358837512205355  SSIM =  0.7207834248780273\n",
      "Batch =  1700  Train Loss =  0.05366878810229109  SSIM =  0.7210163605015945\n",
      "Batch =  1800  Train Loss =  0.05369004924833361  SSIM =  0.7213876505196094\n",
      "Batch =  1900  Train Loss =  0.053604883221713334  SSIM =  0.7217642169955529\n",
      "Batch =  2000  Train Loss =  0.05344518457120284  SSIM =  0.7225581738650799\n",
      "Batch =  2100  Train Loss =  0.05328879816945465  SSIM =  0.7234235597721168\n",
      "Batch =  2200  Train Loss =  0.053116858905096626  SSIM =  0.724672425206412\n",
      "Loader #:  0  Epoch:  2 /  300  -  Time:  0:05:56.552784 s - Validation Loss:  0.0  - Train Loss:  0.053116858905096626  - train SSIM:  0.724672425206412  validation SSIM:  1594.2793354541063\n",
      "Batch =  1  Train Loss =  0.06528964638710022  SSIM =  0.6961513310670853\n",
      "Batch =  100  Train Loss =  0.05096937799826264  SSIM =  0.7386192366480827\n",
      "Batch =  200  Train Loss =  0.05088271355256438  SSIM =  0.7396919251233339\n",
      "Batch =  300  Train Loss =  0.05011226192116738  SSIM =  0.7455354229609171\n",
      "Batch =  400  Train Loss =  0.04941942032426596  SSIM =  0.7482053132355213\n",
      "Batch =  500  Train Loss =  0.04899857014790177  SSIM =  0.7489707372188568\n",
      "Batch =  600  Train Loss =  0.04902074064128101  SSIM =  0.7502351906895638\n",
      "Batch =  700  Train Loss =  0.04965433989784547  SSIM =  0.7461713535764388\n",
      "Batch =  800  Train Loss =  0.04989112435374409  SSIM =  0.7444429111015052\n",
      "Batch =  900  Train Loss =  0.050014068473958305  SSIM =  0.7442437924196322\n",
      "Batch =  1000  Train Loss =  0.049986075755208734  SSIM =  0.7444230111762882\n",
      "Batch =  1100  Train Loss =  0.05022773304276846  SSIM =  0.7431286753917282\n",
      "Batch =  1200  Train Loss =  0.05013722251790265  SSIM =  0.7435231311184665\n",
      "Batch =  1300  Train Loss =  0.050190507821165596  SSIM =  0.7428046455463537\n",
      "Batch =  1400  Train Loss =  0.050037633131391236  SSIM =  0.7431529045264635\n",
      "Batch =  1500  Train Loss =  0.04983691907425721  SSIM =  0.7441188416232666\n",
      "Batch =  1600  Train Loss =  0.04977086522500031  SSIM =  0.744380834759213\n",
      "Batch =  1700  Train Loss =  0.049764218782896504  SSIM =  0.7445357059577808\n",
      "Batch =  1800  Train Loss =  0.04970323753336237  SSIM =  0.7449382373131812\n",
      "Batch =  1900  Train Loss =  0.049616239267940586  SSIM =  0.7455019718506618\n",
      "Batch =  2000  Train Loss =  0.04966387329157442  SSIM =  0.7450115091968328\n",
      "Batch =  2100  Train Loss =  0.04966382446388404  SSIM =  0.7453914157354405\n",
      "Batch =  2200  Train Loss =  0.04968871831470593  SSIM =  0.7452844419889152\n",
      "Loader #:  0  Epoch:  3 /  300  -  Time:  0:05:56.525484 s - Validation Loss:  0.0  - Train Loss:  0.04968871831470593  - train SSIM:  0.7452844419889152  validation SSIM:  1639.6257723756135\n",
      "Batch =  1  Train Loss =  0.05818070098757744  SSIM =  0.6549665927886963\n",
      "Batch =  100  Train Loss =  0.047471870556473734  SSIM =  0.7559261812269688\n",
      "Batch =  200  Train Loss =  0.04780628776177764  SSIM =  0.7557597349025309\n",
      "Batch =  300  Train Loss =  0.04870993432899316  SSIM =  0.7480869993691643\n",
      "Batch =  400  Train Loss =  0.048640645355917514  SSIM =  0.7487737352121622\n",
      "Batch =  500  Train Loss =  0.048491720331832765  SSIM =  0.7485092204585672\n",
      "Batch =  600  Train Loss =  0.04854827020782977  SSIM =  0.7490296259584526\n",
      "Batch =  700  Train Loss =  0.048824012765128696  SSIM =  0.7474853563149061\n",
      "Batch =  800  Train Loss =  0.04871810179087333  SSIM =  0.7487792497314513\n",
      "Batch =  900  Train Loss =  0.04893934407064484  SSIM =  0.7470233500169383\n",
      "Batch =  1000  Train Loss =  0.048884542477317154  SSIM =  0.7473927162587642\n",
      "Batch =  1100  Train Loss =  0.048840809797190805  SSIM =  0.7478214999491518\n",
      "Batch =  1200  Train Loss =  0.048806585429701954  SSIM =  0.7478146412968636\n",
      "Batch =  1300  Train Loss =  0.04874890314486737  SSIM =  0.7485764421751866\n",
      "Batch =  1400  Train Loss =  0.04879907754316394  SSIM =  0.7483733135993992\n",
      "Batch =  1500  Train Loss =  0.048657526092603806  SSIM =  0.749361019462347\n",
      "Batch =  1600  Train Loss =  0.048502930743270556  SSIM =  0.7503170862887054\n",
      "Batch =  1700  Train Loss =  0.04847993186565445  SSIM =  0.7505765346744481\n",
      "Batch =  1800  Train Loss =  0.04860131985973567  SSIM =  0.750180560408367\n",
      "Batch =  1900  Train Loss =  0.04869545979964498  SSIM =  0.7496049087220117\n",
      "Batch =  2000  Train Loss =  0.048647244648542254  SSIM =  0.7496384736672044\n",
      "Batch =  2100  Train Loss =  0.04848679454509346  SSIM =  0.7505072468590168\n",
      "Batch =  2200  Train Loss =  0.04848438339858231  SSIM =  0.7505159532278776\n",
      "Loader #:  0  Epoch:  4 /  300  -  Time:  0:05:55.406285 s - Validation Loss:  0.0  - Train Loss:  0.04848438339858231  - train SSIM:  0.7505159532278776  validation SSIM:  1651.1350971013308\n",
      "Batch =  1  Train Loss =  0.09189390391111374  SSIM =  0.5095980912446976\n",
      "Batch =  100  Train Loss =  0.0532881049439311  SSIM =  0.717045051753521\n",
      "Batch =  200  Train Loss =  0.05017677544616163  SSIM =  0.7365919078141451\n",
      "Batch =  300  Train Loss =  0.04983146931665639  SSIM =  0.73941582630078\n",
      "Batch =  400  Train Loss =  0.048737142372410745  SSIM =  0.7461363042518496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch =  500  Train Loss =  0.04906866979412734  SSIM =  0.7444135282337666\n",
      "Batch =  600  Train Loss =  0.048617805185106895  SSIM =  0.7491575050602357\n",
      "Batch =  700  Train Loss =  0.04844485450402967  SSIM =  0.7494508224938597\n",
      "Batch =  800  Train Loss =  0.04836893605301157  SSIM =  0.7497583018429578\n",
      "Batch =  900  Train Loss =  0.04833603214679493  SSIM =  0.7502779449853633\n",
      "Batch =  1000  Train Loss =  0.04828092296794057  SSIM =  0.7512542942464352\n",
      "Batch =  1100  Train Loss =  0.04839171094819903  SSIM =  0.7511204634877768\n",
      "Batch =  1200  Train Loss =  0.04854441503528505  SSIM =  0.7505928872028986\n",
      "Batch =  1300  Train Loss =  0.04859428480554086  SSIM =  0.7499961943580554\n",
      "Batch =  1400  Train Loss =  0.04858346268268568  SSIM =  0.7499245857154685\n",
      "Batch =  1500  Train Loss =  0.04847074403241277  SSIM =  0.7503753724868099\n",
      "Batch =  1600  Train Loss =  0.04843471724772826  SSIM =  0.750532159244176\n",
      "Batch =  1700  Train Loss =  0.048252997143084515  SSIM =  0.7515128518180813\n",
      "Batch =  1800  Train Loss =  0.048257930246699186  SSIM =  0.7513067737532159\n",
      "Batch =  1900  Train Loss =  0.048048890234020196  SSIM =  0.7517029353564507\n",
      "Batch =  2000  Train Loss =  0.04791925252135843  SSIM =  0.7523997061233968\n",
      "Batch =  2100  Train Loss =  0.04777336908326972  SSIM =  0.7528924217075109\n",
      "Batch =  2200  Train Loss =  0.047842617653818295  SSIM =  0.7526672632077878\n",
      "Loader #:  0  Epoch:  5 /  300  -  Time:  0:05:54.779585 s - Validation Loss:  0.0  - Train Loss:  0.047842617653818295  - train SSIM:  0.7526672632077878  validation SSIM:  1655.8679790571332\n",
      "Batch =  1  Train Loss =  0.051197879016399384  SSIM =  0.7707814574241638\n",
      "Batch =  100  Train Loss =  0.047644850015640255  SSIM =  0.7614488427340984\n",
      "Batch =  200  Train Loss =  0.045824986761435864  SSIM =  0.7646327673643828\n",
      "Batch =  300  Train Loss =  0.04701390470688542  SSIM =  0.7565291221191486\n",
      "Batch =  400  Train Loss =  0.04735341960098594  SSIM =  0.7541641352884472\n",
      "Batch =  500  Train Loss =  0.04728508951887488  SSIM =  0.7551859093159438\n",
      "Batch =  600  Train Loss =  0.047233693605909746  SSIM =  0.7554402287428578\n",
      "Batch =  700  Train Loss =  0.04750516921680953  SSIM =  0.7541834168029683\n",
      "Batch =  800  Train Loss =  0.04766297198948451  SSIM =  0.7531784236337989\n",
      "Batch =  900  Train Loss =  0.047495244040878286  SSIM =  0.7541948263719678\n",
      "Batch =  1000  Train Loss =  0.047499708919785916  SSIM =  0.7551284652687609\n",
      "Batch =  1100  Train Loss =  0.04724265563979067  SSIM =  0.7558864305134524\n",
      "Batch =  1200  Train Loss =  0.04735377431303884  SSIM =  0.7550760791233431\n",
      "Batch =  1300  Train Loss =  0.04743575411705443  SSIM =  0.7541951439615625\n",
      "Batch =  1400  Train Loss =  0.04735436405454363  SSIM =  0.7540941613726317\n",
      "Batch =  1500  Train Loss =  0.047438933495432135  SSIM =  0.7540953424001734\n",
      "Batch =  1600  Train Loss =  0.04741290639794897  SSIM =  0.7541852042288519\n",
      "Batch =  1700  Train Loss =  0.047417764706975396  SSIM =  0.7540154820088955\n",
      "Batch =  1800  Train Loss =  0.04755601811584913  SSIM =  0.7533049817652339\n",
      "Batch =  1900  Train Loss =  0.04766563853199937  SSIM =  0.7526542168404711\n",
      "Batch =  2000  Train Loss =  0.04763736814307049  SSIM =  0.7528621375728398\n",
      "Batch =  2100  Train Loss =  0.047535947008235824  SSIM =  0.7531479100474999\n",
      "Batch =  2200  Train Loss =  0.04747925573240288  SSIM =  0.7537099247307263\n",
      "Loader #:  0  Epoch:  6 /  300  -  Time:  0:05:55.164846 s - Validation Loss:  0.0  - Train Loss:  0.04747925573240288  - train SSIM:  0.7537099247307263  validation SSIM:  1658.1618344075978\n",
      "Batch =  1  Train Loss =  0.036196839064359665  SSIM =  0.7750829756259918\n",
      "Batch =  100  Train Loss =  0.048788728909567  SSIM =  0.7474924786388875\n",
      "Batch =  200  Train Loss =  0.04806362948846072  SSIM =  0.745931845009327\n",
      "Batch =  300  Train Loss =  0.04684367278590798  SSIM =  0.7552363644043605\n",
      "Batch =  400  Train Loss =  0.047267393432557585  SSIM =  0.753116591386497\n",
      "Batch =  500  Train Loss =  0.046744163937866685  SSIM =  0.7558815386891365\n",
      "Batch =  600  Train Loss =  0.047143750631560885  SSIM =  0.7531549951682488\n",
      "Batch =  700  Train Loss =  0.04687493035037603  SSIM =  0.7547679896759135\n",
      "Batch =  800  Train Loss =  0.046933196999598296  SSIM =  0.7555530987959355\n",
      "Batch =  900  Train Loss =  0.04692560377634234  SSIM =  0.7556355261554321\n",
      "Batch =  1000  Train Loss =  0.04697285928204656  SSIM =  0.7559795485213399\n",
      "Batch =  1100  Train Loss =  0.04728493139147758  SSIM =  0.7542758334428072\n",
      "Batch =  1200  Train Loss =  0.04733256978603701  SSIM =  0.7545006399042904\n",
      "Batch =  1300  Train Loss =  0.04745450486643956  SSIM =  0.7537571671318549\n",
      "Batch =  1400  Train Loss =  0.04724749703625483  SSIM =  0.7549696109497122\n",
      "Batch =  1500  Train Loss =  0.04726585805850724  SSIM =  0.7548242739289999\n",
      "Batch =  1600  Train Loss =  0.047307804374140686  SSIM =  0.7544731753831729\n",
      "Batch =  1700  Train Loss =  0.04734791498576455  SSIM =  0.7542290753257626\n",
      "Batch =  1800  Train Loss =  0.04728157922522062  SSIM =  0.7547210156710611\n",
      "Batch =  1900  Train Loss =  0.047263476713805604  SSIM =  0.7547625556921488\n",
      "Batch =  2000  Train Loss =  0.047212312902789565  SSIM =  0.7548811144698411\n",
      "Batch =  2100  Train Loss =  0.047225745695953565  SSIM =  0.7550521871820092\n",
      "Batch =  2200  Train Loss =  0.04729502675843171  SSIM =  0.7548981270685114\n",
      "Loader #:  0  Epoch:  7 /  300  -  Time:  0:05:55.105300 s - Validation Loss:  0.0  - Train Loss:  0.04729502675843171  - train SSIM:  0.7548981270685114  validation SSIM:  1660.7758795507252\n",
      "Batch =  1  Train Loss =  0.0684593915939331  SSIM =  0.6472377777099609\n",
      "Batch =  100  Train Loss =  0.04721745292656124  SSIM =  0.7556233717501164\n",
      "Batch =  200  Train Loss =  0.04683665059506893  SSIM =  0.7558084505237639\n",
      "Batch =  300  Train Loss =  0.04700770078226924  SSIM =  0.7555255481228232\n",
      "Batch =  400  Train Loss =  0.04708558814134449  SSIM =  0.755199028449133\n",
      "Batch =  500  Train Loss =  0.047389393378049136  SSIM =  0.7527961680069566\n",
      "Batch =  600  Train Loss =  0.04684516100678593  SSIM =  0.7562014136277139\n",
      "Batch =  700  Train Loss =  0.04697999878520412  SSIM =  0.7551652870486889\n",
      "Batch =  800  Train Loss =  0.04659046179498546  SSIM =  0.7578023633314297\n",
      "Batch =  900  Train Loss =  0.04667499116414951  SSIM =  0.7577293942827318\n",
      "Batch =  1000  Train Loss =  0.04674640022404492  SSIM =  0.7569177352152765\n",
      "Batch =  1100  Train Loss =  0.04688764220611616  SSIM =  0.7564964235743339\n",
      "Batch =  1200  Train Loss =  0.04700012150375794  SSIM =  0.7557216521569838\n",
      "Batch =  1300  Train Loss =  0.046896187398725975  SSIM =  0.7559973255802805\n",
      "Batch =  1400  Train Loss =  0.0469874221478988  SSIM =  0.7555096135687616\n",
      "Batch =  1500  Train Loss =  0.04696679612621665  SSIM =  0.7560303656732043\n",
      "Batch =  1600  Train Loss =  0.04714163556578569  SSIM =  0.7556500747217797\n",
      "Batch =  1700  Train Loss =  0.047079444869476204  SSIM =  0.7558497844351565\n",
      "Batch =  1800  Train Loss =  0.04704289507483029  SSIM =  0.7562509211463233\n",
      "Batch =  1900  Train Loss =  0.04708534584135601  SSIM =  0.7560660348420865\n",
      "Batch =  2000  Train Loss =  0.04708333476819098  SSIM =  0.7556540922876448\n",
      "Batch =  2100  Train Loss =  0.047150499694758936  SSIM =  0.7555933612212539\n",
      "Batch =  2200  Train Loss =  0.04714676075022329  SSIM =  0.7552849566919559\n",
      "Loader #:  0  Epoch:  8 /  300  -  Time:  0:05:55.101076 s - Validation Loss:  0.0  - Train Loss:  0.04714676075022329  - train SSIM:  0.7552849566919559  validation SSIM:  1661.6269047223032\n",
      "Batch =  1  Train Loss =  0.05339031666517258  SSIM =  0.7094067335128784\n",
      "Batch =  100  Train Loss =  0.0507249597646296  SSIM =  0.735614263266325\n",
      "Batch =  200  Train Loss =  0.04911573823541403  SSIM =  0.7421767024695873\n",
      "Batch =  300  Train Loss =  0.048483778821925325  SSIM =  0.7456210728486379\n",
      "Batch =  400  Train Loss =  0.047938832473009826  SSIM =  0.7469871102645993\n",
      "Batch =  500  Train Loss =  0.04762825710698962  SSIM =  0.7493540522009134\n",
      "Batch =  600  Train Loss =  0.04716615730586151  SSIM =  0.7530786855891347\n",
      "Batch =  700  Train Loss =  0.04724543538144124  SSIM =  0.7524076694356544\n",
      "Batch =  800  Train Loss =  0.047060431336867624  SSIM =  0.7539614094560966\n",
      "Batch =  900  Train Loss =  0.047125065113003885  SSIM =  0.7540112122189667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch =  1000  Train Loss =  0.04716753929434344  SSIM =  0.7535637502186\n",
      "Batch =  1100  Train Loss =  0.04709193829574029  SSIM =  0.754429669634185\n",
      "Batch =  1200  Train Loss =  0.04696562137726384  SSIM =  0.7550901977562656\n",
      "Batch =  1300  Train Loss =  0.04719817934092134  SSIM =  0.754240158506884\n",
      "Batch =  1400  Train Loss =  0.047272380946669725  SSIM =  0.7537545837328903\n",
      "Batch =  1500  Train Loss =  0.04723288032133132  SSIM =  0.7539837591523926\n",
      "Batch =  1600  Train Loss =  0.04727843918808503  SSIM =  0.7538343140739017\n",
      "Batch =  1700  Train Loss =  0.047214366791715076  SSIM =  0.7540796475967063\n",
      "Batch =  1800  Train Loss =  0.04711560657894653  SSIM =  0.7549220788230498\n",
      "Batch =  1900  Train Loss =  0.047074228511682074  SSIM =  0.7552175739956529\n",
      "Batch =  2000  Train Loss =  0.047166847471380606  SSIM =  0.7547406126633287\n",
      "Batch =  2100  Train Loss =  0.04705287262863879  SSIM =  0.7553785379869598\n",
      "Batch =  2200  Train Loss =  0.04708786593940617  SSIM =  0.7553873244131153\n",
      "Loader #:  0  Epoch:  9 /  300  -  Time:  0:05:54.926788 s - Validation Loss:  0.0  - Train Loss:  0.04708786593940617  - train SSIM:  0.7553873244131153  validation SSIM:  1661.8521137088537\n",
      "Batch =  1  Train Loss =  0.04758103936910629  SSIM =  0.6849827170372009\n",
      "Batch =  100  Train Loss =  0.048037344273179766  SSIM =  0.751547454893589\n",
      "Batch =  200  Train Loss =  0.04827909268438816  SSIM =  0.7490880928188562\n",
      "Batch =  300  Train Loss =  0.04756846446543932  SSIM =  0.7535224191844463\n",
      "Batch =  400  Train Loss =  0.04787662936840206  SSIM =  0.7538833716511726\n",
      "Batch =  500  Train Loss =  0.04800325672514737  SSIM =  0.75266592669487\n",
      "Batch =  600  Train Loss =  0.04845833921960244  SSIM =  0.7492389862673978\n",
      "Batch =  700  Train Loss =  0.048113041904621894  SSIM =  0.7516659356387598\n",
      "Batch =  800  Train Loss =  0.04778383918222971  SSIM =  0.7527475829934701\n",
      "Batch =  900  Train Loss =  0.047899096098004114  SSIM =  0.7531345214404993\n",
      "Batch =  1000  Train Loss =  0.04787611171510071  SSIM =  0.7528935808651149\n",
      "Batch =  1100  Train Loss =  0.04787974222647873  SSIM =  0.7526663704453544\n",
      "Batch =  1200  Train Loss =  0.047509818791101374  SSIM =  0.7544450123204539\n",
      "Batch =  1300  Train Loss =  0.047304060814472346  SSIM =  0.7556940083406293\n",
      "Batch =  1400  Train Loss =  0.04747331241012684  SSIM =  0.7548028726849173\n",
      "Batch =  1500  Train Loss =  0.04731068063775699  SSIM =  0.7555265609299143\n",
      "Batch =  1600  Train Loss =  0.0471897208376322  SSIM =  0.7563587406720035\n",
      "Batch =  1700  Train Loss =  0.04728310457923833  SSIM =  0.7557638414532822\n",
      "Batch =  1800  Train Loss =  0.047191828359435826  SSIM =  0.7560279489370684\n",
      "Batch =  1900  Train Loss =  0.04709602447863864  SSIM =  0.7561743048127545\n",
      "Batch =  2000  Train Loss =  0.04723267578007653  SSIM =  0.7553964051362128\n",
      "Batch =  2100  Train Loss =  0.04717015307352301  SSIM =  0.7555023502860041\n",
      "Batch =  2200  Train Loss =  0.047056154803880916  SSIM =  0.755998116587712\n",
      "Loader #:  0  Epoch:  10 /  300  -  Time:  0:05:54.758452 s - Validation Loss:  0.0  - Train Loss:  0.047056154803880916  - train SSIM:  0.755998116587712  validation SSIM:  1663.1958564929664\n",
      "Batch =  1  Train Loss =  0.11651825904846191  SSIM =  0.4269697070121765\n",
      "Batch =  100  Train Loss =  0.04387539111077785  SSIM =  0.7726654755696655\n",
      "Batch =  200  Train Loss =  0.04585643911734223  SSIM =  0.7597767081670463\n",
      "Batch =  300  Train Loss =  0.04540300597126285  SSIM =  0.7646378690873583\n",
      "Batch =  400  Train Loss =  0.04564889750443399  SSIM =  0.7615056693833321\n",
      "Batch =  500  Train Loss =  0.04634553954005241  SSIM =  0.7579012717232108\n",
      "Batch =  600  Train Loss =  0.04683101876949271  SSIM =  0.7550040525135895\n",
      "Batch =  700  Train Loss =  0.04672731837257743  SSIM =  0.7553953233414462\n",
      "Batch =  800  Train Loss =  0.04707822029013187  SSIM =  0.7534625496016816\n",
      "Batch =  900  Train Loss =  0.046649602846139004  SSIM =  0.7557384276265899\n",
      "Batch =  1000  Train Loss =  0.04661784679815173  SSIM =  0.756185508672148\n",
      "Batch =  1100  Train Loss =  0.04676874936812303  SSIM =  0.7561389477313919\n",
      "Batch =  1200  Train Loss =  0.04690530232464274  SSIM =  0.7554154492697368\n",
      "Batch =  1300  Train Loss =  0.046696600819436405  SSIM =  0.7564849028363824\n",
      "Batch =  1400  Train Loss =  0.046738290688289066  SSIM =  0.7565573229986642\n",
      "Batch =  1500  Train Loss =  0.046665169142186644  SSIM =  0.7571231609508395\n",
      "Batch =  1600  Train Loss =  0.046655871879775074  SSIM =  0.7572889503021725\n",
      "Batch =  1700  Train Loss =  0.046664004174115904  SSIM =  0.7570435783113627\n",
      "Batch =  1800  Train Loss =  0.04670078293446245  SSIM =  0.7567735834026502\n",
      "Batch =  1900  Train Loss =  0.04687865522861677  SSIM =  0.7557257055706884\n",
      "Batch =  2000  Train Loss =  0.04685134925157763  SSIM =  0.7560867567081004\n",
      "Batch =  2100  Train Loss =  0.046809106590891526  SSIM =  0.7566402925710592\n",
      "Batch =  2200  Train Loss =  0.046941752174488184  SSIM =  0.756260231694376\n",
      "Loader #:  0  Epoch:  11 /  300  -  Time:  0:05:54.672429 s - Validation Loss:  0.0  - Train Loss:  0.046941752174488184  - train SSIM:  0.756260231694376  validation SSIM:  1663.772509727627\n",
      "Batch =  1  Train Loss =  0.060600049793720245  SSIM =  0.6687302142381668\n",
      "Batch =  100  Train Loss =  0.049350346038118005  SSIM =  0.7443067781627178\n",
      "Batch =  200  Train Loss =  0.04822805829811841  SSIM =  0.74969496242702\n",
      "Batch =  300  Train Loss =  0.04786852958612144  SSIM =  0.7510217496752739\n",
      "Batch =  400  Train Loss =  0.04805072433548048  SSIM =  0.7494592378288507\n",
      "Batch =  500  Train Loss =  0.04740920413844287  SSIM =  0.7515796269476414\n",
      "Batch =  600  Train Loss =  0.047395064992209274  SSIM =  0.7533507541318735\n",
      "Batch =  700  Train Loss =  0.047306062688252756  SSIM =  0.7536018614045211\n",
      "Batch =  800  Train Loss =  0.047395696034654974  SSIM =  0.7540859959088266\n",
      "Batch =  900  Train Loss =  0.047618200609253515  SSIM =  0.7527887061817778\n",
      "Batch =  1000  Train Loss =  0.047660818522796036  SSIM =  0.7521208587512374\n",
      "Batch =  1100  Train Loss =  0.04737417971969328  SSIM =  0.7540160146287896\n",
      "Batch =  1200  Train Loss =  0.04739472038190191  SSIM =  0.7536720479466021\n",
      "Batch =  1300  Train Loss =  0.0472394267276216  SSIM =  0.7544285525668126\n",
      "Batch =  1400  Train Loss =  0.04729063676869763  SSIM =  0.7541483426040837\n",
      "Batch =  1500  Train Loss =  0.04704167101656397  SSIM =  0.7554306087940932\n",
      "Batch =  1600  Train Loss =  0.047213895481545476  SSIM =  0.754528929288499\n",
      "Batch =  1700  Train Loss =  0.04708008569939172  SSIM =  0.755216452343499\n",
      "Batch =  1800  Train Loss =  0.04705371218060868  SSIM =  0.7552866199488442\n",
      "Batch =  1900  Train Loss =  0.04704867609678522  SSIM =  0.7555341414832755\n",
      "Batch =  2000  Train Loss =  0.04695863520679995  SSIM =  0.7562846719808877\n",
      "Batch =  2100  Train Loss =  0.04680282988097696  SSIM =  0.756964568846992\n",
      "Batch =  2200  Train Loss =  0.046881758504631846  SSIM =  0.7563303113627163\n",
      "Loader #:  0  Epoch:  12 /  300  -  Time:  0:05:54.623711 s - Validation Loss:  0.0  - Train Loss:  0.046881758504631846  - train SSIM:  0.7563303113627163  validation SSIM:  1663.9266849979758\n",
      "Batch =  1  Train Loss =  0.03136100620031357  SSIM =  0.8788436651229858\n",
      "Batch =  100  Train Loss =  0.0479127830080688  SSIM =  0.7582423575222492\n",
      "Batch =  200  Train Loss =  0.049049395206384364  SSIM =  0.7476489868760109\n",
      "Batch =  300  Train Loss =  0.048456452044968804  SSIM =  0.749530200449129\n",
      "Batch =  400  Train Loss =  0.048434951109811664  SSIM =  0.7514120774716139\n",
      "Batch =  500  Train Loss =  0.04878157367371023  SSIM =  0.7493101537525654\n",
      "Batch =  600  Train Loss =  0.04862709246110171  SSIM =  0.7497085063656171\n",
      "Batch =  700  Train Loss =  0.04813151729825352  SSIM =  0.751677502074412\n",
      "Batch =  800  Train Loss =  0.047723776992643255  SSIM =  0.7538490627333522\n",
      "Batch =  900  Train Loss =  0.04717889848889576  SSIM =  0.7568438814746009\n",
      "Batch =  1000  Train Loss =  0.04690769673511386  SSIM =  0.757948389172554\n",
      "Batch =  1100  Train Loss =  0.04689611465500837  SSIM =  0.7576759534396909\n",
      "Batch =  1200  Train Loss =  0.04705797122015307  SSIM =  0.756863295721511\n",
      "Batch =  1300  Train Loss =  0.04709779927650323  SSIM =  0.7566750996502546\n",
      "Batch =  1400  Train Loss =  0.04716263466913785  SSIM =  0.7563164229691028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch =  1500  Train Loss =  0.04711488491296768  SSIM =  0.7564963608980179\n",
      "Batch =  1600  Train Loss =  0.047202464184956626  SSIM =  0.7557568147778511\n",
      "Batch =  1700  Train Loss =  0.047274982584092545  SSIM =  0.7549527084564461\n",
      "Batch =  1800  Train Loss =  0.047169085790713626  SSIM =  0.7555526764276955\n",
      "Batch =  1900  Train Loss =  0.04694596687380813  SSIM =  0.7566670762551458\n",
      "Batch =  2000  Train Loss =  0.04690125779155642  SSIM =  0.7569154429063201\n",
      "Batch =  2100  Train Loss =  0.04701116324269346  SSIM =  0.7556509350843372\n",
      "Batch =  2200  Train Loss =  0.046987179048698056  SSIM =  0.756076258431104\n",
      "Loader #:  0  Epoch:  13 /  300  -  Time:  0:05:54.868749 s - Validation Loss:  0.0  - Train Loss:  0.046987179048698056  - train SSIM:  0.756076258431104  validation SSIM:  1663.367768548429\n",
      "Batch =  1  Train Loss =  0.0231393501162529  SSIM =  0.8768608570098877\n",
      "Batch =  100  Train Loss =  0.04668420023284853  SSIM =  0.7544915230944753\n",
      "Batch =  200  Train Loss =  0.045810110331512985  SSIM =  0.761181706879288\n",
      "Batch =  300  Train Loss =  0.04578783563959102  SSIM =  0.7625070701166987\n",
      "Batch =  400  Train Loss =  0.04673262560740113  SSIM =  0.757447342472151\n",
      "Batch =  500  Train Loss =  0.04683671665936708  SSIM =  0.756979080028832\n",
      "Batch =  600  Train Loss =  0.046849706703796984  SSIM =  0.7561966335338851\n",
      "Batch =  700  Train Loss =  0.046750378853508405  SSIM =  0.7567355008902295\n",
      "Batch =  800  Train Loss =  0.046926462268456814  SSIM =  0.7559456551866606\n",
      "Batch =  900  Train Loss =  0.04691815953080853  SSIM =  0.7559471176771654\n",
      "Batch =  1000  Train Loss =  0.04710840781033039  SSIM =  0.7551031276322901\n",
      "Batch =  1100  Train Loss =  0.04692044445398179  SSIM =  0.7566166904670271\n",
      "Batch =  1200  Train Loss =  0.046695007868111135  SSIM =  0.757925632170712\n",
      "Batch =  1300  Train Loss =  0.04657380650536372  SSIM =  0.75839596873006\n",
      "Batch =  1400  Train Loss =  0.04642369104283196  SSIM =  0.7591333748267165\n",
      "Batch =  1500  Train Loss =  0.04629146886989474  SSIM =  0.7598091032082835\n",
      "Batch =  1600  Train Loss =  0.046513910033972934  SSIM =  0.7586010042834096\n",
      "Batch =  1700  Train Loss =  0.04660835417864077  SSIM =  0.7575452784821392\n",
      "Batch =  1800  Train Loss =  0.04654725402283172  SSIM =  0.7580041482030517\n",
      "Batch =  1900  Train Loss =  0.046604599496839864  SSIM =  0.7572245381674484\n",
      "Batch =  2000  Train Loss =  0.04644961368571967  SSIM =  0.7580120878424496\n",
      "Batch =  2100  Train Loss =  0.046500559685130914  SSIM =  0.7577955101696509\n",
      "Batch =  2200  Train Loss =  0.04664108857427808  SSIM =  0.7567563355155289\n",
      "Loader #:  0  Epoch:  14 /  300  -  Time:  0:05:54.573682 s - Validation Loss:  0.0  - Train Loss:  0.04664108857427808  - train SSIM:  0.7567563355155289  validation SSIM:  1664.8639381341636\n",
      "Batch =  1  Train Loss =  0.03898271545767784  SSIM =  0.8019876778125763\n",
      "Batch =  100  Train Loss =  0.04533469971269369  SSIM =  0.7622462955117225\n",
      "Batch =  200  Train Loss =  0.04460785324219614  SSIM =  0.7675878502428531\n",
      "Batch =  300  Train Loss =  0.04536099265950422  SSIM =  0.7661535373826822\n",
      "Batch =  400  Train Loss =  0.045172775520477444  SSIM =  0.766225808262825\n",
      "Batch =  500  Train Loss =  0.04563728629611433  SSIM =  0.7632474767565727\n",
      "Batch =  600  Train Loss =  0.04556672121242931  SSIM =  0.7634033553302288\n",
      "Batch =  700  Train Loss =  0.045837984158258356  SSIM =  0.7621851924274649\n",
      "Batch =  800  Train Loss =  0.0460140919405967  SSIM =  0.7618240069784225\n",
      "Batch =  900  Train Loss =  0.04631805295952492  SSIM =  0.7603718741403686\n",
      "Batch =  1000  Train Loss =  0.046651424795389175  SSIM =  0.7577129485160112\n",
      "Batch =  1100  Train Loss =  0.04669152746993032  SSIM =  0.7578413309156895\n",
      "Batch =  1200  Train Loss =  0.046670719385147094  SSIM =  0.7580274432276686\n",
      "Batch =  1300  Train Loss =  0.04678186696166029  SSIM =  0.756934395134449\n",
      "Batch =  1400  Train Loss =  0.046762554978153535  SSIM =  0.7568145525561911\n",
      "Batch =  1500  Train Loss =  0.04682899515330791  SSIM =  0.7565770578285058\n",
      "Batch =  1600  Train Loss =  0.04671473491762299  SSIM =  0.7574974753148854\n",
      "Batch =  1700  Train Loss =  0.046699664655196316  SSIM =  0.7573620704429991\n",
      "Batch =  1800  Train Loss =  0.04669192210408962  SSIM =  0.7575104755411546\n",
      "Batch =  1900  Train Loss =  0.04665355762613839  SSIM =  0.7575621392460246\n",
      "Batch =  2000  Train Loss =  0.04671989939874038  SSIM =  0.7572692061066627\n",
      "Batch =  2100  Train Loss =  0.04669033331752178  SSIM =  0.7570800882613375\n",
      "Batch =  2200  Train Loss =  0.046792593300765885  SSIM =  0.7565213602730496\n",
      "Loader #:  0  Epoch:  15 /  300  -  Time:  0:05:54.649568 s - Validation Loss:  0.0  - Train Loss:  0.046792593300765885  - train SSIM:  0.7565213602730496  validation SSIM:  1664.3469926007092\n",
      "Batch =  1  Train Loss =  0.03835761547088623  SSIM =  0.776927262544632\n",
      "Batch =  100  Train Loss =  0.04675553854554892  SSIM =  0.7578963784873486\n",
      "Batch =  200  Train Loss =  0.04795614004600793  SSIM =  0.7519762025028467\n",
      "Batch =  300  Train Loss =  0.047433453348154825  SSIM =  0.7560988790293535\n",
      "Batch =  400  Train Loss =  0.04768718407256529  SSIM =  0.7530272778868675\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:11\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m G_optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(NIO_Af\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m losses_D, losses_G, D, G \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_NIO_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNIO_Af\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_optimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36mtrain_NIO_transformer\u001b[0;34m(D, G, train_data, epochs, D_optim, G_optim, scheduler)\u001b[0m\n\u001b[1;32m     87\u001b[0m ssim_batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ssim_individual\u001b[38;5;241m/\u001b[39my\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     88\u001b[0m loss \u001b[38;5;241m=\u001b[39m W_loss\n\u001b[0;32m---> 89\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m loss_G \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \n\u001b[1;32m     91\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(G\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.1\u001b[39m)\n",
      "File \u001b[0;32m/depot/bera89/apps/IDEAS_hviswan/lib/python3.8/site-packages/torch/_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    248\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    249\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    254\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 255\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/bera89/apps/IDEAS_hviswan/lib/python3.8/site-packages/torch/autograd/__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 147\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "from timm.optim import create_optimizer\n",
    "#from apex import optimizers\n",
    "args = SimpleNamespace()\n",
    "args.opt = 'adamw'\n",
    "args.lr = 1e-4\n",
    "args.weight_decay=1e-4 \n",
    "args.opt_eps = 1e-8\n",
    "args.momentum = 0.9\n",
    "optimizer = create_optimizer(args, NIO_Af)\n",
    "epochs = 300\n",
    "NIO_Af.train()\n",
    "device = 'cuda:11'\n",
    "\n",
    "G_optimizer = torch.optim.Adam(NIO_Af.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "losses_D, losses_G, D, G = train_NIO_transformer(None, NIO_Af, None, epochs, None, G_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31bb32d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My IDEAS_hviswan Kernel)",
   "language": "python",
   "name": "ideas_hviswan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
